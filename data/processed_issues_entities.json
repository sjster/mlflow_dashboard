[{"html_url":"https:\/\/github.com\/mlflow\/mlflow\/issues\/5518","id":1175090284,"number":5518,"title":"[bug] source activate command not working for conda environment","state":"open","locked":false,"assignee":null,"assignees":[],"comments":1,"created_at":1647853804000,"updated_at":1647878660000,"closed_at":null,"body":"I am following the quickstart tutorial (https:\/\/www.mlflow.org\/docs\/latest\/tutorials-and-examples\/tutorial.html) and when running trying to package the model in a conda environment with `mlflow run sklearn_elasticnet_wine -P alpha=0.42` I get this error: \r\n`2022\/03\/21 02:05:21 INFO mlflow.projects.utils: === Created directory \/tmp\/tmpspwgyjnt for downloading remote URIs passed to arguments of type 'path' ===\r\n2022\/03\/21 02:05:21 INFO mlflow.projects.backend.local: === Running command 'source activate mlflow-7122f0cb71f385d249fbb61cc599afd8045ab238 1>&2 && python train.py 0.42 0.1' in run with ID '84f9342f31b64da29c22eaf8a0870659' === \r\nbash: activate: No such file or directory\r\n2022\/03\/21 02:05:21 ERROR mlflow.cli: === Run (ID '84f9342f31b64da29c22eaf8a0870659') failed ===\r\n`\r\n","pull_request":null,"labels":["bug"],"labels_description":["Something isn't working"],"entities":["source","command","conda","environment"]},{"html_url":"https:\/\/github.com\/mlflow\/mlflow\/pull\/5517","id":1174838592,"number":5517,"title":"show minimum and maximum metric values in the run view ui","state":"open","locked":false,"assignee":null,"assignees":[],"comments":0,"created_at":1647833205000,"updated_at":1647836702000,"closed_at":null,"body":"## What changes are proposed in this pull request?\r\n\r\nAdds \"Min\" and \"Max\" columns to the metrics table in the run view, and renames the \"Value\" column to \"Latest\". The minimum and maximum values are computed using the GetMetricHistory API, as discussed in #4750\r\n\r\n## How is this patch tested?\r\n\r\nUnit tests added for the new reduce functions and for the new component behaviour.\r\n\r\n## Does this PR change the documentation?\r\n\r\n- [x] No. You can skip the rest of this section.\r\n- [ ] Yes. Make sure the changed pages \/ sections render correctly by following the steps below.\r\n\r\n1. Check the status of the `ci\/circleci: build_doc` check. If it's successful, proceed to the\r\n   next step, otherwise fix it.\r\n2. Click `Details` on the right to open the job page of CircleCI.\r\n3. Click the `Artifacts` tab.\r\n4. Click `docs\/build\/html\/index.html`.\r\n5. Find the changed pages \/ sections and make sure they render correctly.\r\n\r\n## Release Notes\r\n\r\n### Is this a user-facing change?\r\n\r\n- [ ] No. You can skip the rest of this section.\r\n- [x] Yes. Give a description of this change to be included in the release notes for MLflow users.\r\n\r\nThe run view in the UI now displays the minimum and maximum metric values for a run.\r\n\r\n### What component(s), interfaces, languages, and integrations does this PR affect?\r\nComponents \r\n- [ ] `area\/artifacts`: Artifact stores and artifact logging\r\n- [ ] `area\/build`: Build and test infrastructure for MLflow\r\n- [ ] `area\/docs`: MLflow documentation pages\r\n- [ ] `area\/examples`: Example code\r\n- [ ] `area\/model-registry`: Model Registry service, APIs, and the fluent client calls for Model Registry\r\n- [ ] `area\/models`: MLmodel format, model serialization\/deserialization, flavors\r\n- [ ] `area\/projects`: MLproject format, project running backends\r\n- [ ] `area\/scoring`: MLflow Model server, model deployment tools, Spark UDFs\r\n- [ ] `area\/server-infra`: MLflow Tracking server backend\r\n- [ ] `area\/tracking`: Tracking Service, tracking client APIs, autologging\r\n\r\nInterface \r\n- [x] `area\/uiux`: Front-end, user experience, plotting, JavaScript, JavaScript dev server\r\n- [ ] `area\/docker`: Docker use across MLflow's components, such as MLflow Projects and MLflow Models\r\n- [ ] `area\/sqlalchemy`: Use of SQLAlchemy in the Tracking Service or Model Registry\r\n- [ ] `area\/windows`: Windows support\r\n\r\nLanguage \r\n- [ ] `language\/r`: R APIs and clients\r\n- [ ] `language\/java`: Java APIs and clients\r\n- [ ] `language\/new`: Proposals for new client languages\r\n\r\nIntegrations\r\n- [ ] `integrations\/azure`: Azure and Azure ML integrations\r\n- [ ] `integrations\/sagemaker`: SageMaker integrations\r\n- [ ] `integrations\/databricks`: Databricks integrations\r\n\r\n<!--\r\nInsert an empty named anchor here to allow jumping to this section with a fragment URL\r\n(e.g. https:\/\/github.com\/mlflow\/mlflow\/pull\/123#user-content-release-note-category).\r\nNote that GitHub prefixes anchor names in markdown with \"user-content-\".\r\n-->\r\n<a name=\"release-note-category\"><\/a>\r\n### How should the PR be classified in the release notes? Choose one:\r\n\r\n- [ ] `rn\/breaking-change` - The PR will be mentioned in the \"Breaking Changes\" section\r\n- [ ] `rn\/none` - No description will be included. The PR will be mentioned only by the PR number in the \"Small Bugfixes and Documentation Updates\" section\r\n- [x] `rn\/feature` - A new user-facing feature worth mentioning in the release notes\r\n- [ ] `rn\/bug-fix` - A user-facing bug fix worth mentioning in the release notes\r\n- [ ] `rn\/documentation` - A user-facing documentation change worth mentioning in the release notes\r\n","pull_request":{"url":"https:\/\/api.github.com\/repos\/mlflow\/mlflow\/pulls\/5517","html_url":"https:\/\/github.com\/mlflow\/mlflow\/pull\/5517","diff_url":"https:\/\/github.com\/mlflow\/mlflow\/pull\/5517.diff","patch_url":"https:\/\/github.com\/mlflow\/mlflow\/pull\/5517.patch","merged_at":null},"labels":["rn\/feature","area\/uiux"],"labels_description":["Mention under Features in Changelogs.","Front-end, user experience, plotting, JavaScript, JavaScript dev server"],"entities":["minimum","maximum","run","view"]},{"html_url":"https:\/\/github.com\/mlflow\/mlflow\/pull\/5515","id":1174714315,"number":5515,"title":"implement issue-5501 - add mlflow_version info to logged model","state":"open","locked":false,"assignee":null,"assignees":[],"comments":1,"created_at":1647816410000,"updated_at":1647909021000,"closed_at":null,"body":"Signed-off-by: Nikolay Ulmasov <ulmasov@hotmail.com>\r\n\r\n## What changes are proposed in this pull request?\r\n\r\nImplement issue #5501 - Add an mlflow_version field to the MLModel spec that is automatically populated with the current MLflow version when an MLflow Model is saved \/ logged.\r\n\r\nModels logged with previous versions will have `mlflow_version=None` when loaded from a file.\r\n\r\n## How is this patch tested?\r\n\r\nExtended existing tests and added a test to load previously logged models without mlflow_version info\r\n\r\n## Does this PR change the documentation?\r\n\r\n- [ x] No. You can skip the rest of this section.\r\n- [ ] Yes. Make sure the changed pages \/ sections render correctly by following the steps below.\r\n\r\n1. Check the status of the `ci\/circleci: build_doc` check. If it's successful, proceed to the\r\n   next step, otherwise fix it.\r\n2. Click `Details` on the right to open the job page of CircleCI.\r\n3. Click the `Artifacts` tab.\r\n4. Click `docs\/build\/html\/index.html`.\r\n5. Find the changed pages \/ sections and make sure they render correctly.\r\n\r\n## Release Notes\r\n\r\n### Is this a user-facing change?\r\n\r\n- [ ] No. You can skip the rest of this section.\r\n- [x] Yes. Give a description of this change to be included in the release notes for MLflow users.\r\n\r\nUsers can check the version of MLFlow the model with logged with. \r\n\r\n(Details in 1-2 sentences. You can just refer to another PR with a description if this PR is part of a larger change.)\r\n\r\n### What component(s), interfaces, languages, and integrations does this PR affect?\r\nComponents \r\n- [ ] `area\/artifacts`: Artifact stores and artifact logging\r\n- [ ] `area\/build`: Build and test infrastructure for MLflow\r\n- [ ] `area\/docs`: MLflow documentation pages\r\n- [ ] `area\/examples`: Example code\r\n- [ ] `area\/model-registry`: Model Registry service, APIs, and the fluent client calls for Model Registry\r\n- [ x] `area\/models`: MLmodel format, model serialization\/deserialization, flavors\r\n- [ ] `area\/projects`: MLproject format, project running backends\r\n- [ ] `area\/scoring`: MLflow Model server, model deployment tools, Spark UDFs\r\n- [ ] `area\/server-infra`: MLflow Tracking server backend\r\n- [ ] `area\/tracking`: Tracking Service, tracking client APIs, autologging\r\n\r\nInterface \r\n- [ ] `area\/uiux`: Front-end, user experience, plotting, JavaScript, JavaScript dev server\r\n- [ ] `area\/docker`: Docker use across MLflow's components, such as MLflow Projects and MLflow Models\r\n- [ ] `area\/sqlalchemy`: Use of SQLAlchemy in the Tracking Service or Model Registry\r\n- [ ] `area\/windows`: Windows support\r\n\r\nLanguage \r\n- [ ] `language\/r`: R APIs and clients\r\n- [ ] `language\/java`: Java APIs and clients\r\n- [ ] `language\/new`: Proposals for new client languages\r\n\r\nIntegrations\r\n- [ ] `integrations\/azure`: Azure and Azure ML integrations\r\n- [ ] `integrations\/sagemaker`: SageMaker integrations\r\n- [ ] `integrations\/databricks`: Databricks integrations\r\n\r\n<!--\r\nInsert an empty named anchor here to allow jumping to this section with a fragment URL\r\n(e.g. https:\/\/github.com\/mlflow\/mlflow\/pull\/123#user-content-release-note-category).\r\nNote that GitHub prefixes anchor names in markdown with \"user-content-\".\r\n-->\r\n<a name=\"release-note-category\"><\/a>\r\n### How should the PR be classified in the release notes? Choose one:\r\n\r\n- [ ] `rn\/breaking-change` - The PR will be mentioned in the \"Breaking Changes\" section\r\n- [ ] `rn\/none` - No description will be included. The PR will be mentioned only by the PR number in the \"Small Bugfixes and Documentation Updates\" section\r\n- [x ] `rn\/feature` - A new user-facing feature worth mentioning in the release notes\r\n- [ ] `rn\/bug-fix` - A user-facing bug fix worth mentioning in the release notes\r\n- [ ] `rn\/documentation` - A user-facing documentation change worth mentioning in the release notes\r\n","pull_request":{"url":"https:\/\/api.github.com\/repos\/mlflow\/mlflow\/pulls\/5515","html_url":"https:\/\/github.com\/mlflow\/mlflow\/pull\/5515","diff_url":"https:\/\/github.com\/mlflow\/mlflow\/pull\/5515.diff","patch_url":"https:\/\/github.com\/mlflow\/mlflow\/pull\/5515.patch","merged_at":null},"labels":["rn\/feature","area\/models"],"labels_description":["Mention under Features in Changelogs.","MLmodel format, model serialization\/deserialization, flavors"],"entities":["mlflow_version","info","model"]},{"html_url":"https:\/\/github.com\/mlflow\/mlflow\/pull\/5513","id":1174246753,"number":5513,"title":"refactor files in `tests\/db` to allow running tests in repository root","state":"open","locked":false,"assignee":null,"assignees":[],"comments":0,"created_at":1647690355000,"updated_at":1647893860000,"closed_at":null,"body":"## What changes are proposed in this pull request?\r\n\r\nRefactor files under `tests\/db` for the following purposes:\r\n\r\n- Mount the repository root for faster development cycle.\r\n- Allow running tests for `SqlAlchemyStore` (e.g. `tests\/store\/tracking\/test_sqlalchemy_store.py`) using various databases.\r\n\r\n## How is this patch tested?\r\n\r\nExisting tests \r\n\r\n## Does this PR change the documentation?\r\n\r\n- [x] No. You can skip the rest of this section.\r\n- [ ] Yes. Make sure the changed pages \/ sections render correctly by following the steps below.\r\n\r\n1. Check the status of the `ci\/circleci: build_doc` check. If it's successful, proceed to the\r\n   next step, otherwise fix it.\r\n2. Click `Details` on the right to open the job page of CircleCI.\r\n3. Click the `Artifacts` tab.\r\n4. Click `docs\/build\/html\/index.html`.\r\n5. Find the changed pages \/ sections and make sure they render correctly.\r\n\r\n## Release Notes\r\n\r\n### Is this a user-facing change?\r\n\r\n- [x] No. You can skip the rest of this section.\r\n- [ ] Yes. Give a description of this change to be included in the release notes for MLflow users.\r\n\r\n(Details in 1-2 sentences. You can just refer to another PR with a description if this PR is part of a larger change.)\r\n\r\n### What component(s), interfaces, languages, and integrations does this PR affect?\r\nComponents \r\n- [ ] `area\/artifacts`: Artifact stores and artifact logging\r\n- [x] `area\/build`: Build and test infrastructure for MLflow\r\n- [ ] `area\/docs`: MLflow documentation pages\r\n- [ ] `area\/examples`: Example code\r\n- [ ] `area\/model-registry`: Model Registry service, APIs, and the fluent client calls for Model Registry\r\n- [ ] `area\/models`: MLmodel format, model serialization\/deserialization, flavors\r\n- [ ] `area\/projects`: MLproject format, project running backends\r\n- [ ] `area\/scoring`: MLflow Model server, model deployment tools, Spark UDFs\r\n- [ ] `area\/server-infra`: MLflow Tracking server backend\r\n- [ ] `area\/tracking`: Tracking Service, tracking client APIs, autologging\r\n\r\nInterface \r\n- [ ] `area\/uiux`: Front-end, user experience, plotting, JavaScript, JavaScript dev server\r\n- [ ] `area\/docker`: Docker use across MLflow's components, such as MLflow Projects and MLflow Models\r\n- [ ] `area\/sqlalchemy`: Use of SQLAlchemy in the Tracking Service or Model Registry\r\n- [ ] `area\/windows`: Windows support\r\n\r\nLanguage \r\n- [ ] `language\/r`: R APIs and clients\r\n- [ ] `language\/java`: Java APIs and clients\r\n- [ ] `language\/new`: Proposals for new client languages\r\n\r\nIntegrations\r\n- [ ] `integrations\/azure`: Azure and Azure ML integrations\r\n- [ ] `integrations\/sagemaker`: SageMaker integrations\r\n- [ ] `integrations\/databricks`: Databricks integrations\r\n\r\n<!--\r\nInsert an empty named anchor here to allow jumping to this section with a fragment URL\r\n(e.g. https:\/\/github.com\/mlflow\/mlflow\/pull\/123#user-content-release-note-category).\r\nNote that GitHub prefixes anchor names in markdown with \"user-content-\".\r\n-->\r\n<a name=\"release-note-category\"><\/a>\r\n### How should the PR be classified in the release notes? Choose one:\r\n\r\n- [ ] `rn\/breaking-change` - The PR will be mentioned in the \"Breaking Changes\" section\r\n- [x] `rn\/none` - No description will be included. The PR will be mentioned only by the PR number in the \"Small Bugfixes and Documentation Updates\" section\r\n- [ ] `rn\/feature` - A new user-facing feature worth mentioning in the release notes\r\n- [ ] `rn\/bug-fix` - A user-facing bug fix worth mentioning in the release notes\r\n- [ ] `rn\/documentation` - A user-facing documentation change worth mentioning in the release notes\r\n","pull_request":{"url":"https:\/\/api.github.com\/repos\/mlflow\/mlflow\/pulls\/5513","html_url":"https:\/\/github.com\/mlflow\/mlflow\/pull\/5513","diff_url":"https:\/\/github.com\/mlflow\/mlflow\/pull\/5513.diff","patch_url":"https:\/\/github.com\/mlflow\/mlflow\/pull\/5513.patch","merged_at":null},"labels":["rn\/none","area\/build"],"labels_description":["List under Small Changes in Changelogs.","Build and test infrastructure for MLflow"],"entities":["refactor","running","root"]},{"html_url":"https:\/\/github.com\/mlflow\/mlflow\/issues\/5512","id":1174017897,"number":5512,"title":"[fr] improve binary classification metric support in `eval_and_log_metrics`","state":"open","locked":false,"assignee":null,"assignees":[],"comments":0,"created_at":1647637412000,"updated_at":1647637427000,"closed_at":null,"body":"Thank you for submitting a feature request. **Before proceeding, please review MLflow's [Issue Policy for feature requests](https:\/\/www.github.com\/mlflow\/mlflow\/blob\/master\/ISSUE_POLICY.md#feature-requests) and the [MLflow Contributing Guide](https:\/\/github.com\/mlflow\/mlflow\/blob\/master\/CONTRIBUTING.rst)**.\r\n\r\n**Please fill in this feature request template to ensure a timely and thorough response.**\r\n\r\n## Willingness to contribute\r\nThe MLflow Community encourages new feature contributions. Would you or another member of your organization be willing to contribute an implementation of this feature (either as an MLflow Plugin or an enhancement to the MLflow code base)?\r\n\r\n- [ ] Yes. I can contribute this feature independently.\r\n- [ ] Yes. I would be willing to contribute this feature with guidance from the MLflow community.\r\n- [ ] No. I cannot contribute this feature at this time.\r\n\r\n## Proposal Summary\r\n\r\n(In a few sentences, provide a clear, high-level description of the feature request)\r\n\r\n## Motivation\r\n- What is the use case for this feature?\r\n- Why is this use case valuable to support for MLflow users in general?\r\n- Why is this use case valuable to support for your project(s) or organization?\r\n- Why is it currently difficult to achieve this use case? (please be as specific as possible about why related MLflow features and components are insufficient)\r\n\r\n### What component(s), interfaces, languages, and integrations does this feature affect?\r\nComponents \r\n- [ ] `area\/artifacts`: Artifact stores and artifact logging\r\n- [ ] `area\/build`: Build and test infrastructure for MLflow\r\n- [ ] `area\/docs`: MLflow documentation pages\r\n- [ ] `area\/examples`: Example code\r\n- [ ] `area\/model-registry`: Model Registry service, APIs, and the fluent client calls for Model Registry\r\n- [ ] `area\/models`: MLmodel format, model serialization\/deserialization, flavors\r\n- [ ] `area\/projects`: MLproject format, project running backends\r\n- [ ] `area\/scoring`: MLflow Model server, model deployment tools, Spark UDFs\r\n- [ ] `area\/server-infra`: MLflow Tracking server backend\r\n- [x] `area\/tracking`: Tracking Service, tracking client APIs, autologging\r\n\r\nInterfaces\r\n- [ ] `area\/uiux`: Front-end, user experience, plotting, JavaScript, JavaScript dev server\r\n- [ ] `area\/docker`: Docker use across MLflow's components, such as MLflow Projects and MLflow Models\r\n- [ ] `area\/sqlalchemy`: Use of SQLAlchemy in the Tracking Service or Model Registry\r\n- [ ] `area\/windows`: Windows support\r\n\r\nLanguages \r\n- [ ] `language\/r`: R APIs and clients\r\n- [ ] `language\/java`: Java APIs and clients\r\n- [ ] `language\/new`: Proposals for new client languages\r\n\r\nIntegrations\r\n- [ ] `integrations\/azure`: Azure and Azure ML integrations\r\n- [ ] `integrations\/sagemaker`: SageMaker integrations\r\n- [ ] `integrations\/databricks`: Databricks integrations\r\n\r\n## Details\r\n\r\n(Use this section to include any additional information about the feature. If you have a proposal for how to implement this feature, please include it here. For implementation guidelines, please refer to the [Contributing Guide](https:\/\/github.com\/mlflow\/mlflow\/blob\/master\/CONTRIBUTING.rst#contribution-guidelines).)\r\n\r\nCurrently in `mlflow.sklearn.eval_and_log_metrics` function [code](https:\/\/github.com\/mlflow\/mlflow\/blob\/41deb9ee5705d2727dc3df7fe60ce2c09231c8cc\/mlflow\/sklearn\/__init__.py#L1722), when the model is a classifier, the function computes precision\/recall\/f1 metrics as multi-class classification with the `average=\"weighted\"` label: [code](https:\/\/github.com\/mlflow\/mlflow\/blob\/41deb9ee5705d2727dc3df7fe60ce2c09231c8cc\/mlflow\/sklearn\/utils.py#L192-L213). This could produce drastically different metric when the classifier is doing binary classification, in which case we should supply a `pos_label` parameter (sklearn [doc](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.recall_score.html). We should expose the interface of `eval_and_log_users` function for user to optionally fill in the `pos_label` if they are doing binary classification.\r\n","pull_request":null,"labels":["enhancement","area\/tracking"],"labels_description":["New feature or request","Tracking service, tracking client APIs, autologging"],"entities":["classification","metric","support"]},{"html_url":"https:\/\/github.com\/mlflow\/mlflow\/pull\/5510","id":1172955087,"number":5510,"title":"added autologging of input example and signature for tf.estimator and keras fit_generator","state":"open","locked":false,"assignee":null,"assignees":[],"comments":0,"created_at":1647553841000,"updated_at":1647888473000,"closed_at":null,"body":"Signed-off-by: Jas Bali <bali0019@gmail.com>\r\n\r\n## What changes are proposed in this pull request?\r\n\r\nAutologging of input example\/signature for tf.estimator\r\nAutologging of input example\/signature for fit_generator for keras\r\nCleanup of log models inside keras callback\r\n\r\n## How is this patch tested?\r\n\r\nUnit tests\r\n\r\n## Does this PR change the documentation?\r\n\r\n- [ ] No. You can skip the rest of this section.\r\n- [ ] Yes. Make sure the changed pages \/ sections render correctly by following the steps below.\r\n\r\n1. Check the status of the `ci\/circleci: build_doc` check. If it's successful, proceed to the\r\n   next step, otherwise fix it.\r\n2. Click `Details` on the right to open the job page of CircleCI.\r\n3. Click the `Artifacts` tab.\r\n4. Click `docs\/build\/html\/index.html`.\r\n5. Find the changed pages \/ sections and make sure they render correctly.\r\n\r\n## Release Notes\r\n\r\n### Is this a user-facing change?\r\n\r\n- [ ] No. You can skip the rest of this section.\r\n- [x] Yes. Give a description of this change to be included in the release notes for MLflow users.\r\n\r\nAdded autologging of input example and signature for tf.estimator and keras fit_generator\r\n\r\n### What component(s), interfaces, languages, and integrations does this PR affect?\r\nComponents \r\n- [ ] `area\/artifacts`: Artifact stores and artifact logging\r\n- [ ] `area\/build`: Build and test infrastructure for MLflow\r\n- [ ] `area\/docs`: MLflow documentation pages\r\n- [ ] `area\/examples`: Example code\r\n- [ ] `area\/model-registry`: Model Registry service, APIs, and the fluent client calls for Model Registry\r\n- [x] `area\/models`: MLmodel format, model serialization\/deserialization, flavors\r\n- [ ] `area\/projects`: MLproject format, project running backends\r\n- [ ] `area\/scoring`: MLflow Model server, model deployment tools, Spark UDFs\r\n- [ ] `area\/server-infra`: MLflow Tracking server backend\r\n- [ ] `area\/tracking`: Tracking Service, tracking client APIs, autologging\r\n\r\nInterface \r\n- [ ] `area\/uiux`: Front-end, user experience, plotting, JavaScript, JavaScript dev server\r\n- [ ] `area\/docker`: Docker use across MLflow's components, such as MLflow Projects and MLflow Models\r\n- [ ] `area\/sqlalchemy`: Use of SQLAlchemy in the Tracking Service or Model Registry\r\n- [ ] `area\/windows`: Windows support\r\n\r\nLanguage \r\n- [ ] `language\/r`: R APIs and clients\r\n- [ ] `language\/java`: Java APIs and clients\r\n- [ ] `language\/new`: Proposals for new client languages\r\n\r\nIntegrations\r\n- [ ] `integrations\/azure`: Azure and Azure ML integrations\r\n- [ ] `integrations\/sagemaker`: SageMaker integrations\r\n- [ ] `integrations\/databricks`: Databricks integrations\r\n\r\n<!--\r\nInsert an empty named anchor here to allow jumping to this section with a fragment URL\r\n(e.g. https:\/\/github.com\/mlflow\/mlflow\/pull\/123#user-content-release-note-category).\r\nNote that GitHub prefixes anchor names in markdown with \"user-content-\".\r\n-->\r\n<a name=\"release-note-category\"><\/a>\r\n### How should the PR be classified in the release notes? Choose one:\r\n\r\n- [ ] `rn\/breaking-change` - The PR will be mentioned in the \"Breaking Changes\" section\r\n- [ ] `rn\/none` - No description will be included. The PR will be mentioned only by the PR number in the \"Small Bugfixes and Documentation Updates\" section\r\n- [x] `rn\/feature` - A new user-facing feature worth mentioning in the release notes\r\n- [ ] `rn\/bug-fix` - A user-facing bug fix worth mentioning in the release notes\r\n- [ ] `rn\/documentation` - A user-facing documentation change worth mentioning in the release notes\r\n","pull_request":{"url":"https:\/\/api.github.com\/repos\/mlflow\/mlflow\/pulls\/5510","html_url":"https:\/\/github.com\/mlflow\/mlflow\/pull\/5510","diff_url":"https:\/\/github.com\/mlflow\/mlflow\/pull\/5510.diff","patch_url":"https:\/\/github.com\/mlflow\/mlflow\/pull\/5510.patch","merged_at":null},"labels":["rn\/feature","area\/models"],"labels_description":["Mention under Features in Changelogs.","MLmodel format, model serialization\/deserialization, flavors"],"entities":["autologging","input","example","signature","tf.estimator","keras","fit_generator"]},{"html_url":"https:\/\/github.com\/mlflow\/mlflow\/issues\/5507","id":1172538922,"number":5507,"title":"[fr] improve performance by lowering amount of calls to retrieve model","state":"open","locked":false,"assignee":null,"assignees":[],"comments":4,"created_at":1647533761000,"updated_at":1647908641000,"closed_at":null,"body":"Thank you for submitting a feature request. **Before proceeding, please review MLflow's [Issue Policy for feature requests](https:\/\/www.github.com\/mlflow\/mlflow\/blob\/master\/ISSUE_POLICY.md#feature-requests) and the [MLflow Contributing Guide](https:\/\/github.com\/mlflow\/mlflow\/blob\/master\/CONTRIBUTING.rst)**.\r\n\r\n**Please fill in this feature request template to ensure a timely and thorough response.**\r\n\r\n## Willingness to contribute\r\nThe MLflow Community encourages new feature contributions. Would you or another member of your organization be willing to contribute an implementation of this feature (either as an MLflow Plugin or an enhancement to the MLflow code base)?\r\n\r\n- [ ] Yes. I can contribute this feature independently.\r\n- [ ] Yes. I would be willing to contribute this feature with guidance from the MLflow community.\r\n- [X] No. I cannot contribute this feature at this time.\r\n\r\n## Proposal Summary\r\n\r\nRetrieve models more efficiently by lowering required amount of requests.\r\n\r\nCurrently to retrieve a model we have to do 3 requests:\r\nexperiment_name=\"energy_forecast_10001_Amsterdam\"\r\nexperiment = mlflow.get_experiment_by_name(experiment_name)\r\nrun = mlflow.search_runs(experiment.experiment_id, max_results=1)\r\nmodel = mlflow.sklearn.load_model(os.path.join(run.artifact_uri[0], \"model\/\"))\r\n\r\nIt would be nice if this can be speeded up by getting model in only 1 request:\r\nmodel = mlflow.sklearn.load_latest_model(experiment_name)\r\n\r\nor 2 requests:\r\nrun = mlflow.search_runs(experiment_name, max_results=1)\r\nmodel = mlflow.sklearn.load_model(os.path.join(run.artifact_uri[0], \"model\/\"))\r\n\r\n## Motivation\r\n- What is the use case for this feature?\r\nPerformance\r\n- Why is this use case valuable to support for MLflow users in general?\r\nPerformance for all users to load models.\r\n- Why is this use case valuable to support for your project(s) or organization?\r\nPerformance.\r\n- Why is it currently difficult to achieve this use case? (please be as specific as possible about why related MLflow features and components are insufficient)\r\nIt's more difficult\/impossible to improve the performance at higher level when lower calls are not performant.\r\n\r\n### What component(s), interfaces, languages, and integrations does this feature affect?\r\nComponents \r\n- [X] `area\/artifacts`: Artifact stores and artifact logging\r\n- [ ] `area\/build`: Build and test infrastructure for MLflow\r\n- [ ] `area\/docs`: MLflow documentation pages\r\n- [ ] `area\/examples`: Example code\r\n- [X] `area\/model-registry`: Model Registry service, APIs, and the fluent client calls for Model Registry\r\n- [X] `area\/models`: MLmodel format, model serialization\/deserialization, flavors\r\n- [ ] `area\/projects`: MLproject format, project running backends\r\n- [ ] `area\/scoring`: MLflow Model server, model deployment tools, Spark UDFs\r\n- [X] `area\/server-infra`: MLflow Tracking server backend\r\n- [ ] `area\/tracking`: Tracking Service, tracking client APIs, autologging\r\n\r\nInterfaces\r\n- [ ] `area\/uiux`: Front-end, user experience, plotting, JavaScript, JavaScript dev server\r\n- [ ] `area\/docker`: Docker use across MLflow's components, such as MLflow Projects and MLflow Models\r\n- [ ] `area\/sqlalchemy`: Use of SQLAlchemy in the Tracking Service or Model Registry\r\n- [ ] `area\/windows`: Windows support\r\n\r\nLanguages \r\n- [ ] `language\/r`: R APIs and clients\r\n- [ ] `language\/java`: Java APIs and clients\r\n- [ ] `language\/new`: Proposals for new client languages\r\n\r\nIntegrations\r\n- [ ] `integrations\/azure`: Azure and Azure ML integrations\r\n- [ ] `integrations\/sagemaker`: SageMaker integrations\r\n- [ ] `integrations\/databricks`: Databricks integrations\r\n\r\n## Details\r\n\r\n(Use this section to include any additional information about the feature. If you have a proposal for how to implement this feature, please include it here. For implementation guidelines, please refer to the [Contributing Guide](https:\/\/github.com\/mlflow\/mlflow\/blob\/master\/CONTRIBUTING.rst#contribution-guidelines).)\r\n","pull_request":null,"labels":["enhancement","area\/artifacts","area\/model-registry","area\/models","area\/server-infra"],"labels_description":["New feature or request","Artifact stores and artifact logging","Model registry, model registry APIs, and the fluent client calls for model registry","MLmodel format, model serialization\/deserialization, flavors","MLflow Tracking server backend"],"entities":["performance","amount","model"]},{"html_url":"https:\/\/github.com\/mlflow\/mlflow\/pull\/5503","id":1171714151,"number":5503,"title":"implement function to print mlflow pyfunc model dependencies","state":"open","locked":false,"assignee":null,"assignees":[],"comments":2,"created_at":1647475767000,"updated_at":1647894258000,"closed_at":null,"body":"## What changes are proposed in this pull request?\r\n\r\nImplement function to print MLflow pyfunc model dependencies\r\n\r\nContext: when `pyfunc.load_model` called, the model might require different python env with current python env. So we need to print model dependencies for user and tell user instructions for how to recover the dependencies.\r\n\r\nFor now we don't provide \"env_manager\" argument for `pyfunc.load_model` because `pyfunc_model.predict` allow many kinds of input data , transferring input data between different python process is hard.\r\n\r\n@mengxr 's suggestion:\r\nIn mlflow.pyfunc.load_mode()\r\nIf python environment mismatch, only warn and provide instructions of: \r\n> Update this python environment with model\u2019s requirements.txt, rerun this process.\r\n\r\n\r\n## How is this patch tested?\r\n\r\nN\/A\r\n\r\n## Does this PR change the documentation?\r\n\r\n- [ ] No. You can skip the rest of this section.\r\n- [x] Yes. Make sure the changed pages \/ sections render correctly by following the steps below.\r\n\r\n1. Check the status of the `ci\/circleci: build_doc` check. If it's successful, proceed to the\r\n   next step, otherwise fix it.\r\n2. Click `Details` on the right to open the job page of CircleCI.\r\n3. Click the `Artifacts` tab.\r\n4. Click `docs\/build\/html\/index.html`.\r\n5. Find the changed pages \/ sections and make sure they render correctly.\r\n\r\n## Release Notes\r\n\r\n### Is this a user-facing change?\r\n\r\n- [ ] No. You can skip the rest of this section.\r\n- [x] Yes. Give a description of this change to be included in the release notes for MLflow users.\r\n\r\n(Details in 1-2 sentences. You can just refer to another PR with a description if this PR is part of a larger change.)\r\n\r\n### What component(s), interfaces, languages, and integrations does this PR affect?\r\nComponents \r\n- [ ] `area\/artifacts`: Artifact stores and artifact logging\r\n- [ ] `area\/build`: Build and test infrastructure for MLflow\r\n- [ ] `area\/docs`: MLflow documentation pages\r\n- [ ] `area\/examples`: Example code\r\n- [ ] `area\/model-registry`: Model Registry service, APIs, and the fluent client calls for Model Registry\r\n- [ ] `area\/models`: MLmodel format, model serialization\/deserialization, flavors\r\n- [ ] `area\/projects`: MLproject format, project running backends\r\n- [x] `area\/scoring`: MLflow Model server, model deployment tools, Spark UDFs\r\n- [ ] `area\/server-infra`: MLflow Tracking server backend\r\n- [ ] `area\/tracking`: Tracking Service, tracking client APIs, autologging\r\n\r\nInterface \r\n- [ ] `area\/uiux`: Front-end, user experience, plotting, JavaScript, JavaScript dev server\r\n- [ ] `area\/docker`: Docker use across MLflow's components, such as MLflow Projects and MLflow Models\r\n- [ ] `area\/sqlalchemy`: Use of SQLAlchemy in the Tracking Service or Model Registry\r\n- [ ] `area\/windows`: Windows support\r\n\r\nLanguage \r\n- [ ] `language\/r`: R APIs and clients\r\n- [ ] `language\/java`: Java APIs and clients\r\n- [ ] `language\/new`: Proposals for new client languages\r\n\r\nIntegrations\r\n- [ ] `integrations\/azure`: Azure and Azure ML integrations\r\n- [ ] `integrations\/sagemaker`: SageMaker integrations\r\n- [ ] `integrations\/databricks`: Databricks integrations\r\n\r\n<!--\r\nInsert an empty named anchor here to allow jumping to this section with a fragment URL\r\n(e.g. https:\/\/github.com\/mlflow\/mlflow\/pull\/123#user-content-release-note-category).\r\nNote that GitHub prefixes anchor names in markdown with \"user-content-\".\r\n-->\r\n<a name=\"release-note-category\"><\/a>\r\n### How should the PR be classified in the release notes? Choose one:\r\n\r\n- [ ] `rn\/breaking-change` - The PR will be mentioned in the \"Breaking Changes\" section\r\n- [ ] `rn\/none` - No description will be included. The PR will be mentioned only by the PR number in the \"Small Bugfixes and Documentation Updates\" section\r\n- [x] `rn\/feature` - A new user-facing feature worth mentioning in the release notes\r\n- [ ] `rn\/bug-fix` - A user-facing bug fix worth mentioning in the release notes\r\n- [ ] `rn\/documentation` - A user-facing documentation change worth mentioning in the release notes\r\n","pull_request":{"url":"https:\/\/api.github.com\/repos\/mlflow\/mlflow\/pulls\/5503","html_url":"https:\/\/github.com\/mlflow\/mlflow\/pull\/5503","diff_url":"https:\/\/github.com\/mlflow\/mlflow\/pull\/5503.diff","patch_url":"https:\/\/github.com\/mlflow\/mlflow\/pull\/5503.patch","merged_at":null},"labels":["rn\/feature","area\/scoring"],"labels_description":["Mention under Features in Changelogs.","MLflow Model server, model deployment tools, Spark UDFs"],"entities":["function","mlflow","pyfunc","model"]},{"html_url":"https:\/\/github.com\/mlflow\/mlflow\/pull\/5502","id":1171509651,"number":5502,"title":"append user agent added by request_header_provider to default user agent","state":"open","locked":false,"assignee":null,"assignees":[],"comments":3,"created_at":1647462178000,"updated_at":1647897485000,"closed_at":null,"body":"\r\n## What changes are proposed in this pull request?\r\n\r\nThis PR append the user agent added by request_header_provider plugin to the default user agent added by rest_utils. It fixes [this ](https:\/\/github.com\/mlflow\/mlflow\/issues\/5498)issue\r\n\r\n## How is this patch tested?\r\n\r\nRan the test locally\r\n\r\n## Does this PR change the documentation?\r\n\r\n- [X] No. You can skip the rest of this section.\r\n- [ ] Yes. Make sure the changed pages \/ sections render correctly by following the steps below.\r\n\r\n1. Check the status of the `ci\/circleci: build_doc` check. If it's successful, proceed to the\r\n   next step, otherwise fix it.\r\n2. Click `Details` on the right to open the job page of CircleCI.\r\n3. Click the `Artifacts` tab.\r\n4. Click `docs\/build\/html\/index.html`.\r\n5. Find the changed pages \/ sections and make sure they render correctly.\r\n\r\n## Release Notes\r\n\r\n### Is this a user-facing change?\r\n\r\n- [X] No. You can skip the rest of this section.\r\n- [ ] Yes. Give a description of this change to be included in the release notes for MLflow users.\r\n\r\n(Details in 1-2 sentences. You can just refer to another PR with a description if this PR is part of a larger change.)\r\n\r\n### What component(s), interfaces, languages, and integrations does this PR affect?\r\nComponents \r\n- [ ] `area\/artifacts`: Artifact stores and artifact logging\r\n- [ ] `area\/build`: Build and test infrastructure for MLflow\r\n- [ ] `area\/docs`: MLflow documentation pages\r\n- [ ] `area\/examples`: Example code\r\n- [ ] `area\/model-registry`: Model Registry service, APIs, and the fluent client calls for Model Registry\r\n- [ ] `area\/models`: MLmodel format, model serialization\/deserialization, flavors\r\n- [ ] `area\/projects`: MLproject format, project running backends\r\n- [ ] `area\/scoring`: MLflow Model server, model deployment tools, Spark UDFs\r\n- [ ] `area\/server-infra`: MLflow Tracking server backend\r\n- [X] `area\/tracking`: Tracking Service, tracking client APIs, autologging\r\n\r\nInterface \r\n- [ ] `area\/uiux`: Front-end, user experience, plotting, JavaScript, JavaScript dev server\r\n- [ ] `area\/docker`: Docker use across MLflow's components, such as MLflow Projects and MLflow Models\r\n- [ ] `area\/sqlalchemy`: Use of SQLAlchemy in the Tracking Service or Model Registry\r\n- [ ] `area\/windows`: Windows support\r\n\r\nLanguage \r\n- [ ] `language\/r`: R APIs and clients\r\n- [ ] `language\/java`: Java APIs and clients\r\n- [ ] `language\/new`: Proposals for new client languages\r\n\r\nIntegrations\r\n- [ ] `integrations\/azure`: Azure and Azure ML integrations\r\n- [ ] `integrations\/sagemaker`: SageMaker integrations\r\n- [ ] `integrations\/databricks`: Databricks integrations\r\n\r\n<!--\r\nInsert an empty named anchor here to allow jumping to this section with a fragment URL\r\n(e.g. https:\/\/github.com\/mlflow\/mlflow\/pull\/123#user-content-release-note-category).\r\nNote that GitHub prefixes anchor names in markdown with \"user-content-\".\r\n-->\r\n<a name=\"release-note-category\"><\/a>\r\n### How should the PR be classified in the release notes? Choose one:\r\n\r\n- [ ] `rn\/breaking-change` - The PR will be mentioned in the \"Breaking Changes\" section\r\n- [X] `rn\/none` - No description will be included. The PR will be mentioned only by the PR number in the \"Small Bugfixes and Documentation Updates\" section\r\n- [ ] `rn\/feature` - A new user-facing feature worth mentioning in the release notes\r\n- [ ] `rn\/bug-fix` - A user-facing bug fix worth mentioning in the release notes\r\n- [ ] `rn\/documentation` - A user-facing documentation change worth mentioning in the release notes\r\n","pull_request":{"url":"https:\/\/api.github.com\/repos\/mlflow\/mlflow\/pulls\/5502","html_url":"https:\/\/github.com\/mlflow\/mlflow\/pull\/5502","diff_url":"https:\/\/github.com\/mlflow\/mlflow\/pull\/5502.diff","patch_url":"https:\/\/github.com\/mlflow\/mlflow\/pull\/5502.patch","merged_at":null},"labels":["rn\/none","area\/tracking"],"labels_description":["List under Small Changes in Changelogs.","Tracking service, tracking client APIs, autologging"],"entities":["user","agent","request_header_provider","user","agent"]},{"html_url":"https:\/\/github.com\/mlflow\/mlflow\/issues\/5501","id":1171444528,"number":5501,"title":"[fr] add mlflow version to the mlmodel spec","state":"open","locked":false,"assignee":null,"assignees":[],"comments":5,"created_at":1647457854000,"updated_at":1647757178000,"closed_at":null,"body":"Thank you for submitting a feature request. **Before proceeding, please review MLflow's [Issue Policy for feature requests](https:\/\/www.github.com\/mlflow\/mlflow\/blob\/master\/ISSUE_POLICY.md#feature-requests) and the [MLflow Contributing Guide](https:\/\/github.com\/mlflow\/mlflow\/blob\/master\/CONTRIBUTING.rst)**.\r\n\r\n**Please fill in this feature request template to ensure a timely and thorough response.**\r\n\r\n## Willingness to contribute\r\nThe MLflow Community encourages new feature contributions. Would you or another member of your organization be willing to contribute an implementation of this feature (either as an MLflow Plugin or an enhancement to the MLflow code base)?\r\n\r\n- [ ] Yes. I can contribute this feature independently.\r\n- [ ] Yes. I would be willing to contribute this feature with guidance from the MLflow community.\r\n- [X] No. I cannot contribute this feature at this time.\r\n\r\n## Proposal Summary\r\n\r\nAdd an `mlflow_version` field to the MLModel spec that is automatically populated with the current MLflow version when an MLflow Model is saved \/ logged.\r\n\r\n## Motivation\r\n- What is the use case for this feature? When saving an MLflow Model, it is helpful to know what version of MLflow was used to save the model for backwards compatibility & validation purposes.\r\n- Why is this use case valuable to support for MLflow users in general? ^\r\n- Why is this use case valuable to support for your project(s) or organization? ^\r\n- Why is it currently difficult to achieve this use case? MLflow version information isn't available anywhere in the serialized MLflow Model format.\r\n\r\n### What component(s), interfaces, languages, and integrations does this feature affect?\r\nComponents \r\n- [ ] `area\/artifacts`: Artifact stores and artifact logging\r\n- [ ] `area\/build`: Build and test infrastructure for MLflow\r\n- [ ] `area\/docs`: MLflow documentation pages\r\n- [ ] `area\/examples`: Example code\r\n- [ ] `area\/model-registry`: Model Registry service, APIs, and the fluent client calls for Model Registry\r\n- [X] `area\/models`: MLmodel format, model serialization\/deserialization, flavors\r\n- [ ] `area\/projects`: MLproject format, project running backends\r\n- [ ] `area\/scoring`: MLflow Model server, model deployment tools, Spark UDFs\r\n- [ ] `area\/server-infra`: MLflow Tracking server backend\r\n- [ ] `area\/tracking`: Tracking Service, tracking client APIs, autologging\r\n\r\nInterfaces\r\n- [ ] `area\/uiux`: Front-end, user experience, plotting, JavaScript, JavaScript dev server\r\n- [ ] `area\/docker`: Docker use across MLflow's components, such as MLflow Projects and MLflow Models\r\n- [ ] `area\/sqlalchemy`: Use of SQLAlchemy in the Tracking Service or Model Registry\r\n- [ ] `area\/windows`: Windows support\r\n\r\nLanguages \r\n- [ ] `language\/r`: R APIs and clients\r\n- [ ] `language\/java`: Java APIs and clients\r\n- [ ] `language\/new`: Proposals for new client languages\r\n\r\nIntegrations\r\n- [ ] `integrations\/azure`: Azure and Azure ML integrations\r\n- [ ] `integrations\/sagemaker`: SageMaker integrations\r\n- [ ] `integrations\/databricks`: Databricks integrations\r\n\r\n## Details\r\n\r\n(Use this section to include any additional information about the feature. If you have a proposal for how to implement this feature, please include it here. For implementation guidelines, please refer to the [Contributing Guide](https:\/\/github.com\/mlflow\/mlflow\/blob\/master\/CONTRIBUTING.rst#contribution-guidelines).)\r\n","pull_request":null,"labels":["enhancement","area\/models"],"labels_description":["New feature or request","MLmodel format, model serialization\/deserialization, flavors"],"entities":["mlflow","version","mlmodel","spec"]},{"html_url":"https:\/\/github.com\/mlflow\/mlflow\/issues\/5498","id":1170426842,"number":5498,"title":"[bug] user-agent added by as a default header gets overwritten if request_header_provider adds the same header","state":"open","locked":false,"assignee":null,"assignees":[],"comments":2,"created_at":1647393808000,"updated_at":1647462591000,"closed_at":null,"body":"Thank you for submitting an issue. Please refer to our [issue policy](https:\/\/www.github.com\/mlflow\/mlflow\/blob\/master\/ISSUE_POLICY.md) for additional information about bug reports. For help with debugging your code, please refer to [Stack Overflow](https:\/\/stackoverflow.com\/questions\/tagged\/mlflow).\r\n\r\n**Please fill in this bug report template to ensure a timely and thorough response.**\r\n\r\n### Willingness to contribute\r\nThe MLflow Community encourages bug fix contributions. Would you or another member of your organization be willing to contribute a fix for this bug to the MLflow code base?\r\n\r\n- [ ] Yes. I can contribute a fix for this bug independently.\r\n- [X] Yes. I would be willing to contribute a fix for this bug with guidance from the MLflow community.\r\n- [ ] No. I cannot contribute a bug fix at this time.\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in MLflow)**: Custom **request_header_provider** plugin\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Any\r\n- **MLflow installed from (source or binary)**: N\/A\r\n- **MLflow version (run ``mlflow --version``)**: 1.24.0\r\n- **Python version**: 3.8\r\n- **npm version, if running the dev UI**: N\/A\r\n- **Exact command to reproduce**: N\/A\r\n\r\n### Describe the problem\r\nMLflow added capability to custom write **request_header_provider** plugin to added custom request headers. However if the request_header_provider custom plugin added header **User-Agent** it overwrites the default **User-Agent** header added [here](https:\/\/github.com\/mlflow\/mlflow\/blob\/f9e5200a3a1e8d5ec86fd4ed9776f1c2392ce18e\/mlflow\/utils\/rest_utils.py#L136) instead of appending to it.\r\n\r\n### Code to reproduce issue\r\nWrite a custom **request_header_provider** plugin and set **User-Agent** to a specific value. \r\n\r\n### Other info \/ logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\n### What component(s), interfaces, languages, and integrations does this bug affect?\r\nComponents \r\n- [ ] `area\/artifacts`: Artifact stores and artifact logging\r\n- [ ] `area\/build`: Build and test infrastructure for MLflow\r\n- [ ] `area\/docs`: MLflow documentation pages\r\n- [ ] `area\/examples`: Example code\r\n- [ ] `area\/model-registry`: Model Registry service, APIs, and the fluent client calls for Model Registry\r\n- [ ] `area\/models`: MLmodel format, model serialization\/deserialization, flavors\r\n- [ ] `area\/projects`: MLproject format, project running backends\r\n- [ ] `area\/scoring`: MLflow Model server, model deployment tools, Spark UDFs\r\n- [ ] `area\/server-infra`: MLflow Tracking server backend\r\n- [X] `area\/tracking`: Tracking Service, tracking client APIs, autologging\r\n\r\nInterface \r\n- [ ] `area\/uiux`: Front-end, user experience, plotting, JavaScript, JavaScript dev server\r\n- [ ] `area\/docker`: Docker use across MLflow's components, such as MLflow Projects and MLflow Models\r\n- [ ] `area\/sqlalchemy`: Use of SQLAlchemy in the Tracking Service or Model Registry\r\n- [ ] `area\/windows`: Windows support\r\n\r\nLanguage \r\n- [ ] `language\/r`: R APIs and clients\r\n- [ ] `language\/java`: Java APIs and clients\r\n- [ ] `language\/new`: Proposals for new client languages\r\n\r\nIntegrations\r\n- [ ] `integrations\/azure`: Azure and Azure ML integrations\r\n- [ ] `integrations\/sagemaker`: SageMaker integrations\r\n- [ ] `integrations\/databricks`: Databricks integrations\r\n","pull_request":null,"labels":["bug","area\/tracking"],"labels_description":["Something isn't working","Tracking service, tracking client APIs, autologging"],"entities":["user-agent","default","header","request_header_provider","header"]},{"html_url":"https:\/\/github.com\/mlflow\/mlflow\/issues\/5494","id":1170276023,"number":5494,"title":"[bug] unsupported conversion in pyspark udf prediction from numpy array(numpy.ndarray) to a tensor with tf custom estimator ","state":"open","locked":false,"assignee":null,"assignees":[],"comments":1,"created_at":1647378914000,"updated_at":1647404263000,"closed_at":null,"body":"Thank you for submitting an issue. Please refer to our [issue policy](https:\/\/www.github.com\/mlflow\/mlflow\/blob\/master\/ISSUE_POLICY.md) for additional information about bug reports. For help with debugging your code, please refer to [Stack Overflow](https:\/\/stackoverflow.com\/questions\/tagged\/mlflow).\r\n\r\n**Please fill in this bug report template to ensure a timely and thorough response.**\r\n\r\n### Willingness to contribute\r\nThe MLflow Community encourages bug fix contributions. Would you or another member of your organization be willing to contribute a fix for this bug to the MLflow code base?\r\n\r\n- [ ] Yes. I can contribute a fix for this bug independently.\r\n- [ ] Yes. I would be willing to contribute a fix for this bug with guidance from the MLflow community.\r\n- [X] No. I cannot contribute a bug fix at this time.\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in MLflow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: MacOS Catalina Version 10.15.5, Ubuntu 18.04.5 LTS\r\n- **MLflow installed from (source or binary)**: Source\r\n- **MLflow version (run ``mlflow --version``)**: 1.23.1\r\n- **Python version**: 3.7.5\r\n- **npm version, if running the dev UI**: N\/A\r\n- **Exact command to reproduce**: see below\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Include descriptions of the expected behavior and the actual behavior.\r\nUsing a TF simple custom estimator with the tensorflow iris data in combination with MLFlow introduces a bug when trying to perform inference via mlflow.pyfunc.spark_udf.\r\n\r\nThe input to this model is a float array of length 4 -- this is where the conversion issue stems from as numpy.ndarray is unsupported:\r\n\r\n`PythonException: An exception was thrown from a UDF: 'ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type numpy.ndarray).'. Full traceback below:`\r\n\r\n### Code to reproduce issue\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n```\r\n\r\nimport mlflow\r\nimport argparse\r\nimport sys\r\nfrom mlflow import pyfunc\r\nimport pandas as pd\r\nimport shutil\r\nimport tempfile\r\nimport tensorflow as tf\r\nimport mlflow.tensorflow\r\ntf.compat.v1.disable_eager_execution()\r\nimport pyspark.sql.functions as F\r\nfrom pyspark.sql.functions import row_number,lit\r\nfrom pyspark.sql.window import Window\r\nfrom pyspark.sql.types import *\r\nimport numpy as np\r\n\r\nfrom mlflow.models.signature import ModelSignature, infer_signature\r\nfrom mlflow.types.schema import *\r\n\r\nfrom pyspark.sql import functions as F\r\nfrom pyspark.sql.functions import struct,col, pandas_udf, PandasUDFType, struct\r\nimport pickle        \r\nfrom tensorflow.python.util import lazy_loader\r\nimport tensorflow as tf\r\nfrom tensorflow.estimator import Estimator\r\nfrom tensorflow.python import pywrap_tensorflow\r\nimport fnmatch\r\nfrom pyspark.sql.types import *\r\nfrom importlib import import_module\r\nfrom pyspark import StorageLevel\r\nimport json\r\nimport logging\r\nimport os\r\n\r\nfrom pyspark.sql import SparkSession\r\n\r\n\r\n\r\ndef generate_data(URL, COL_NAMES):\r\n  \"\"\"\r\n  Generate iris data as a spark dataframe formatted with all inputs in a single array as follows: \r\n  [\"RECIPIENT_ID\",\"DATA\"] where DATA = ['SepalLength', 'SepalWidth',\r\n                    'PetalLength', 'PetalWidth'] values\r\n              \r\n  \"\"\"\r\n  csv_train = pd.read_csv(URL, names=COL_NAMES, header=0)\r\n\r\n  w = Window().orderBy(lit('A'))\r\n  spark_train =spark.createDataFrame(csv_train)\r\n  spark_train = spark_train.withColumn('SepalLength', F.col('SepalLength').cast(FloatType())).withColumn('SepalWidth', F.col('SepalWidth').cast(FloatType())).withColumn('PetalLength', F.col('PetalLength').cast(FloatType())).withColumn('PetalWidth', F.col('PetalWidth').cast(FloatType())).withColumn('Species', F.col('Species').cast(IntegerType()))\r\n  spark_train = spark_train.withColumn('DATA', (F.array('SepalLength','SepalWidth','PetalLength','PetalWidth'))).withColumn(\"RECIPIENT_ID\", (row_number().over(w)).cast(StringType())).select('RECIPIENT_ID','DATA', 'Species')\r\n  \r\n  return spark_train\r\n\r\n\"\"\"\r\nSend time optimization using meta-learning. Currently implemented using Tensorflow and the built-in Estimator class.\r\nThe higest level class here is Trainer and the highest level method is Trainer.fit.\r\n\"\"\"\r\n\r\n# length of each individual message represented in a recipient's CONCAT_MESSAGE_REPR\r\n\r\nclass Trainer:\r\n    \"\"\"\r\n    High level class which performs the training and writes the result to storage.\r\n\r\n    :param loss_fn: str, loss function name\r\n    :param train_ratio: float, percentage of recipients in training group\r\n    :param learning_rate: float\r\n    :param batch_size: int\r\n    :param training_steps: int\r\n    \"\"\"\r\n    def __init__(self, \r\n                 train_ratio=0.9,\r\n                 learning_rate=0.0002,\r\n                 batch_size=250,\r\n                 training_steps=5000):\r\n\r\n        \r\n        self.params = {                       \r\n                       'train_ratio': train_ratio,\r\n                       'learning_rate': learning_rate,\r\n                       'batch_size': batch_size,\r\n                       'training_steps': training_steps,\r\n                       'train_location': '\/dbfs\/ml\/iris_data\/train_tf_data\/',\r\n                       'test_location': '\/dbfs\/ml\/iris_data\/test_tf_data\/',\r\n                       'model_dir': model_dir,\r\n                        # Two hidden layers of 10 nodes each.\r\n                        'hidden_units': [10, 10],\r\n                        # The model must choose between 3 classes.\r\n                        'n_classes': 3,\r\n                        }\r\n\r\n    def fit(self, training_data):\r\n        \"\"\"\r\n        Train model using training_data and save results to storage.\r\n        Converts the Spark DF to a tfrecords format to allow batch data to be\r\n        read into the estimator during training\r\n\r\n        :param training_data: pyspark dataframe with columns [RECIPIENT_ID, DATA]\r\n            RECIPIENT_ID: string\r\n            DATA: array\r\n        \"\"\"\r\n        # subset training_data and write out to tmp location \r\n        training_data.repartition(1).write.format(\"tfrecords\").option(\"recordType\", \"Example\").mode(\"overwrite\").save('file:' + self.params['train_location'])\r\n\r\n        # using Tensorflow's built in Estimator\r\n        est = Estimator(model_fn=estimator_model_fn, model_dir=self.params['model_dir'], params=self.params)\r\n\r\n\r\n        my_estimator = est.train(lambda: self._train_input_fn(self.params['batch_size']),\r\n                                  hooks=None,\r\n                                  max_steps=self.params['training_steps'],\r\n                                  saving_listeners=None)\r\n        \r\n        return my_estimator\r\n      \r\n    def _decode(self, serialized_example):\r\n        \"\"\"\r\n        Parses the DATA from the given `serialized_example`.\r\n        :param serialized_example: Raw TFRecordDataset data\r\n        :param fixed_num_message: size of DATA to be used when parsing the data\r\n        :return DATA: Parsed data column\r\n        \"\"\"\r\n        # define a parser\r\n        features = tf.io.parse_single_example(serialized_example,\r\n                                        features={'DATA': tf.io.FixedLenFeature([4], tf.float32),\r\n                                                 'Species': tf.io.FixedLenFeature((), tf.int64, default_value=0)})\r\n\r\n        # convert the data to correct type\r\n        DATA = tf.cast(features['DATA'], tf.float32)\r\n        Species =  tf.cast(features['Species'], tf.int64)\r\n        return DATA, Species\r\n      \r\n    # input_fn for training\r\n    def _train_input_fn(self, batch_size, shuffle_count=100, repeat_count=30):\r\n        \"\"\"\r\n        Input function used to read the TFRecord from a temp location, decode (parse the serialized example), and finally output a batch of data for training\r\n        :param batch size: int\r\n        :param shuffle_count: int, number of times to shuffle data\r\n        :return dataset: batch of data for training of size equivalent to batch_size\r\n        \"\"\"            \r\n        #location of all partitions of train subset data\r\n        train_files = [os.path.join(self.params['train_location'], item) for item in fnmatch.filter(os.listdir( self.params['train_location']), 'part*')]\r\n        raw_train_dataset = tf.data.TFRecordDataset(train_files)\r\n\r\n        #parse the serialized raw_train_dataset as CONCAT_MESSAGE_REPR column\r\n        dataset = raw_train_dataset.map(self._decode)\r\n        assert batch_size is not None, \"batch_size must not be None\"\r\n\r\n        #shuffle the data and return a batch of data for training corresponding to the given batch_size\r\n        dataset = dataset.shuffle(shuffle_count).repeat().batch(batch_size)\r\n        return dataset\r\n      \r\n      \r\n\r\n    # input_fn for evaluation and predictions\r\n    def _eval_input_fn(self):\r\n        \"\"\"\r\n        Input function used to read the TFRecord from a temp location, decode (parse the serialized example), and finally output the data for evaluation\r\n        :return dataset: data for model evaluation\r\n        \"\"\"        \r\n        #location of all partitions of eval subset data    \r\n        test_files = [os.path.join(self.params['train_location'], item) for item in fnmatch.filter(os.listdir( self.params['train_location']), 'part*')]\r\n        raw_test_dataset = tf.data.TFRecordDataset(test_files)\r\n        #parse the serialized raw_train_dataset as CONCAT_MESSAGE_REPR column\r\n        dataset = raw_test_dataset.map(self._decode)\r\n        return dataset\r\n\r\n    def _flat_serving_input_receiver_fn(self):\r\n        \"\"\"\r\n        Function to pass in the correct feature tensor to the tfInputGraph\r\n        Feature corresponds to DATA array data\r\n        :param fixed_num_message: size of DATA to be used when parsing the data\r\n        :return FlatServingInputReceiver: class to help create the TFTransformer object for saving model\r\n        \"\"\"\r\n\r\n        feature_tensor = tf.compat.v1.placeholder(tf.float32, [None, 4])\r\n        return FlatServingInputReceiver(feature_tensor)\r\n        \r\n\r\ndef estimator_model_fn(features, labels, mode, params):\r\n    \"\"\"\r\n    modelFn for Estimator. \r\n    Interface is designed by Databricks devs and cannot be changed.\r\n\r\n    :param features: Dict of DataFrame input column name to tensor (each tensor corresponding to\r\n                    batch of data from the input column)\r\n    :param labels: Tensor, batch of labels\r\n    :param mode: Specifies if the estimator is being run for training, evaluation or prediction.\r\n    :param params: Dict of hyperparameters. Will receive what is passed to\r\n                the Estimator in params parameter. This allows for configuring Estimators for\r\n                hyperparameter tuning.\r\n    :return: tf.estimator.EstimatorSpec describing our model.  \r\n    \"\"\"\r\n\r\n    serving_key = 'predict'\r\n  \r\n  \r\n    data = tf.reshape(features, shape=[-1, 4])\r\n    \r\n    layer1 = tf.keras.layers.Dense(3, activation='linear', input_shape=(-1, 4))\r\n    layer2 = tf.keras.layers.BatchNormalization()\r\n    layer3 = tf.keras.layers.Activation('sigmoid')\r\n    \r\n    scores = layer3(layer2(layer1(data), training=True if mode == tf.estimator.ModeKeys.TRAIN else False))\r\n    \r\n    print(\"scores:\",scores)\r\n    print(\"score shape:\", scores.shape)\r\n    \r\n    logits = scores\r\n    \r\n    print(\"logits\",logits)\r\n    print(\"labels:\", labels)\r\n    # for PREDICT mode, run model on all messages and export scores\r\n    predicted_classes = tf.argmax(logits, axis=1)\r\n    print(\"predicted_classes:\", predicted_classes)\r\n\r\n    if mode == tf.estimator.ModeKeys.PREDICT:\r\n        predictions = {\r\n            'class_ids': predicted_classes[:, tf.newaxis],\r\n            'probabilities': tf.identity(logits, name='scores_tensor'),\r\n            'logits': logits,\r\n        }\r\n        export_outputs = {serving_key: tf.estimator.export.PredictOutput(predictions)}\r\n\r\n        return tf.estimator.EstimatorSpec(mode, predictions=predictions, export_outputs=export_outputs)\r\n      \r\n\r\n    predictions = {\r\n        'class_ids': predicted_classes[:, tf.newaxis],\r\n        'probabilities': tf.identity(logits, name='scores_tensor'),\r\n        'logits': logits,\r\n    }\r\n    export_outputs = {serving_key: tf.estimator.export.PredictOutput(predictions)}\r\n\r\n # Compute loss.\r\n    loss_def = tf.keras.losses.SparseCategoricalCrossentropy() \r\n    loss = loss_def(labels, logits)\r\n    # Compute evaluation metrics.\r\n    accuracy =tf.compat.v1.metrics.accuracy(labels=labels,\r\n                                   predictions=predicted_classes,\r\n                                   name='acc_op')\r\n    metrics = {'accuracy': accuracy}\r\n    tf.summary.scalar('accuracy', accuracy[1])\r\n\r\n    if mode == tf.estimator.ModeKeys.EVAL:\r\n        return tf.estimator.EstimatorSpec(\r\n            mode, loss=loss, eval_metric_ops=metrics)\r\n\r\n    # Create training op.\r\n    assert mode == tf.estimator.ModeKeys.TRAIN\r\n\r\n    optimizer = tf.compat.v1.train.AdagradOptimizer(learning_rate=0.1)\r\n    train_op = optimizer.minimize(loss, global_step=tf.compat.v1.train.get_global_step())\r\n    return tf.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op,export_outputs=export_outputs)\r\n\r\n  # define the receiver Class which can be fed to export a saved model\r\nclass FlatServingInputReceiver(object):\r\n    \"\"\"\r\n    Define a class to use when creating input receiver function\r\n    The input received function will be sent to the tfInputGraph to create a TFTransformer object\r\n    \"\"\"\r\n    def __init__(self, feature):\r\n        self.features = feature\r\n        self.receiver_tensors = {'DATA': feature}\r\n\r\n\r\ndef _flat_serving_input_receiver_fn():\r\n    \"\"\"\r\n    Function to pass in the correct feature tensor to the tfInputGraph\r\n    Feature corresponds to CONCAT_MESSAGE_REPR array data\r\n    :param fixed_num_message: size of CONCAT_MESSAGE_REPR to be used when parsing the data\r\n    :return FlatServingInputReceiver: class to help create the TFTransformer object for saving model\r\n    \"\"\"\r\n    # calculate the length of CONCAT_MESSAGE_REPR from fixed_num_message param\r\n    feature_length = 4\r\n    feature_tensor = tf.compat.v1.placeholder(tf.float32, [None, feature_length])\r\n    return FlatServingInputReceiver(feature_tensor)\r\n\r\n```\r\n\r\n```\r\n# code to run the training and scoring\r\nmlflow.autolog()\r\ntrain_ratio=0.9\r\nlearning_rate=0.0002\r\nbatch_size=100\r\ntraining_steps=1000\r\n\r\nTRAIN_URL = \"http:\/\/download.tensorflow.org\/data\/iris_training.csv\"\r\nTEST_URL = \"http:\/\/download.tensorflow.org\/data\/iris_test.csv\"\r\n\r\nCSV_COLUMN_NAMES = ['SepalLength', 'SepalWidth',\r\n                    'PetalLength', 'PetalWidth', 'Species']\r\n\r\nspark_train = generate_data(TRAIN_URL, CSV_COLUMN_NAMES)\r\n\r\nwith mlflow.start_run(run_name=\"test_meta\") as run:\r\n    run_id = run.info.run_id\r\n    print(run_id)\r\n\r\n    model_dir = \"\/tmp\/estimator\/2\"\r\n\r\n    trainer = Trainer(\"test\",\r\n                        learning_rate=learning_rate,\r\n                        batch_size=batch_size,\r\n                        training_steps=1000)\r\n\r\n    model_est = trainer.fit(spark_train)\r\n\r\n    model_results = {}\r\n\r\n    output_dir = model_dir+'\/saved_model'\r\n    saved_model_dir = model_est.export_saved_model(output_dir, _flat_serving_input_receiver_fn, ).decode(\"utf-8\")\r\n    print(saved_model_dir)\r\n\r\n    #print(mlflow.get_artifact_uri(\"model\"))\r\n\r\n    loaded_model = mlflow.pyfunc.spark_udf(spark, model_uri = f'runs:\/{run_id}\/model', result_type='double')\r\n    print(\"loaded_model:  \")\r\n    print(loaded_model)\r\n\r\n    spark_train1 = spark_train.withColumn('predictions', loaded_model('DATA'))\r\n\r\n    spark_train1.show()\r\n\r\n```\r\n\r\n\r\n### Other info \/ logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\nError log:\r\n\r\n```\r\n\r\nPythonException: An exception was thrown from a UDF: 'ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type numpy.ndarray).'. Full traceback below:\r\n---------------------------------------------------------------------------\r\nPythonException                           Traceback (most recent call last)\r\n<[command-2793002156562455]()> in <module>\r\n     40     spark_train1 = spark_train.withColumn('predictions', loaded_model('DATA'))\r\n     41 \r\n---> 42     spark_train1.show()\r\n\r\n\/databricks\/spark\/python\/pyspark\/sql\/dataframe.py in show(self, n, truncate, vertical)\r\n    439         \"\"\"\r\n    440         if isinstance(truncate, bool) and truncate:\r\n--> 441             print(self._jdf.showString(n, 20, vertical))\r\n    442         else:\r\n    443             print(self._jdf.showString(n, int(truncate), vertical))\r\n\r\n\/databricks\/spark\/python\/lib\/py4j-0.10.9-src.zip\/py4j\/java_gateway.py in __call__(self, *args)\r\n   1303         answer = self.gateway_client.send_command(command)\r\n   1304         return_value = get_return_value(\r\n-> 1305             answer, self.gateway_client, self.target_id, self.name)\r\n   1306 \r\n   1307         for temp_arg in temp_args:\r\n\r\n\/databricks\/spark\/python\/pyspark\/sql\/utils.py in deco(*a, **kw)\r\n    131                 # Hide where the exception came from that shows a non-Pythonic\r\n    132                 # JVM exception message.\r\n--> 133                 raise_from(converted)\r\n    134             else:\r\n    135                 raise\r\n\r\n\/databricks\/spark\/python\/pyspark\/sql\/utils.py in raise_from(e)\r\n\r\nPythonException: An exception was thrown from a UDF: 'ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type numpy.ndarray).'. Full traceback below:\r\nTraceback (most recent call last):\r\n  File \"\/databricks\/spark\/python\/pyspark\/worker.py\", line 654, in main\r\n    process()\r\n  File \"\/databricks\/spark\/python\/pyspark\/worker.py\", line 646, in process\r\n    serializer.dump_stream(out_iter, outfile)\r\n  File \"\/databricks\/spark\/python\/pyspark\/sql\/pandas\/serializers.py\", line 281, in dump_stream\r\n    timely_flush_timeout_ms=self.timely_flush_timeout_ms)\r\n  File \"\/databricks\/spark\/python\/pyspark\/sql\/pandas\/serializers.py\", line 97, in dump_stream\r\n    for batch in iterator:\r\n  File \"\/databricks\/spark\/python\/pyspark\/sql\/pandas\/serializers.py\", line 271, in init_stream_yield_batches\r\n    for series in iterator:\r\n  File \"\/databricks\/spark\/python\/pyspark\/worker.py\", line 467, in mapper\r\n    result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)\r\n  File \"\/databricks\/spark\/python\/pyspark\/worker.py\", line 467, in <genexpr>\r\n    result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)\r\n  File \"\/databricks\/spark\/python\/pyspark\/worker.py\", line 111, in <lambda>\r\n    verify_result_type(f(*a)), len(a[0])), arrow_return_type)\r\n  File \"\/databricks\/spark\/python\/pyspark\/util.py\", line 109, in wrapper\r\n    return f(*args, **kwargs)\r\n  File \"\/databricks\/python\/lib\/python3.7\/site-packages\/mlflow\/pyfunc\/__init__.py\", line 856, in predict\r\n    result = model.predict(pdf)\r\n  File \"\/databricks\/python\/lib\/python3.7\/site-packages\/mlflow\/pyfunc\/__init__.py\", line 605, in predict\r\n    return self._model_impl.predict(data)\r\n  File \"\/databricks\/python\/lib\/python3.7\/site-packages\/mlflow\/tensorflow\/__init__.py\", line 493, in predict\r\n    feed_dict[df_col_name] = tensorflow.constant(val)\r\n  File \"\/databricks\/python\/lib\/python3.7\/site-packages\/tensorflow\/python\/framework\/constant_op.py\", line 264, in constant\r\n    allow_broadcast=True)\r\n  File \"\/databricks\/python\/lib\/python3.7\/site-packages\/tensorflow\/python\/framework\/constant_op.py\", line 275, in _constant_impl\r\n    return _constant_eager_impl(ctx, value, dtype, shape, verify_shape)\r\n  File \"\/databricks\/python\/lib\/python3.7\/site-packages\/tensorflow\/python\/framework\/constant_op.py\", line 300, in _constant_eager_impl\r\n    t = convert_to_eager_tensor(value, ctx, dtype)\r\n  File \"\/databricks\/python\/lib\/python3.7\/site-packages\/tensorflow\/python\/framework\/constant_op.py\", line 98, in convert_to_eager_tensor\r\n    return ops.EagerTensor(value, ctx.device_name, dtype)\r\nValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type numpy.ndarray).\r\n\r\n```\r\n\r\nCan this NumPy array be supported? Is this issue partially stemming from Custom Estimators or solely due to the incompatibility of array input?\r\n\r\n### What component(s), interfaces, languages, and integrations does this bug affect?\r\nComponents \r\n- [ ] `area\/artifacts`: Artifact stores and artifact logging\r\n- [ ] `area\/build`: Build and test infrastructure for MLflow\r\n- [X] `area\/docs`: MLflow documentation pages\r\n- [ ] `area\/examples`: Example code\r\n- [ ] `area\/model-registry`: Model Registry service, APIs, and the fluent client calls for Model Registry\r\n- [X] `area\/models`: MLmodel format, model serialization\/deserialization, flavors\r\n- [ ] `area\/projects`: MLproject format, project running backends\r\n- [X] `area\/scoring`: MLflow Model server, model deployment tools, Spark UDFs\r\n- [ ] `area\/server-infra`: MLflow Tracking server backend\r\n- [ ] `area\/tracking`: Tracking Service, tracking client APIs, autologging\r\n\r\nInterface \r\n- [ ] `area\/uiux`: Front-end, user experience, plotting, JavaScript, JavaScript dev server\r\n- [ ] `area\/docker`: Docker use across MLflow's components, such as MLflow Projects and MLflow Models\r\n- [ ] `area\/sqlalchemy`: Use of SQLAlchemy in the Tracking Service or Model Registry\r\n- [ ] `area\/windows`: Windows support\r\n\r\nLanguage \r\n- [ ] `language\/r`: R APIs and clients\r\n- [ ] `language\/java`: Java APIs and clients\r\n- [ ] `language\/new`: Proposals for new client languages\r\n\r\nIntegrations\r\n- [ ] `integrations\/azure`: Azure and Azure ML integrations\r\n- [ ] `integrations\/sagemaker`: SageMaker integrations\r\n- [X] `integrations\/databricks`: Databricks integrations\r\n","pull_request":null,"labels":["bug","area\/docs","area\/models","area\/scoring","integrations\/databricks"],"labels_description":["Something isn't working","Documentation issues","MLmodel format, model serialization\/deserialization, flavors","MLflow Model server, model deployment tools, Spark UDFs","Databricks integrations"],"entities":["conversion","pyspark","udf","prediction","tensor","custom","estimator"]},{"html_url":"https:\/\/github.com\/mlflow\/mlflow\/pull\/5491","id":1168808161,"number":5491,"title":"[docs] deploy a python_function model on microsoft azure ml - guidance","state":"open","locked":false,"assignee":null,"assignees":[],"comments":1,"created_at":1647285841000,"updated_at":1647892407000,"closed_at":null,"body":"## What changes are proposed in this pull request?\r\n\r\nCurrent documentation for [Deploy a python_function model on Microsoft Azure ML](https:\/\/www.mlflow.org\/docs\/latest\/models.html#id50) suggests the use of `mlflow.azureml.deploy` method to deploy mlflow models into Azure. However, such method is deprecated as indicated on [its documentation](https:\/\/www.mlflow.org\/docs\/latest\/python_api\/mlflow.azureml.html#mlflow.azureml.deploy). The current recommended way to deploy models is using an MLflow plug in `azureml-mlflow`. Documentation needs to be updated in order to reflect that.\r\n\r\n## How is this patch tested?\r\n\r\nDocumentation has been updated accordingly.\r\n\r\n## Does this PR change the documentation?\r\n\r\n- [ ] No. You can skip the rest of this section.\r\n- [x] Yes. Make sure the changed pages \/ sections render correctly by following the steps below.\r\n\r\n1. Check the status of the `ci\/circleci: build_doc` check. If it's successful, proceed to the\r\n   next step, otherwise fix it.\r\n2. Click `Details` on the right to open the job page of CircleCI.\r\n3. Click the `Artifacts` tab.\r\n4. Click `docs\/build\/html\/index.html`.\r\n5. Find the changed pages \/ sections and make sure they render correctly.\r\n\r\n## Release Notes\r\n\r\n### Is this a user-facing change?\r\n\r\n- [ ] No. You can skip the rest of this section.\r\n- [x] Yes. Give a description of this change to be included in the release notes for MLflow users.\r\n\r\n(Details in 1-2 sentences. You can just refer to another PR with a description if this PR is part of a larger change.)\r\n\r\n### What component(s), interfaces, languages, and integrations does this PR affect?\r\nComponents \r\n- [ ] `area\/artifacts`: Artifact stores and artifact logging\r\n- [ ] `area\/build`: Build and test infrastructure for MLflow\r\n- [x] `area\/docs`: MLflow documentation pages\r\n- [ ] `area\/examples`: Example code\r\n- [ ] `area\/model-registry`: Model Registry service, APIs, and the fluent client calls for Model Registry\r\n- [ ] `area\/models`: MLmodel format, model serialization\/deserialization, flavors\r\n- [ ] `area\/projects`: MLproject format, project running backends\r\n- [ ] `area\/scoring`: MLflow Model server, model deployment tools, Spark UDFs\r\n- [ ] `area\/server-infra`: MLflow Tracking server backend\r\n- [ ] `area\/tracking`: Tracking Service, tracking client APIs, autologging\r\n\r\nInterface \r\n- [ ] `area\/uiux`: Front-end, user experience, plotting, JavaScript, JavaScript dev server\r\n- [ ] `area\/docker`: Docker use across MLflow's components, such as MLflow Projects and MLflow Models\r\n- [ ] `area\/sqlalchemy`: Use of SQLAlchemy in the Tracking Service or Model Registry\r\n- [ ] `area\/windows`: Windows support\r\n\r\nLanguage \r\n- [ ] `language\/r`: R APIs and clients\r\n- [ ] `language\/java`: Java APIs and clients\r\n- [ ] `language\/new`: Proposals for new client languages\r\n\r\nIntegrations\r\n- [x] `integrations\/azure`: Azure and Azure ML integrations\r\n- [ ] `integrations\/sagemaker`: SageMaker integrations\r\n- [ ] `integrations\/databricks`: Databricks integrations\r\n\r\n<!--\r\nInsert an empty named anchor here to allow jumping to this section with a fragment URL\r\n(e.g. https:\/\/github.com\/mlflow\/mlflow\/pull\/123#user-content-release-note-category).\r\nNote that GitHub prefixes anchor names in markdown with \"user-content-\".\r\n-->\r\n<a name=\"release-note-category\"><\/a>\r\n### How should the PR be classified in the release notes? Choose one:\r\n\r\n- [ ] `rn\/breaking-change` - The PR will be mentioned in the \"Breaking Changes\" section\r\n- [ ] `rn\/none` - No description will be included. The PR will be mentioned only by the PR number in the \"Small Bugfixes and Documentation Updates\" section\r\n- [ ] `rn\/feature` - A new user-facing feature worth mentioning in the release notes\r\n- [ ] `rn\/bug-fix` - A user-facing bug fix worth mentioning in the release notes\r\n- [x] `rn\/documentation` - A user-facing documentation change worth mentioning in the release notes\r\n","pull_request":{"url":"https:\/\/api.github.com\/repos\/mlflow\/mlflow\/pulls\/5491","html_url":"https:\/\/github.com\/mlflow\/mlflow\/pull\/5491","diff_url":"https:\/\/github.com\/mlflow\/mlflow\/pull\/5491.diff","patch_url":"https:\/\/github.com\/mlflow\/mlflow\/pull\/5491.patch","merged_at":null},"labels":["area\/docs","rn\/documentation","integrations\/azure"],"labels_description":["Documentation issues","Mention under Documentation Changes in Changelogs.","Azure and Azure ML integrations"],"entities":["python_function","model","guidance"]},{"html_url":"https:\/\/github.com\/mlflow\/mlflow\/issues\/5490","id":1168573860,"number":5490,"title":"[bug] keyerror: 'metadata'  - kubernetes.py","state":"open","locked":false,"assignee":null,"assignees":[],"comments":8,"created_at":1647273061000,"updated_at":1647481406000,"closed_at":null,"body":"Thank you for submitting an issue. Please refer to our [issue policy](https:\/\/www.github.com\/mlflow\/mlflow\/blob\/master\/ISSUE_POLICY.md) for additional information about bug reports. For help with debugging your code, please refer to [Stack Overflow](https:\/\/stackoverflow.com\/questions\/tagged\/mlflow).\r\n\r\n**Please fill in this bug report template to ensure a timely and thorough response.**\r\n\r\n### Willingness to contribute\r\nThe MLflow Community encourages bug fix contributions. Would you or another member of your organization be willing to contribute a fix for this bug to the MLflow code base?\r\n\r\n- [ ] Yes. I can contribute a fix for this bug independently.\r\n- [x] Yes. I would be willing to contribute a fix for this bug with guidance from the MLflow community.\r\n- [ ] No. I cannot contribute a bug fix at this time.\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in MLflow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **MLflow installed from (source or binary)**:\r\n- **MLflow version (run ``mlflow --version``)**:\r\n- **Python version**:\r\n- **npm version, if running the dev UI**:\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Include descriptions of the expected behavior and the actual behavior.\r\n\r\n### Code to reproduce issue\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n### Other info \/ logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\n\r\n### What component(s), interfaces, languages, and integrations does this bug affect?\r\nComponents \r\n- [ ] `area\/artifacts`: Artifact stores and artifact logging\r\n- [ ] `area\/build`: Build and test infrastructure for MLflow\r\n- [ ] `area\/docs`: MLflow documentation pages\r\n- [ ] `area\/examples`: Example code\r\n- [ ] `area\/model-registry`: Model Registry service, APIs, and the fluent client calls for Model Registry\r\n- [ ] `area\/models`: MLmodel format, model serialization\/deserialization, flavors\r\n- [x] `area\/projects`: MLproject format, project running backends\r\n- [ ] `area\/scoring`: MLflow Model server, model deployment tools, Spark UDFs\r\n- [ ] `area\/server-infra`: MLflow Tracking server backend\r\n- [ ] `area\/tracking`: Tracking Service, tracking client APIs, autologging\r\n\r\nInterface \r\n- [ ] `area\/uiux`: Front-end, user experience, plotting, JavaScript, JavaScript dev server\r\n- [x] `area\/docker`: Docker use across MLflow's components, such as MLflow Projects and MLflow Models\r\n- [ ] `area\/sqlalchemy`: Use of SQLAlchemy in the Tracking Service or Model Registry\r\n- [ ] `area\/windows`: Windows support\r\n\r\nLanguage \r\n- [ ] `language\/r`: R APIs and clients\r\n- [ ] `language\/java`: Java APIs and clients\r\n- [ ] `language\/new`: Proposals for new client languages\r\n\r\nIntegrations\r\n- [ ] `integrations\/azure`: Azure and Azure ML integrations\r\n- [ ] `integrations\/sagemaker`: SageMaker integrations\r\n- [ ] `integrations\/databricks`: Databricks integrations\r\n\r\n\r\n\r\nHello there, \r\n\r\nI'm trying to run a training job packaged with MLFlow.  I followed all the steps written [here](https:\/\/mlflow.org\/docs\/latest\/projects.html#run-an-mlflow-project-on-kubernetes-experimental) but when I do : \r\n\r\n`mlflow run <project_uri> --backend kubernetes --backend-config my\/path\/to\/kubernetes_config.json`\r\n\r\nI get the following error: \r\n\r\n```\r\n2022\/03\/14 16:41:31 INFO mlflow.projects.docker: === Building docker image MY_IMAGE_NAME:c190677 ===\r\n2022\/03\/14 16:41:32 INFO mlflow.projects.kubernetes: === Pushing docker image MY_MLPROJECT_NAME:c190677 ===\r\n2022\/03\/14 16:41:37 INFO mlflow.projects.utils: === Created directory \/var\/folders\/sq\/v5232vgn40zcsvcspx7xgdcm0000gn\/T\/tmp3ecls9ej for downloading remote URIs passed to arguments of type 'path' ===\r\n2022\/03\/14 16:41:37 INFO mlflow.projects.kubernetes: === Creating Job MY_MLPROJECT_NAME-2022-03-14-16-41-37-575621 ===\r\nTraceback (most recent call last):\r\n  File \"\/Users\/antoinekrajnc\/opt\/anaconda3\/bin\/mlflow\", line 8, in <module>\r\n    sys.exit(cli())\r\n  File \"\/Users\/antoinekrajnc\/opt\/anaconda3\/lib\/python3.8\/site-packages\/click\/core.py\", line 829, in __call__\r\n    return self.main(*args, **kwargs)\r\n  File \"\/Users\/antoinekrajnc\/opt\/anaconda3\/lib\/python3.8\/site-packages\/click\/core.py\", line 782, in main\r\n    rv = self.invoke(ctx)\r\n  File \"\/Users\/antoinekrajnc\/opt\/anaconda3\/lib\/python3.8\/site-packages\/click\/core.py\", line 1259, in invoke\r\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\r\n  File \"\/Users\/antoinekrajnc\/opt\/anaconda3\/lib\/python3.8\/site-packages\/click\/core.py\", line 1066, in invoke\r\n    return ctx.invoke(self.callback, **ctx.params)\r\n  File \"\/Users\/antoinekrajnc\/opt\/anaconda3\/lib\/python3.8\/site-packages\/click\/core.py\", line 610, in invoke\r\n    return callback(*args, **kwargs)\r\n  File \"\/Users\/antoinekrajnc\/opt\/anaconda3\/lib\/python3.8\/site-packages\/mlflow\/cli.py\", line 168, in run\r\n    projects.run(\r\n  File \"\/Users\/antoinekrajnc\/opt\/anaconda3\/lib\/python3.8\/site-packages\/mlflow\/projects\/__init__.py\", line 293, in run\r\n    submitted_run_obj = _run(\r\n  File \"\/Users\/antoinekrajnc\/opt\/anaconda3\/lib\/python3.8\/site-packages\/mlflow\/projects\/__init__.py\", line 152, in _run\r\n    submitted_run = kb.run_kubernetes_job(\r\n  File \"\/Users\/antoinekrajnc\/opt\/anaconda3\/lib\/python3.8\/site-packages\/mlflow\/projects\/kubernetes.py\", line 82, in run_kubernetes_job\r\n    job_template = _get_kubernetes_job_definition(\r\n  File \"\/Users\/antoinekrajnc\/opt\/anaconda3\/lib\/python3.8\/site-packages\/mlflow\/projects\/kubernetes.py\", line 44, in _get_kubernetes_job_definition\r\n    job_template[\"metadata\"][\"name\"] = job_name\r\nKeyError: 'metadata'\r\n```\r\n\r\n- [x] I have configured a `kubernetes_config.json`\r\n- [x] I used the right kubernetes context \r\n\r\nAnyway, anyone has any idea on how to debug this? \r\n\r\nThanks a lot!","pull_request":null,"labels":["bug","area\/projects","area\/docker"],"labels_description":["Something isn't working","MLproject format, project running backends","Docker use anywhere, such as MLprojects and MLmodels"],"entities":[]},{"html_url":"https:\/\/github.com\/mlflow\/mlflow\/pull\/5487","id":1167755428,"number":5487,"title":"restoring virtual python environment for mlflow pyfunc spark_udf","state":"open","locked":false,"assignee":null,"assignees":[],"comments":0,"created_at":1647219629000,"updated_at":1647872147000,"closed_at":null,"body":"Signed-off-by: Weichen Xu <weichen.xu@databricks.com>\r\n\r\n## What changes are proposed in this pull request?\r\n\r\nNew PR instead of https:\/\/github.com\/mlflow\/mlflow\/pull\/5470\r\n\r\n## How is this patch tested?\r\n\r\n* (WIP) Test on spark local mode, will add unit tests in mlflow repo.\r\n* (WIP) Test on spark cluster mode, NFS available, will add DriverDustTest tests in databricks\/universe.\r\n* (WIP) Test on spark cluster mode, NFS unavailable, will add DriverDustTest tests in databricks\/universe.\r\n\r\n## Does this PR change the documentation?\r\n\r\n- [ ] No. You can skip the rest of this section.\r\n- [ ] Yes. Make sure the changed pages \/ sections render correctly by following the steps below.\r\n\r\n1. Check the status of the `ci\/circleci: build_doc` check. If it's successful, proceed to the\r\n   next step, otherwise fix it.\r\n2. Click `Details` on the right to open the job page of CircleCI.\r\n3. Click the `Artifacts` tab.\r\n4. Click `docs\/build\/html\/index.html`.\r\n5. Find the changed pages \/ sections and make sure they render correctly.\r\n\r\n## Release Notes\r\n\r\n### Is this a user-facing change?\r\n\r\n- [ ] No. You can skip the rest of this section.\r\n- [ ] Yes. Give a description of this change to be included in the release notes for MLflow users.\r\n\r\n(Details in 1-2 sentences. You can just refer to another PR with a description if this PR is part of a larger change.)\r\n\r\n### What component(s), interfaces, languages, and integrations does this PR affect?\r\nComponents \r\n- [ ] `area\/artifacts`: Artifact stores and artifact logging\r\n- [ ] `area\/build`: Build and test infrastructure for MLflow\r\n- [ ] `area\/docs`: MLflow documentation pages\r\n- [ ] `area\/examples`: Example code\r\n- [ ] `area\/model-registry`: Model Registry service, APIs, and the fluent client calls for Model Registry\r\n- [ ] `area\/models`: MLmodel format, model serialization\/deserialization, flavors\r\n- [ ] `area\/projects`: MLproject format, project running backends\r\n- [ ] `area\/scoring`: MLflow Model server, model deployment tools, Spark UDFs\r\n- [ ] `area\/server-infra`: MLflow Tracking server backend\r\n- [ ] `area\/tracking`: Tracking Service, tracking client APIs, autologging\r\n\r\nInterface \r\n- [ ] `area\/uiux`: Front-end, user experience, plotting, JavaScript, JavaScript dev server\r\n- [ ] `area\/docker`: Docker use across MLflow's components, such as MLflow Projects and MLflow Models\r\n- [ ] `area\/sqlalchemy`: Use of SQLAlchemy in the Tracking Service or Model Registry\r\n- [ ] `area\/windows`: Windows support\r\n\r\nLanguage \r\n- [ ] `language\/r`: R APIs and clients\r\n- [ ] `language\/java`: Java APIs and clients\r\n- [ ] `language\/new`: Proposals for new client languages\r\n\r\nIntegrations\r\n- [ ] `integrations\/azure`: Azure and Azure ML integrations\r\n- [ ] `integrations\/sagemaker`: SageMaker integrations\r\n- [ ] `integrations\/databricks`: Databricks integrations\r\n\r\n<!--\r\nInsert an empty named anchor here to allow jumping to this section with a fragment URL\r\n(e.g. https:\/\/github.com\/mlflow\/mlflow\/pull\/123#user-content-release-note-category).\r\nNote that GitHub prefixes anchor names in markdown with \"user-content-\".\r\n-->\r\n<a name=\"release-note-category\"><\/a>\r\n### How should the PR be classified in the release notes? Choose one:\r\n\r\n- [ ] `rn\/breaking-change` - The PR will be mentioned in the \"Breaking Changes\" section\r\n- [ ] `rn\/none` - No description will be included. The PR will be mentioned only by the PR number in the \"Small Bugfixes and Documentation Updates\" section\r\n- [ ] `rn\/feature` - A new user-facing feature worth mentioning in the release notes\r\n- [ ] `rn\/bug-fix` - A user-facing bug fix worth mentioning in the release notes\r\n- [ ] `rn\/documentation` - A user-facing documentation change worth mentioning in the release notes\r\n","pull_request":{"url":"https:\/\/api.github.com\/repos\/mlflow\/mlflow\/pulls\/5487","html_url":"https:\/\/github.com\/mlflow\/mlflow\/pull\/5487","diff_url":"https:\/\/github.com\/mlflow\/mlflow\/pull\/5487.diff","patch_url":"https:\/\/github.com\/mlflow\/mlflow\/pull\/5487.patch","merged_at":null},"labels":[],"labels_description":[],"entities":["python","environment","pyfunc","spark_udf"]},{"html_url":"https:\/\/github.com\/mlflow\/mlflow\/pull\/5484","id":1167144203,"number":5484,"title":"[draft] go client support","state":"open","locked":false,"assignee":null,"assignees":[],"comments":4,"created_at":1647050607000,"updated_at":1647404997000,"closed_at":null,"body":"## What changes are proposed in this pull request?\r\n\r\nImplement go SDK support as requested in #1792\r\n\r\n## How is this patch tested?\r\n\r\nTODO, I need to use the generated code in my code\r\n\r\n## Does this PR change the documentation?\r\n\r\n- [x] No. You can skip the rest of this section.\r\n- [ ] Yes. Make sure the changed pages \/ sections render correctly by following the steps below.\r\n\r\n1. Check the status of the `ci\/circleci: build_doc` check. If it's successful, proceed to the\r\n   next step, otherwise fix it.\r\n2. Click `Details` on the right to open the job page of CircleCI.\r\n3. Click the `Artifacts` tab.\r\n4. Click `docs\/build\/html\/index.html`.\r\n5. Find the changed pages \/ sections and make sure they render correctly.\r\n\r\n## Release Notes\r\n\r\n### Is this a user-facing change?\r\n\r\n- [x] No. You can skip the rest of this section.\r\n- [ ] Yes. Give a description of this change to be included in the release notes for MLflow users.\r\n\r\n(Details in 1-2 sentences. You can just refer to another PR with a description if this PR is part of a larger change.)\r\n\r\nLanguage \r\n- [ ] `language\/r`: R APIs and clients\r\n- [ ] `language\/java`: Java APIs and clients\r\n- [x] `language\/new`: Proposals for new client languages\r\n\r\n","pull_request":{"url":"https:\/\/api.github.com\/repos\/mlflow\/mlflow\/pulls\/5484","html_url":"https:\/\/github.com\/mlflow\/mlflow\/pull\/5484","diff_url":"https:\/\/github.com\/mlflow\/mlflow\/pull\/5484.diff","patch_url":"https:\/\/github.com\/mlflow\/mlflow\/pull\/5484.patch","merged_at":null},"labels":["language\/new"],"labels_description":["Proposals for new client languages"],"entities":["client","support"]},{"html_url":"https:\/\/github.com\/mlflow\/mlflow\/pull\/5481","id":1166219938,"number":5481,"title":"feat: pyspark autologging enhancement","state":"open","locked":false,"assignee":null,"assignees":[],"comments":2,"created_at":1646992637000,"updated_at":1647479645000,"closed_at":null,"body":"## What changes are proposed in this pull request?\r\n\r\nAdd general support for pyspark.ml autologging by incorporating logs of parameters of type pyspark.ml.param.Params for the estimator. Updated logged hierarchy to include those Params-type parameters as well.\r\nFixed error message for spark._validate_model.\r\n\r\n## How is this patch tested?\r\n\r\nTested all existing pyspark.ml autologging tests and tested packed python wheel package in databricks.\r\n\r\n## Does this PR change the documentation?\r\n\r\n- [x] No. You can skip the rest of this section.\r\n- [ ] Yes. Make sure the changed pages \/ sections render correctly by following the steps below.\r\n\r\n1. Check the status of the `ci\/circleci: build_doc` check. If it's successful, proceed to the\r\n   next step, otherwise fix it.\r\n2. Click `Details` on the right to open the job page of CircleCI.\r\n3. Click the `Artifacts` tab.\r\n4. Click `docs\/build\/html\/index.html`.\r\n5. Find the changed pages \/ sections and make sure they render correctly.\r\n\r\n## Release Notes\r\n\r\n### Is this a user-facing change?\r\n\r\n- [ ] No. You can skip the rest of this section.\r\n- [x] Yes. Give a description of this change to be included in the release notes for MLflow users.\r\n\r\nSupport autologging for estimators whose params contain parameter of type pyspark.ml.param.Params.\r\n\r\n### What component(s), interfaces, languages, and integrations does this PR affect?\r\nComponents \r\n- [ ] `area\/artifacts`: Artifact stores and artifact logging\r\n- [ ] `area\/build`: Build and test infrastructure for MLflow\r\n- [ ] `area\/docs`: MLflow documentation pages\r\n- [ ] `area\/examples`: Example code\r\n- [ ] `area\/model-registry`: Model Registry service, APIs, and the fluent client calls for Model Registry\r\n- [ ] `area\/models`: MLmodel format, model serialization\/deserialization, flavors\r\n- [ ] `area\/projects`: MLproject format, project running backends\r\n- [ ] `area\/scoring`: MLflow Model server, model deployment tools, Spark UDFs\r\n- [ ] `area\/server-infra`: MLflow Tracking server backend\r\n- [x] `area\/tracking`: Tracking Service, tracking client APIs, autologging\r\n\r\nInterface \r\n- [ ] `area\/uiux`: Front-end, user experience, plotting, JavaScript, JavaScript dev server\r\n- [ ] `area\/docker`: Docker use across MLflow's components, such as MLflow Projects and MLflow Models\r\n- [ ] `area\/sqlalchemy`: Use of SQLAlchemy in the Tracking Service or Model Registry\r\n- [ ] `area\/windows`: Windows support\r\n\r\nLanguage \r\n- [ ] `language\/r`: R APIs and clients\r\n- [ ] `language\/java`: Java APIs and clients\r\n- [ ] `language\/new`: Proposals for new client languages\r\n\r\nIntegrations\r\n- [ ] `integrations\/azure`: Azure and Azure ML integrations\r\n- [ ] `integrations\/sagemaker`: SageMaker integrations\r\n- [ ] `integrations\/databricks`: Databricks integrations\r\n\r\n<!--\r\nInsert an empty named anchor here to allow jumping to this section with a fragment URL\r\n(e.g. https:\/\/github.com\/mlflow\/mlflow\/pull\/123#user-content-release-note-category).\r\nNote that GitHub prefixes anchor names in markdown with \"user-content-\".\r\n-->\r\n<a name=\"release-note-category\"><\/a>\r\n### How should the PR be classified in the release notes? Choose one:\r\n\r\n- [ ] `rn\/breaking-change` - The PR will be mentioned in the \"Breaking Changes\" section\r\n- [ ] `rn\/none` - No description will be included. The PR will be mentioned only by the PR number in the \"Small Bugfixes and Documentation Updates\" section\r\n- [x] `rn\/feature` - A new user-facing feature worth mentioning in the release notes\r\n- [ ] `rn\/bug-fix` - A user-facing bug fix worth mentioning in the release notes\r\n- [ ] `rn\/documentation` - A user-facing documentation change worth mentioning in the release notes\r\n","pull_request":{"url":"https:\/\/api.github.com\/repos\/mlflow\/mlflow\/pulls\/5481","html_url":"https:\/\/github.com\/mlflow\/mlflow\/pull\/5481","diff_url":"https:\/\/github.com\/mlflow\/mlflow\/pull\/5481.diff","patch_url":"https:\/\/github.com\/mlflow\/mlflow\/pull\/5481.patch","merged_at":null},"labels":["rn\/feature","area\/tracking"],"labels_description":["Mention under Features in Changelogs.","Tracking service, tracking client APIs, autologging"],"entities":["pyspark","autologging","enhancement"]},{"html_url":"https:\/\/github.com\/mlflow\/mlflow\/issues\/5474","id":1164029387,"number":5474,"title":"[bug] a problem with type checking for string objects (mlflow-deployed model in sagemaker)","state":"open","locked":false,"assignee":null,"assignees":[],"comments":5,"created_at":1646836971000,"updated_at":1647419463000,"closed_at":null,"body":"Thank you for submitting an issue. Please refer to our [issue policy](https:\/\/www.github.com\/mlflow\/mlflow\/blob\/master\/ISSUE_POLICY.md) for additional information about bug reports. For help with debugging your code, please refer to [Stack Overflow](https:\/\/stackoverflow.com\/questions\/tagged\/mlflow).\r\n\r\n**Please fill in this bug report template to ensure a timely and thorough response.**\r\n\r\n### Willingness to contribute\r\nThe MLflow Community encourages bug fix contributions. Would you or another member of your organization be willing to contribute a fix for this bug to the MLflow code base?\r\n\r\n- [ ] Yes. I can contribute a fix for this bug independently.\r\n- [x] Yes. I would be willing to contribute a fix for this bug with guidance from the MLflow community.\r\n- [ ] No. I cannot contribute a bug fix at this time.\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in MLflow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: 'Linux', '4.14.252-131.483.amzn1.x86_64'\r\n- **MLflow installed from (source or binary)**: `pip`\r\n- **MLflow version (run ``mlflow --version``)**: `mlflow==1.22.0`\r\n- **Python version**: `python=3.6.10`\r\n- **npm version, if running the dev UI**: -\r\n- **Exact command to reproduce**: -\r\n\r\n\r\n\r\n### Describe the problem\r\n\r\nI deployed a Huggingface Transformer model in SageMaker using MLflow's `sagemaker.deploy()`.\r\n\r\nThe model had been tested after training (using the same test example that was used in the code that led to the described bug).\r\n\r\nWhen logging the model I used `infer_signature(np.array(test_example), loaded_model.predict(test_example))` to infer input and output signatures.\r\n\r\nModel is deployed successfully. When trying to query the model I get `ModelError` (full traceback below).\r\n\r\nTo query the model, I am using precisely the same test_example that I used for `infer_signature()`:\r\n\r\n`test_example = [['This is the subject', 'This is the body']]`\r\n\r\nThe only difference is that when querying the deployed model, I am not wrapping the test example in `np.array()` as that is not `json`-serializeable.\r\n\r\nTo query the model I tried two different approaches:\r\n\r\n```python\r\nimport json\r\nimport boto3\r\nimport pandas as pd\r\n\r\nSAGEMAKER_REGION = 'us-west-2'\r\nMODEL_NAME = '...'\r\n\r\nclient = boto3.client(\"sagemaker-runtime\", region_name=SAGEMAKER_REGION)\r\n\r\ntest_example = [['This is the subject', 'This is the body']]\r\n\r\n# Approach 1\r\nclient.invoke_endpoint(\r\n                EndpointName=MODEL_NAME,\r\n                Body=json.dumps(test_example),\r\n                ContentType=\"application\/json\",\r\n            )\r\n\r\n# Approach 2\r\nclient.invoke_endpoint(\r\n                EndpointName=MODEL_NAME,\r\n                Body=pd.DataFrame(test_example).to_json(orient=\"split\"),\r\n                ContentType=\"application\/json; format=pandas-split\",\r\n            )\r\n```\r\n            \r\nbut they result in the same error.\r\n\r\nTo check if the problem is not in the model itself or in other components, I built a simple workaround. \r\n\r\nI encoded strings into numbers (using `ord()`) and then decoded them back to strings (using `chr()`) inside the model wrapper. This solved the issue. \r\n\r\nSummarizing, the same code worked for integer data, but not for string data.\r\n\r\n### Code to reproduce issue\r\n---\r\n\r\n### Other info \/ logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\n```python\r\n---------------------------------------------------------------------------\r\nModelError                                Traceback (most recent call last)\r\n<ipython-input-89-d09862a5f494> in <module>\r\n      2                 EndpointName=MODEL_NAME,\r\n      3                 Body=test_example,\r\n----> 4                 ContentType=\"application\/json; format=pandas-split\",\r\n      5             )\r\n\r\n~\/anaconda3\/envs\/amazonei_tensorflow_p36\/lib\/python3.6\/site-packages\/botocore\/client.py in _api_call(self, *args, **kwargs)\r\n    393                     \"%s() only accepts keyword arguments.\" % py_operation_name)\r\n    394             # The \"self\" in this scope is referring to the BaseClient.\r\n--> 395             return self._make_api_call(operation_name, kwargs)\r\n    396 \r\n    397         _api_call.__name__ = str(py_operation_name)\r\n\r\n~\/anaconda3\/envs\/amazonei_tensorflow_p36\/lib\/python3.6\/site-packages\/botocore\/client.py in _make_api_call(self, operation_name, api_params)\r\n    723             error_code = parsed_response.get(\"Error\", {}).get(\"Code\")\r\n    724             error_class = self.exceptions.from_code(error_code)\r\n--> 725             raise error_class(parsed_response, operation_name)\r\n    726         else:\r\n    727             return parsed_response\r\n\r\nModelError: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received client error (400) from primary with message \"{\"error_code\": \"BAD_REQUEST\", \"message\": \"dtype of input object does not match expected dtype <U0\"}\". See https:\/\/us-west-2.console.aws.amazon.com\/cloudwatch\/home?region=us-west-2#logEventViewer:group=\/aws\/sagemaker\/Endpoints\/bec-sagemaker-model-test-app in account 543052680787 for more information.\r\n```\r\n\r\nEnvironment info:\r\n```\r\n{'channels': ['defaults', 'conda-forge', 'pytorch'],\r\n 'dependencies': ['python=3.6.10',\r\n  'pip==21.3.1',\r\n  'pytorch=1.10.2',\r\n  'cudatoolkit=10.2',\r\n  {'pip': ['mlflow==1.22.0',\r\n    'transformers==4.17.0',\r\n    'datasets==1.18.4',\r\n    'cloudpickle==1.3.0']}],\r\n 'name': 'bert_bec_test_env'}\r\n```\r\n\r\n### What component(s), interfaces, languages, and integrations does this bug affect?\r\nComponents \r\n- [ ] `area\/artifacts`: Artifact stores and artifact logging\r\n- [ ] `area\/build`: Build and test infrastructure for MLflow\r\n- [ ] `area\/docs`: MLflow documentation pages\r\n- [ ] `area\/examples`: Example code\r\n- [ ] `area\/model-registry`: Model Registry service, APIs, and the fluent client calls for Model Registry\r\n- [ ] `area\/models`: MLmodel format, model serialization\/deserialization, flavors\r\n- [ ] `area\/projects`: MLproject format, project running backends\r\n- [ ] `area\/scoring`: MLflow Model server, model deployment tools, Spark UDFs\r\n- [ ] `area\/server-infra`: MLflow Tracking server backend\r\n- [ ] `area\/tracking`: Tracking Service, tracking client APIs, autologging\r\n\r\nInterface \r\n- [ ] `area\/uiux`: Front-end, user experience, plotting, JavaScript, JavaScript dev server\r\n- [ ] `area\/docker`: Docker use across MLflow's components, such as MLflow Projects and MLflow Models\r\n- [ ] `area\/sqlalchemy`: Use of SQLAlchemy in the Tracking Service or Model Registry\r\n- [ ] `area\/windows`: Windows support\r\n\r\nLanguage \r\n- [ ] `language\/r`: R APIs and clients\r\n- [ ] `language\/java`: Java APIs and clients\r\n- [ ] `language\/new`: Proposals for new client languages\r\n\r\nIntegrations\r\n- [ ] `integrations\/azure`: Azure and Azure ML integrations\r\n- [x] `integrations\/sagemaker`: SageMaker integrations\r\n- [ ] `integrations\/databricks`: Databricks integrations\r\n","pull_request":null,"labels":["bug","integrations\/sagemaker"],"labels_description":["Something isn't working","Sagemaker integrations"],"entities":["problem","type","string","model"]},{"html_url":"https:\/\/github.com\/mlflow\/mlflow\/issues\/5469","id":1162308213,"number":5469,"title":"[bug]requiring the length of values for each input to be same seems too strict","state":"open","locked":false,"assignee":null,"assignees":[],"comments":3,"created_at":1646723941000,"updated_at":1647405901000,"closed_at":null,"body":"Thank you for submitting an issue. Please refer to our [issue policy](https:\/\/www.github.com\/mlflow\/mlflow\/blob\/master\/ISSUE_POLICY.md) for additional information about bug reports. For help with debugging your code, please refer to [Stack Overflow](https:\/\/stackoverflow.com\/questions\/tagged\/mlflow).\r\n\r\n**Please fill in this bug report template to ensure a timely and thorough response.**\r\n\r\n### Willingness to contribute\r\nThe MLflow Community encourages bug fix contributions. Would you or another member of your organization be willing to contribute a fix for this bug to the MLflow code base?\r\n\r\n- [ ] Yes. I can contribute a fix for this bug independently.\r\n- [ ] Yes. I would be willing to contribute a fix for this bug with guidance from the MLflow community.\r\n- [x] No. I cannot contribute a bug fix at this time.\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in MLflow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Ubuntu 18.04.4\r\n- **MLflow installed from (source or binary)**:binary\r\n- **MLflow version (run ``mlflow --version``)**:1.19.1\r\n- **Python version**:Python 3.6.10\r\n- **npm version, if running the dev UI**:N\/A\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\nThought this sanity check is too strict.\r\nhttps:\/\/github.com\/mlflow\/mlflow\/blob\/5784e7e833385e59cd194fd63e4ae5e456abd779\/mlflow\/utils\/proto_json_utils.py#L347\r\n\r\nBased on the official documentation of tensorflow RESTful API https:\/\/www.tensorflow.org\/tfx\/serving\/api_rest#request_format_2\r\nThe value for inputs key can either a single input tensor or a map of input name to tensors (listed in their natural nested form). Each input can have arbitrary shape and need **not** share the\/ same 0-th dimension \r\n\r\nFor example, if our inputs include two tensors, one tensor with shape (-1, 5) (a batch N of product sequences, each with length 5) and one tensor with shape (-1) (it is a multi-classification problem but we don't calculate the prob for all classes, instead, we randomly pick K classes and only calculate the probs of these k classes for all sequences in this batch). K and N are not necessarily the same in each call. \r\n\r\nIt works well in tensorflow serving but doesn't work in mlflow serving due to the above restriction.  \r\n\r\n### Code to reproduce issue\r\n```\r\ncurl -H 'Content-Type: application\/json' 'localhost:5000\/invocations' -d '{\r\n\"inputs\":{\r\n  \"input_seq\":[[101,275,323,444,512],[289,303,156,223,357]],\r\n  \"input_candidates\":[100,101,102,104,107,119,124]\r\n  }\r\n}'\r\n{\"error_code\": \"MALFORMED_REQUEST\", \"message\": \"Failed to parse data as TF serving input. The length of values for each input\/column name are not the same\", \"stack_trace\": \"Traceback (most recent call last):\\n  File \\\"\/home\/ubuntu\/anaconda3\/envs\/tensorflow_p36\/lib\/python3.6\/site-packages\/mlflow\/pyfunc\/scoring_server\/__init__.py\\\", line 90, in infer_and_parse_json_input\\n    return parse_tf_serving_input(decoded_input, schema=schema)\\n  File \\\"\/home\/ubuntu\/anaconda3\/envs\/tensorflow_p36\/lib\/python3.6\/site-packages\/mlflow\/utils\/proto_json_utils.py\\\", line 244, in parse_tf_serving_input\\n    \\\"Failed to parse data as TF serving input. The length of values for\\\"\\nmlflow.exceptions.MlflowException: Failed to parse data as TF serving input. The length of values for each input\/column name are not the same\\n\"}\r\n```\r\n### Other info \/ logs\r\n\r\n### What component(s), interfaces, languages, and integrations does this bug affect?\r\nComponents \r\n- [ ] `area\/artifacts`: Artifact stores and artifact logging\r\n- [ ] `area\/build`: Build and test infrastructure for MLflow\r\n- [ ] `area\/docs`: MLflow documentation pages\r\n- [ ] `area\/examples`: Example code\r\n- [ ] `area\/model-registry`: Model Registry service, APIs, and the fluent client calls for Model Registry\r\n- [ ] `area\/models`: MLmodel format, model serialization\/deserialization, flavors\r\n- [ ] `area\/projects`: MLproject format, project running backends\r\n- [ ] `area\/scoring`: MLflow Model server, model deployment tools, Spark UDFs\r\n- [ ] `area\/server-infra`: MLflow Tracking server backend\r\n- [ ] `area\/tracking`: Tracking Service, tracking client APIs, autologging\r\n\r\nInterface \r\n- [ ] `area\/uiux`: Front-end, user experience, plotting, JavaScript, JavaScript dev server\r\n- [ ] `area\/docker`: Docker use across MLflow's components, such as MLflow Projects and MLflow Models\r\n- [ ] `area\/sqlalchemy`: Use of SQLAlchemy in the Tracking Service or Model Registry\r\n- [ ] `area\/windows`: Windows support\r\n\r\nLanguage \r\n- [ ] `language\/r`: R APIs and clients\r\n- [ ] `language\/java`: Java APIs and clients\r\n- [ ] `language\/new`: Proposals for new client languages\r\n\r\nIntegrations\r\n- [ ] `integrations\/azure`: Azure and Azure ML integrations\r\n- [ ] `integrations\/sagemaker`: SageMaker integrations\r\n- [ ] `integrations\/databricks`: Databricks integrations\r\n","pull_request":null,"labels":["bug"],"labels_description":["Something isn't working"],"entities":["length","input"]},{"html_url":"https:\/\/github.com\/mlflow\/mlflow\/pull\/5460","id":1160695323,"number":5460,"title":"#4299 log_batch actually writes in batches","state":"open","locked":false,"assignee":null,"assignees":[],"comments":3,"created_at":1646595077000,"updated_at":1647460857000,"closed_at":null,"body":"## What changes are proposed in this pull request?\r\n\r\n- Relates to https:\/\/github.com\/mlflow\/mlflow\/pull\/3024\r\n- Fixes https:\/\/github.com\/mlflow\/mlflow\/issues\/4299 \r\n\r\nIn this PR, SqlAlchemyStore.log_batch is implemented in a way that it actually writes in batches. Three new public methods are added:\r\n\r\n* log_params: log the given params at once. In case of any IntegrityError, rollback the session and call log_param for\r\neach param and let it handle IntegrityError\r\n* log_metrics: log the given metrics at once, and call _update_latest_metric_if_necessary if the metric instance is recently created.\r\n* set_tags: set the given tags at once.\r\n\r\nIn all of them, logging logic of single param, metric, or tag is preserved.\r\n\r\nThe main intuition is to share the same session and not to open new session for each atomic operation and commit the session. To maintain the current behavior, log_params actually commits its own session to catch the integrity error and let it handled by `log_param` method, by logging all params one by one by calling `log_param`.\r\n\r\nSqlAlchemyStore lacks docstrings and typing, so I did not add them. But I can add if it is desired. I only added it to protected `_get_or_create_many` method to clarify intention of it.\r\n\r\n## How is this patch tested?\r\n\r\nIt passes current unit tests cases of `tests\/store\/tracking\/test_sqlalchemy_store.py` and new test cases are added for `log_params`, `log_metrics`, and `set_tags`.\r\n\r\nI also did a small load test:\r\n\r\n```python\r\nimport os\r\nfrom timeit import default_timer as timer\r\n\r\nimport mlflow\r\nfrom mlflow.tracking import MlflowClient\r\nfrom mlflow.entities import Param, RunTag, Metric\r\n\r\n\r\nos.environ[\"MLFLOW_TRACKING_URI\"] = \"sqlite:\/\/\/mock.db\"\r\n\r\nparams = []\r\nmetrics = []\r\ntags = []\r\nN = 100 * 100\r\nMAX_BATCH_SIZE = 100\r\n\r\nfor i in range(N):\r\n    params.append(Param(str(i), \"val\"))\r\n    metrics.append(Metric(str(i), i, 0, 0))\r\n    tags.append(RunTag(str(i), \"val\"))\r\n\r\n\r\nwith mlflow.start_run() as run:\r\n    start = timer()\r\n    for i in range(0, N, MAX_BATCH_SIZE):\r\n        MlflowClient().log_batch(\r\n            run.info.run_id,\r\n            metrics=metrics[i:i+MAX_BATCH_SIZE],\r\n            params=params[i:i+MAX_BATCH_SIZE],\r\n            tags=tags[i:i+MAX_BATCH_SIZE]\r\n        )\r\n    runtime = round(timer() - start, 3)\r\n    print(f\"Runtime: {runtime} seconds...\")\r\n```\r\n\r\nResults of master branch: `Runtime: 230.747 seconds...`\r\nResults of this branch: `Runtime: 25.18 seconds...` \r\n\r\n## Does this PR change the documentation?\r\n\r\n- [x] No. You can skip the rest of this section.\r\n- [ ] Yes. Make sure the changed pages \/ sections render correctly by following the steps below.\r\n\r\n1. Check the status of the `ci\/circleci: build_doc` check. If it's successful, proceed to the\r\n   next step, otherwise fix it.\r\n2. Click `Details` on the right to open the job page of CircleCI.\r\n3. Click the `Artifacts` tab.\r\n4. Click `docs\/build\/html\/index.html`.\r\n5. Find the changed pages \/ sections and make sure they render correctly.\r\n\r\n## Release Notes\r\n\r\n### Is this a user-facing change?\r\n\r\n- [ ] No. You can skip the rest of this section.\r\n- [x] Yes. Give a description of this change to be included in the release notes for MLflow users.\r\n\r\nIn this change, SqlAlchemyStore.log_batch method is modified and it actually writes in batches now. Also, new public methods are added to SqlAlchemyStore, `log_params`, `log_metrics`, and `set_tags`.\r\n\r\n### What component(s), interfaces, languages, and integrations does this PR affect?\r\nComponents \r\n- [ ] `area\/artifacts`: Artifact stores and artifact logging\r\n- [ ] `area\/build`: Build and test infrastructure for MLflow\r\n- [ ] `area\/docs`: MLflow documentation pages\r\n- [ ] `area\/examples`: Example code\r\n- [ ] `area\/model-registry`: Model Registry service, APIs, and the fluent client calls for Model Registry\r\n- [ ] `area\/models`: MLmodel format, model serialization\/deserialization, flavors\r\n- [ ] `area\/projects`: MLproject format, project running backends\r\n- [ ] `area\/scoring`: MLflow Model server, model deployment tools, Spark UDFs\r\n- [ ] `area\/server-infra`: MLflow Tracking server backend\r\n- [x] `area\/tracking`: Tracking Service, tracking client APIs, autologging\r\n\r\nInterface \r\n- [ ] `area\/uiux`: Front-end, user experience, plotting, JavaScript, JavaScript dev server\r\n- [ ] `area\/docker`: Docker use across MLflow's components, such as MLflow Projects and MLflow Models\r\n- [ ] `area\/sqlalchemy`: Use of SQLAlchemy in the Tracking Service or Model Registry\r\n- [ ] `area\/windows`: Windows support\r\n\r\nLanguage \r\n- [ ] `language\/r`: R APIs and clients\r\n- [ ] `language\/java`: Java APIs and clients\r\n- [ ] `language\/new`: Proposals for new client languages\r\n\r\nIntegrations\r\n- [ ] `integrations\/azure`: Azure and Azure ML integrations\r\n- [ ] `integrations\/sagemaker`: SageMaker integrations\r\n- [ ] `integrations\/databricks`: Databricks integrations\r\n\r\n<!--\r\nInsert an empty named anchor here to allow jumping to this section with a fragment URL\r\n(e.g. https:\/\/github.com\/mlflow\/mlflow\/pull\/123#user-content-release-note-category).\r\nNote that GitHub prefixes anchor names in markdown with \"user-content-\".\r\n-->\r\n<a name=\"sqlalchemy-store-log-batch\"><\/a>\r\n### How should the PR be classified in the release notes? Choose one:\r\n\r\n- [ ] `rn\/breaking-change` - The PR will be mentioned in the \"Breaking Changes\" section\r\n- [ ] `rn\/none` - No description will be included. The PR will be mentioned only by the PR number in the \"Small Bugfixes and Documentation Updates\" section\r\n- [ ] `rn\/feature` - A new user-facing feature worth mentioning in the release notes\r\n- [x] `rn\/bug-fix` - A user-facing bug fix worth mentioning in the release notes\r\n- [ ] `rn\/documentation` - A user-facing documentation change worth mentioning in the release notes\r\n","pull_request":{"url":"https:\/\/api.github.com\/repos\/mlflow\/mlflow\/pulls\/5460","html_url":"https:\/\/github.com\/mlflow\/mlflow\/pull\/5460","diff_url":"https:\/\/github.com\/mlflow\/mlflow\/pull\/5460.diff","patch_url":"https:\/\/github.com\/mlflow\/mlflow\/pull\/5460.patch","merged_at":null},"labels":["rn\/bug-fix","area\/tracking"],"labels_description":["Mention under Bug Fixes in Changelogs.","Tracking service, tracking client APIs, autologging"],"entities":["log_batch"]},{"html_url":"https:\/\/github.com\/mlflow\/mlflow\/pull\/5458","id":1160398974,"number":5458,"title":"adds json schema validation","state":"open","locked":false,"assignee":null,"assignees":[],"comments":9,"created_at":1646499624000,"updated_at":1647435969000,"closed_at":null,"body":"## What changes are proposed in this pull request?\r\n\r\nCloses #5208\r\n\r\nThis PR is a first pass at adding JSON validation against a predetermined schema, in an effort to make error handling more friendly for users. The end goal is to return HTTP 400 for bad parameters instead of 500, which happens currently (at least for most endpoints).\r\n\r\nI'm sure there are many error handling cases I've missed, but I think this should cover the most common issues (misspecified parameter types). And of course, we can always expand on this in the future.\r\n\r\n## How is this patch tested?\r\n\r\nTested with `pytest` as per @dbczumar's [suggestion](https:\/\/github.com\/mlflow\/mlflow\/pull\/5350#issuecomment-1039594468). I added a few tests to check if errors were coming back as expected, but we can always add more.\r\n\r\nI also spun up a local MLFlow instance and ran some test API calls from an R session on local.\r\n\r\n**Before this change:**\r\n\r\nLots of `500`s with nothing helpful.\r\n<img width=\"943\" alt=\"Screen Shot 2022-03-06 at 2 37 37 PM\" src=\"https:\/\/user-images.githubusercontent.com\/44219491\/156939267-bf20bf20-4f47-43ec-bdde-d0ab2e798800.png\">\r\n\r\n<img width=\"929\" alt=\"Screen Shot 2022-03-06 at 2 35 02 PM\" src=\"https:\/\/user-images.githubusercontent.com\/44219491\/156939190-4b353b5f-94eb-40c4-a052-2d70427c316b.png\">\r\n\r\n<img width=\"938\" alt=\"Screen Shot 2022-03-06 at 2 34 44 PM\" src=\"https:\/\/user-images.githubusercontent.com\/44219491\/156939183-bd4b3082-28e0-43f7-bfbc-41d4a8ee6626.png\">\r\n\r\nIn order to debug, you would need to dig into your instance's logs, where you'd see something like this (which is helpful, but is hard to parse and doesn't indicate exactly what went wrong. And also, logs aren't always easily available to MLFlow users):\r\n\r\n```\r\n2022-03-06T17:44:56.390215+00:00 app[web.1]: Traceback (most recent call last):\r\n2022-03-06T17:44:56.390216+00:00 app[web.1]:   File \"\/opt\/conda\/lib\/python3.9\/site-packages\/flask\/app.py\", line 2073, in wsgi_app\r\n\r\n...\r\n\r\n2022-03-06T17:44:56.390219+00:00 app[web.1]:   File \"\/opt\/conda\/lib\/python3.9\/site-packages\/mlflow\/server\/handlers.py\", line 284, in _create_experiment\r\n\r\n...\r\n\r\n2022-03-06T17:44:56.390221+00:00 app[web.1]:   File \"\/opt\/conda\/lib\/python3.9\/site-packages\/google\/protobuf\/json_format.py\", line 445, in ParseDict\r\n2022-03-06T17:44:56.390221+00:00 app[web.1]:     parser.ConvertMessage(js_dict, message)\r\n2022-03-06T17:44:56.390221+00:00 app[web.1]:   File \"\/opt\/conda\/lib\/python3.9\/site-packages\/google\/protobuf\/json_format.py\", line 476, in ConvertMessage\r\n2022-03-06T17:44:56.390222+00:00 app[web.1]:     self._ConvertFieldValuePair(value, message)\r\n2022-03-06T17:44:56.390222+00:00 app[web.1]:   File \"\/opt\/conda\/lib\/python3.9\/site-packages\/google\/protobuf\/json_format.py\", line 594, in _ConvertFieldValuePair\r\n2022-03-06T17:44:56.390222+00:00 app[web.1]:     raise ParseError('Failed to parse {0} field: {1}.'.format(name, e))\r\n2022-03-06T17:44:56.390223+00:00 app[web.1]: google.protobuf.json_format.ParseError: Failed to parse name field: expected string or bytes-like object.\r\n```\r\n\r\n**After this change:**\r\n\r\nHelpful error messages!\r\n\r\n<img width=\"565\" alt=\"Screen Shot 2022-03-13 at 5 43 30 PM\" src=\"https:\/\/user-images.githubusercontent.com\/44219491\/158084450-38bf9e83-9679-411d-bcb8-9ff8e99e7b82.png\">\r\n<img width=\"621\" alt=\"Screen Shot 2022-03-13 at 5 43 42 PM\" src=\"https:\/\/user-images.githubusercontent.com\/44219491\/158084463-d8b5c0d6-f958-459c-ac47-478911cc5f85.png\">\r\n<img width=\"542\" alt=\"Screen Shot 2022-03-13 at 5 43 51 PM\" src=\"https:\/\/user-images.githubusercontent.com\/44219491\/158084467-e502d32d-10d4-48ca-ac6c-00f21626cb41.png\">\r\n\r\n\r\n## Does this PR change the documentation?\r\n\r\n- [x] No. You can skip the rest of this section.\r\n- [ ] Yes. Make sure the changed pages \/ sections render correctly by following the steps below.\r\n\r\n1. Check the status of the `ci\/circleci: build_doc` check. If it's successful, proceed to the\r\n   next step, otherwise fix it.\r\n2. Click `Details` on the right to open the job page of CircleCI.\r\n3. Click the `Artifacts` tab.\r\n4. Click `docs\/build\/html\/index.html`.\r\n5. Find the changed pages \/ sections and make sure they render correctly.\r\n\r\n## Release Notes\r\n\r\n### Is this a user-facing change?\r\n\r\n- [ ] No. You can skip the rest of this section.\r\n- [x] Yes. Give a description of this change to be included in the release notes for MLflow users.\r\n\r\nThis change adds validation of API request bodies against predetermined JSON schemas, which will result in friendlier error messages for MLFlow users.\r\n\r\nSee #5208 \r\n\r\n### What component(s), interfaces, languages, and integrations does this PR affect?\r\nComponents \r\n- [ ] `area\/artifacts`: Artifact stores and artifact logging\r\n- [ ] `area\/build`: Build and test infrastructure for MLflow\r\n- [ ] `area\/docs`: MLflow documentation pages\r\n- [ ] `area\/examples`: Example code\r\n- [ ] `area\/model-registry`: Model Registry service, APIs, and the fluent client calls for Model Registry\r\n- [ ] `area\/models`: MLmodel format, model serialization\/deserialization, flavors\r\n- [ ] `area\/projects`: MLproject format, project running backends\r\n- [ ] `area\/scoring`: MLflow Model server, model deployment tools, Spark UDFs\r\n- [ ] `area\/server-infra`: MLflow Tracking server backend\r\n- [x] `area\/tracking`: Tracking Service, tracking client APIs, autologging\r\n\r\nInterface \r\n- [ ] `area\/uiux`: Front-end, user experience, plotting, JavaScript, JavaScript dev server\r\n- [ ] `area\/docker`: Docker use across MLflow's components, such as MLflow Projects and MLflow Models\r\n- [ ] `area\/sqlalchemy`: Use of SQLAlchemy in the Tracking Service or Model Registry\r\n- [ ] `area\/windows`: Windows support\r\n\r\nLanguage \r\n- [ ] `language\/r`: R APIs and clients\r\n- [ ] `language\/java`: Java APIs and clients\r\n- [ ] `language\/new`: Proposals for new client languages\r\n\r\nIntegrations\r\n- [ ] `integrations\/azure`: Azure and Azure ML integrations\r\n- [ ] `integrations\/sagemaker`: SageMaker integrations\r\n- [ ] `integrations\/databricks`: Databricks integrations\r\n\r\n<!--\r\nInsert an empty named anchor here to allow jumping to this section with a fragment URL\r\n(e.g. https:\/\/github.com\/mlflow\/mlflow\/pull\/123#user-content-release-note-category).\r\nNote that GitHub prefixes anchor names in markdown with \"user-content-\".\r\n-->\r\n<a name=\"release-note-category\"><\/a>\r\n### How should the PR be classified in the release notes? Choose one:\r\n\r\n- [ ] `rn\/breaking-change` - The PR will be mentioned in the \"Breaking Changes\" section\r\n- [ ] `rn\/none` - No description will be included. The PR will be mentioned only by the PR number in the \"Small Bugfixes and Documentation Updates\" section\r\n- [x] `rn\/feature` - A new user-facing feature worth mentioning in the release notes\r\n- [x] `rn\/bug-fix` - A user-facing bug fix worth mentioning in the release notes\r\n- [ ] `rn\/documentation` - A user-facing documentation change worth mentioning in the release notes\r\n","pull_request":{"url":"https:\/\/api.github.com\/repos\/mlflow\/mlflow\/pulls\/5458","html_url":"https:\/\/github.com\/mlflow\/mlflow\/pull\/5458","diff_url":"https:\/\/github.com\/mlflow\/mlflow\/pull\/5458.diff","patch_url":"https:\/\/github.com\/mlflow\/mlflow\/pull\/5458.patch","merged_at":null},"labels":["rn\/feature","rn\/bug-fix","area\/tracking"],"labels_description":["Mention under Features in Changelogs.","Mention under Bug Fixes in Changelogs.","Tracking service, tracking client APIs, autologging"],"entities":["validation"]},{"html_url":"https:\/\/github.com\/mlflow\/mlflow\/issues\/5456","id":1160073448,"number":5456,"title":"[proposal] mlx (codename) - opinionated ml pipelines in mlflow","state":"open","locked":false,"assignee":null,"assignees":[],"comments":7,"created_at":1646429987000,"updated_at":1647449116000,"closed_at":null,"body":"## Proposal Summary\r\n\r\n(You can find [the latest version of the proposal in this Google doc](https:\/\/docs.google.com\/document\/d\/1KDNxLvmS392LdNpqc-bU9EAyoTvv0U3FiB5vRMfQuzI\/edit).)\r\n\r\n## What are you trying to do? Articulate your objectives using absolutely no jargon.\r\n\r\nWe want to introduce MLX (codename), an opinionated approach for MLOps, as part of MLflow\u2019s next major release. It adds two key features to MLflow:\r\n- Predefined ML pipelines to address common ML problems at production quality.\r\n- Utilization of efficient pipeline execution engines to accelerate ML development.\r\n\r\nOur objective is to enable data scientists to stay mostly within their comfort zone utilizing their expert knowledge while following the best practices in ML development and delivering production-ready ML projects, with little help from production engineers and DevOps.\r\n\r\n## How is it done today, and what are the limits of current practice?\r\n\r\nAlmost every company wants to be a Data+AI company. But many are new to ML under production settings, where the core ML component is a relatively small component surrounded by many others like data collection and verification, testing and debugging, automation via CI\/CD, resource provision, model management and monitoring, etc. This is where MLOps kicks in and its job is to connect various components needed for ML productionization.\r\n\r\nHowever, despite being [an emerging topic](https:\/\/trends.google.com\/trends\/explore?date=today%205-y&geo=US&q=MLOps), MLOps is hard and there are no widely established approaches for MLOps. In a Databricks blog post [Need for Data-centric ML Platforms](https:\/\/databricks.com\/blog\/2021\/06\/23\/need-for-data-centric-ml-platforms.html), the authors summarized MLOps as ModelOps + DataOps + DevOps. Connecting components from different areas naturally makes MLOps hard. What makes the job even harder is that in many companies the ownership of MLOps usually falls through the cracks between data science teams and production engineering teams. Data scientists are mostly focused on modeling the business problems and reasoning about data, features, and metrics, while the production engineers\/ops are mostly focused on traditional DevOps for software development, ignoring ML-specific Ops like ML development cycles, experiment tracking, data\/model validation, etc.\r\n\r\nOne solution to this problem is to hire \u201cfull-stack\u201d data scientists or engineers who are capable of doing everything end-to-end. But only a few companies can afford this luxury solution.\r\n\r\nAnother candidate solution is to adopt the MLOps frameworks open-sourced by companies that have matured MLOps practices, for example, [TFX](https:\/\/www.tensorflow.org\/tfx) from Google. However, those frameworks were originally designed for internal users to solve likely complex problems. They are not simple enough for the majority of data scientists to solve less sophisticated ML problems.\r\n\r\nWith ~10 million monthly downloads, MLflow is the most popular open-source MLOps framework. It is used by many users to track ML lifecycle and manage projects, metadata, and models. However, it itself is missing the \u201cflow\u201d part that helps streamline an end-to-end pipeline. This proposal aims to fill in that gap and greatly simplify the end-to-end story.\r\n\r\n## What is new in your approach and why do you think it will be successful?\r\n\r\nWe propose an opinionated approach to simplify and standardize ML development and productionization. On one hand, we offer data scientists production-quality ML pipeline templates they can start with and iterate quickly. On the other hand, we offer production engineers command-line interfaces for easy CI\/CD integration. We focus on simplicity and try to address less sophisticated ML use cases, which we believe are the majority.\r\n\r\nBelow are the key differentiators:\r\n\r\n- Pre-defined production-quality ML pipeline templates. We will build pipeline templates that match common ML problem types, e.g., regression, clustering, recommendation, etc, where we embed best practices collected from industry experts. Instead of constructing an ML pipeline end-to-end, data scientists first pick a pipeline template that matches the problem type to start a project and customize its steps to solve the problem. This leads to a declarative and config-driven approach that saves the boilerplate code. So they can focus on the modeling steps while delivering production-ready projects. We believe the pre-defined templates would have a good coverage of the ML problems.\r\n\r\n- Efficient pipeline execution engine. ML development is a very iterative process. Users frequently jump between steps in a pipeline to understand the problem and improve the model. We will adopt an efficient pipeline engine to optimize the execution. After making changes, users only need to declare what they want and leave the engine to figure out what steps need to be executed and what can be reused. So they can quickly iterate during development, e.g., changing some features and then directly verifying feature importance.\r\n\r\n- Command-line interfaces to execute ML pipelines. During production handoff, instead of letting data scientists and production engineers negotiate the scripts, params, and I\/O to be used in CI\/CD, we want to standardize the interfaces to train and deploy models. The pre-defined pipelines automatically perform checks like schema and model validation. So production engineers have less things to worry about if they adopt our approach.\r\n\r\n- Notebook interfaces to execute ML pipelines. While we promote modular development for production, we love notebooks for data analysis, which is essential during iterative ML development. MLX provides notebook interfaces to trigger pipeline execution and display relevant results in cell outputs for data analysis and comparison.\r\n\r\n## Who cares? If you are successful, what difference will it make?\r\n\r\n- Data scientists. If MLX is successful, data scientists can stay mostly in the comfort zone utilizing their expert knowledge while delivering end-to-end production-quality projects.\r\n- Production engineers. If MLX is successful, production engineers can easily configure CI\/CD for ML projects and let data scientists fully own the development cycles.\r\n\r\n## What are the risks?\r\n\r\nMLX is an opinionated approach. The biggest risk is whether the proposed opinionated piece fits the target use cases well. To de-risk, we plan to build the first pipeline template as a proof-of-concept, make it an optional component in MLflow, and collect feedback from the MLflow community to see how it fits and decide the next step.\r\n\r\n## How much will it cost?\r\n\r\nThe initial work will be focused on creating the first pipeline template, e.g., regression, and providing both command-line and notebook interfaces. We will adopt an existing pipeline engine instead of building one to save cost. Our estimate is ~10PWs.\r\n\r\n## How long will it take?\r\n\r\nWe plan to release the first pipeline template in early May. Contributions from the community would help accelerate the development.\r\n\r\n## What are the mid-term and final \u201cexams\u201d to check for success?\r\n\r\n- Mid-term:\r\n  - We receive positive feedback from the community on the first pipeline template.\r\n  - MLX becomes a default component in MLflow\u2019s next major release and ships with more production-quality pipeline templates.\r\n- Final:\r\n  - Wide adoption of MLX.\r\n\r\n## Appendix\r\n\r\n### Terminology\r\n\r\n- Pipeline: An orchestration to solve one kind of machine learning problem end-to-end. It consists of MLX steps, their inputs and outputs, and how they depend on each other. It is usually pre-defined by engineers and used by data scientists, who can configure the pipeline and its steps and customize certain steps to fit the specific problem to solve.\r\n- Step: An MLX pipeline building block that usually does a single task, e.g., feature transformation. It declares inputs and outputs to chain with other steps in a pipeline. Users can configure its behavior via conf or Python code. Once configured, a step should be deterministic. Step names are verbs.\r\n- Run: A session that tracks the execution of an MLX pipeline, fully or partially.\r\n- Profile: A named set of configurations users can activate when triggering a pipeline. Common profile names are \u201clocal\u201d, \u201cdev\u201d, and \u201cprod\u201d.\r\n\r\n### Example development workflow\r\n\r\nMike is a data scientist. He liked the MLX tutorial and wanted to try MLX on a new ML project.\r\n\r\n- He already installed MLflow w\/ MLX from the tutorial.\r\n- His new project is to predict used car sale prices. It is a regression problem. So he ran \u201cmlx init \u2013name autos \u2013pipeline regression\u201d to create a new project folder.\r\n- He used VS Code to open the generated project folder.\r\n- Inside the project folder, he saw a README.md file, a configuration file \u201cmlx.yaml\u201d, a dependency file \u201crequirements.txt\u201d, and subfolders \u201csteps\/\u201d, \u201cnotebooks\/\u201d, etc.\r\n- He opened README.md first and saw instructions and a list of TODOs:\r\n  - [ ] Check\/update \u201crequirements.txt\u201d and run \u201cpip install -r requirements.txt\u201d.\r\n  - [ ] Open \u201cnotebooks\/runme.ipynb\u201d and try running this project first.\r\n  - [ ] Update the primary evaluation metric to use.\r\n  - [ ] Update data location in \u201cmlx.yaml\u201d and target column.\r\n  - [ ] Update sklearn estimator in \u201csteps\/train.py\u201d.\r\n  - [ ] \u2026\r\n- He took a look at requirements.txt. The packages he needed were all listed. So he ran \u201cpip install -r requirements.txt\u201d directly.\r\n- He opens \u201cnotebooks\/runme.ipynb\u201d in VS Code. He already installed the Jupyter plugin. He activated the Python environment (kernel) for this project.\r\n- He clicked \u201cRun All\u201d and saw the pipeline visualization and outputs from its steps.\r\n- He checked the primary metric defined in \u201cmlx.yaml\u201d. It is RMSE, which is good.\r\n- He checked the data path, which points to a local file under \u201cdata\/\u201d. He moved a parquet file that contains the sample training dataset to \u201cdata\/\u201d and updated the data path.\r\n- He changed the target column to \u201cprice\u201d. \r\n- He switched back to \u201crunme.ipynb\u201d and re-ran the \u201cingest\u201d cell, which displays the data summary of ingested data.\r\n- Then he re-ran the \u201cevaluate\u201d cell to see model performance on the real training dataset. The RMSE was more than $10,000, which is bad. He saw some training examples that have the worst prediction errors.\r\n- He opened \u201csteps\/train.py\u201d and saw it uses an AutoML library to train. It takes a param to limit the cost. He increased the limit in the conf and hoped it would improve the model.\r\n- He switched back to \u201crunme.ipynb\u201d. He created a new cell and triggered the \u201cevaluate\u201d step again. He saw that the new model got RMSE $2,000, which is much better.\r\n- He re-ran the \u201cexplain\u201d cell to see feature importance. One feature didn\u2019t show up among the top ones. He knew the feature was very important, but it needed some parsing.\r\n- So he opened \u201csteps\/transform.py\u201d and implemented a parser for that feature.\r\n- He found himself switching between the notebook and the source code. So he split the VS Code window and put the notebook and source code windows side by side.\r\n- He created a new cell and re-ran \u201cevaluate\u201d. The RMSE got improved to $1,500. He checked model importance again and confirmed the specific feature was among the top.\r\n- After a few more iterations, he successfully improved the RMSE to $1,000 on the sample training dataset.\r\n- He wanted to test it on the full training dataset.\r\n- He updated the \u201cmlx.yaml\u201d file and added a new profile called \u201cdev\u201d. He configured the \u201cingest\u201d step to read from the full dataset and increased the train cost limit again.\r\n- He updated the profile name and reran the notebook. It took much longer. He found the RMSE on the full dataset is $800, which was good enough to ship as the initial version.\r\n- He used git to check in the project folder to a repo shared by the data science team.\r\n- Btw, all the trials and models Mike made were tracked under MLflow automatically:)\r\n","pull_request":null,"labels":["enhancement"],"labels_description":["New feature or request"],"entities":["mlflow"]},{"html_url":"https:\/\/github.com\/mlflow\/mlflow\/pull\/5449","id":1158991777,"number":5449,"title":"create experiment to resolve experiment id if in a repo notebook","state":"open","locked":false,"assignee":null,"assignees":[],"comments":0,"created_at":1646347446000,"updated_at":1646411188000,"closed_at":null,"body":"Signed-off-by: apurva-koti <apurva.koti@databricks.com>\r\n## What changes are proposed in this pull request?\r\nIn Databricks, we use the notebook ID as the experiment ID in `fluent._get_experiment_id`. Changed this to send a `CreateExperiment` request with appropriate tags if we're in a repo notebook.\r\n\r\n## How is this patch tested?\r\n\r\nUnit and databricks testing\r\n\r\n## Does this PR change the documentation?\r\n\r\n- [x] No. You can skip the rest of this section.\r\n- [ ] Yes. Make sure the changed pages \/ sections render correctly by following the steps below.\r\n\r\n1. Check the status of the `ci\/circleci: build_doc` check. If it's successful, proceed to the\r\n   next step, otherwise fix it.\r\n2. Click `Details` on the right to open the job page of CircleCI.\r\n3. Click the `Artifacts` tab.\r\n4. Click `docs\/build\/html\/index.html`.\r\n5. Find the changed pages \/ sections and make sure they render correctly.\r\n\r\n## Release Notes\r\n\r\n### Is this a user-facing change?\r\n\r\n- [ ] No. You can skip the rest of this section.\r\n- [ ] Yes. Give a description of this change to be included in the release notes for MLflow users.\r\n\r\n(Details in 1-2 sentences. You can just refer to another PR with a description if this PR is part of a larger change.)\r\n\r\n### What component(s), interfaces, languages, and integrations does this PR affect?\r\nComponents \r\n- [ ] `area\/artifacts`: Artifact stores and artifact logging\r\n- [ ] `area\/build`: Build and test infrastructure for MLflow\r\n- [ ] `area\/docs`: MLflow documentation pages\r\n- [ ] `area\/examples`: Example code\r\n- [ ] `area\/model-registry`: Model Registry service, APIs, and the fluent client calls for Model Registry\r\n- [ ] `area\/models`: MLmodel format, model serialization\/deserialization, flavors\r\n- [ ] `area\/projects`: MLproject format, project running backends\r\n- [ ] `area\/scoring`: MLflow Model server, model deployment tools, Spark UDFs\r\n- [ ] `area\/server-infra`: MLflow Tracking server backend\r\n- [ ] `area\/tracking`: Tracking Service, tracking client APIs, autologging\r\n\r\nInterface \r\n- [ ] `area\/uiux`: Front-end, user experience, plotting, JavaScript, JavaScript dev server\r\n- [ ] `area\/docker`: Docker use across MLflow's components, such as MLflow Projects and MLflow Models\r\n- [ ] `area\/sqlalchemy`: Use of SQLAlchemy in the Tracking Service or Model Registry\r\n- [ ] `area\/windows`: Windows support\r\n\r\nLanguage \r\n- [ ] `language\/r`: R APIs and clients\r\n- [ ] `language\/java`: Java APIs and clients\r\n- [ ] `language\/new`: Proposals for new client languages\r\n\r\nIntegrations\r\n- [ ] `integrations\/azure`: Azure and Azure ML integrations\r\n- [ ] `integrations\/sagemaker`: SageMaker integrations\r\n- [x] `integrations\/databricks`: Databricks integrations\r\n\r\n<!--\r\nInsert an empty named anchor here to allow jumping to this section with a fragment URL\r\n(e.g. https:\/\/github.com\/mlflow\/mlflow\/pull\/123#user-content-release-note-category).\r\nNote that GitHub prefixes anchor names in markdown with \"user-content-\".\r\n-->\r\n<a name=\"release-note-category\"><\/a>\r\n### How should the PR be classified in the release notes? Choose one:\r\n\r\n- [ ] `rn\/breaking-change` - The PR will be mentioned in the \"Breaking Changes\" section\r\n- [x] `rn\/none` - No description will be included. The PR will be mentioned only by the PR number in the \"Small Bugfixes and Documentation Updates\" section\r\n- [ ] `rn\/feature` - A new user-facing feature worth mentioning in the release notes\r\n- [ ] `rn\/bug-fix` - A user-facing bug fix worth mentioning in the release notes\r\n- [ ] `rn\/documentation` - A user-facing documentation change worth mentioning in the release notes\r\n","pull_request":{"url":"https:\/\/api.github.com\/repos\/mlflow\/mlflow\/pulls\/5449","html_url":"https:\/\/github.com\/mlflow\/mlflow\/pull\/5449","diff_url":"https:\/\/github.com\/mlflow\/mlflow\/pull\/5449.diff","patch_url":"https:\/\/github.com\/mlflow\/mlflow\/pull\/5449.patch","merged_at":null},"labels":["rn\/none","integrations\/databricks"],"labels_description":["List under Small Changes in Changelogs.","Databricks integrations"],"entities":["experiment","experiment","id","repo","notebook"]},{"html_url":"https:\/\/github.com\/mlflow\/mlflow\/pull\/5444","id":1157909633,"number":5444,"title":"[wip] remove mlflow.keras.autolog()","state":"open","locked":false,"assignee":null,"assignees":[],"comments":1,"created_at":1646276038000,"updated_at":1646277423000,"closed_at":null,"body":"Signed-off-by: dbczumar <corey.zumar@databricks.com>\r\n\r\n## What changes are proposed in this pull request?\r\n\r\n(Please fill in changes proposed in this fix)\r\n\r\n## How is this patch tested?\r\n\r\n(Details)\r\n\r\n## Does this PR change the documentation?\r\n\r\n- [ ] No. You can skip the rest of this section.\r\n- [ ] Yes. Make sure the changed pages \/ sections render correctly by following the steps below.\r\n\r\n1. Check the status of the `ci\/circleci: build_doc` check. If it's successful, proceed to the\r\n   next step, otherwise fix it.\r\n2. Click `Details` on the right to open the job page of CircleCI.\r\n3. Click the `Artifacts` tab.\r\n4. Click `docs\/build\/html\/index.html`.\r\n5. Find the changed pages \/ sections and make sure they render correctly.\r\n\r\n## Release Notes\r\n\r\n### Is this a user-facing change?\r\n\r\n- [ ] No. You can skip the rest of this section.\r\n- [ ] Yes. Give a description of this change to be included in the release notes for MLflow users.\r\n\r\n(Details in 1-2 sentences. You can just refer to another PR with a description if this PR is part of a larger change.)\r\n\r\n### What component(s), interfaces, languages, and integrations does this PR affect?\r\nComponents \r\n- [ ] `area\/artifacts`: Artifact stores and artifact logging\r\n- [ ] `area\/build`: Build and test infrastructure for MLflow\r\n- [ ] `area\/docs`: MLflow documentation pages\r\n- [ ] `area\/examples`: Example code\r\n- [ ] `area\/model-registry`: Model Registry service, APIs, and the fluent client calls for Model Registry\r\n- [ ] `area\/models`: MLmodel format, model serialization\/deserialization, flavors\r\n- [ ] `area\/projects`: MLproject format, project running backends\r\n- [ ] `area\/scoring`: MLflow Model server, model deployment tools, Spark UDFs\r\n- [ ] `area\/server-infra`: MLflow Tracking server backend\r\n- [ ] `area\/tracking`: Tracking Service, tracking client APIs, autologging\r\n\r\nInterface \r\n- [ ] `area\/uiux`: Front-end, user experience, plotting, JavaScript, JavaScript dev server\r\n- [ ] `area\/docker`: Docker use across MLflow's components, such as MLflow Projects and MLflow Models\r\n- [ ] `area\/sqlalchemy`: Use of SQLAlchemy in the Tracking Service or Model Registry\r\n- [ ] `area\/windows`: Windows support\r\n\r\nLanguage \r\n- [ ] `language\/r`: R APIs and clients\r\n- [ ] `language\/java`: Java APIs and clients\r\n- [ ] `language\/new`: Proposals for new client languages\r\n\r\nIntegrations\r\n- [ ] `integrations\/azure`: Azure and Azure ML integrations\r\n- [ ] `integrations\/sagemaker`: SageMaker integrations\r\n- [ ] `integrations\/databricks`: Databricks integrations\r\n\r\n<!--\r\nInsert an empty named anchor here to allow jumping to this section with a fragment URL\r\n(e.g. https:\/\/github.com\/mlflow\/mlflow\/pull\/123#user-content-release-note-category).\r\nNote that GitHub prefixes anchor names in markdown with \"user-content-\".\r\n-->\r\n<a name=\"release-note-category\"><\/a>\r\n### How should the PR be classified in the release notes? Choose one:\r\n\r\n- [ ] `rn\/breaking-change` - The PR will be mentioned in the \"Breaking Changes\" section\r\n- [ ] `rn\/none` - No description will be included. The PR will be mentioned only by the PR number in the \"Small Bugfixes and Documentation Updates\" section\r\n- [ ] `rn\/feature` - A new user-facing feature worth mentioning in the release notes\r\n- [ ] `rn\/bug-fix` - A user-facing bug fix worth mentioning in the release notes\r\n- [ ] `rn\/documentation` - A user-facing documentation change worth mentioning in the release notes\r\n","pull_request":{"url":"https:\/\/api.github.com\/repos\/mlflow\/mlflow\/pulls\/5444","html_url":"https:\/\/github.com\/mlflow\/mlflow\/pull\/5444","diff_url":"https:\/\/github.com\/mlflow\/mlflow\/pull\/5444.diff","patch_url":"https:\/\/github.com\/mlflow\/mlflow\/pull\/5444.patch","merged_at":null},"labels":[],"labels_description":[],"entities":[]},{"html_url":"https:\/\/github.com\/mlflow\/mlflow\/issues\/5423","id":1152588150,"number":5423,"title":"discussion: how should i save my object detection.pt weights in mlflow?","state":"open","locked":false,"assignee":null,"assignees":[],"comments":4,"created_at":1645924535000,"updated_at":1646197523000,"closed_at":null,"body":"Hi,\r\n\r\nI am working on an object detection project that uses Yolov5 with weights.pt format.  We can't store the weights locally, so I came up with a solution with mlflow that downloads the weights from an experiment run in mlflow based on tags we created. I know this isn't the intented way to use mlflow, and I should be registering the model and placing this into the model registry.\r\n\r\nWhen making predictions we run the following line from the cli.\r\n\r\n`python detect.py --weights path_to_weights --source path_to_image`\r\n\r\nIn production, this application could be containerized with docker and orchestrated with kubernetes.\r\n\r\nCan anyone suggest the best way to register the weights.pt to mlflow and download them from mlflow?\r\n\r\n","pull_request":null,"labels":[],"labels_description":[],"entities":["object","detection.pt"]},{"html_url":"https:\/\/github.com\/mlflow\/mlflow\/pull\/5421","id":1151404707,"number":5421,"title":"chore: use pickle by default when saving scikit-learn models","state":"open","locked":false,"assignee":null,"assignees":[],"comments":6,"created_at":1645861157000,"updated_at":1646266129000,"closed_at":null,"body":"CloudPickle officially states: \"Using cloudpickle for long-term object storage is not supported and strongly discouraged.\"\r\n\r\nFixes https:\/\/github.com\/mlflow\/mlflow\/issues\/5419\r\nFixes https:\/\/github.com\/mlflow\/mlflow\/issues\/5420\r\n\r\nSigned-off-by: Alexey Volkov <alexey.volkov@ark-kun.com>\r\n\r\n## What changes are proposed in this pull request?\r\n\r\n(Please fill in changes proposed in this fix)\r\n\r\n## How is this patch tested?\r\n\r\n(Details)\r\n\r\n## Does this PR change the documentation?\r\n\r\n- [ ] No. You can skip the rest of this section.\r\n- [ ] Yes. Make sure the changed pages \/ sections render correctly by following the steps below.\r\n\r\n1. Check the status of the `ci\/circleci: build_doc` check. If it's successful, proceed to the\r\n   next step, otherwise fix it.\r\n2. Click `Details` on the right to open the job page of CircleCI.\r\n3. Click the `Artifacts` tab.\r\n4. Click `docs\/build\/html\/index.html`.\r\n5. Find the changed pages \/ sections and make sure they render correctly.\r\n\r\n## Release Notes\r\n\r\n### Is this a user-facing change?\r\n\r\n- [ ] No. You can skip the rest of this section.\r\n- [ ] Yes. Give a description of this change to be included in the release notes for MLflow users.\r\n\r\n(Details in 1-2 sentences. You can just refer to another PR with a description if this PR is part of a larger change.)\r\n\r\n### What component(s), interfaces, languages, and integrations does this PR affect?\r\nComponents \r\n- [ ] `area\/artifacts`: Artifact stores and artifact logging\r\n- [ ] `area\/build`: Build and test infrastructure for MLflow\r\n- [ ] `area\/docs`: MLflow documentation pages\r\n- [ ] `area\/examples`: Example code\r\n- [ ] `area\/model-registry`: Model Registry service, APIs, and the fluent client calls for Model Registry\r\n- [x] `area\/models`: MLmodel format, model serialization\/deserialization, flavors\r\n- [ ] `area\/projects`: MLproject format, project running backends\r\n- [ ] `area\/scoring`: MLflow Model server, model deployment tools, Spark UDFs\r\n- [ ] `area\/server-infra`: MLflow Tracking server backend\r\n- [ ] `area\/tracking`: Tracking Service, tracking client APIs, autologging\r\n\r\nInterface \r\n- [ ] `area\/uiux`: Front-end, user experience, plotting, JavaScript, JavaScript dev server\r\n- [ ] `area\/docker`: Docker use across MLflow's components, such as MLflow Projects and MLflow Models\r\n- [ ] `area\/sqlalchemy`: Use of SQLAlchemy in the Tracking Service or Model Registry\r\n- [ ] `area\/windows`: Windows support\r\n\r\nLanguage \r\n- [ ] `language\/r`: R APIs and clients\r\n- [ ] `language\/java`: Java APIs and clients\r\n- [ ] `language\/new`: Proposals for new client languages\r\n\r\nIntegrations\r\n- [ ] `integrations\/azure`: Azure and Azure ML integrations\r\n- [ ] `integrations\/sagemaker`: SageMaker integrations\r\n- [ ] `integrations\/databricks`: Databricks integrations\r\n\r\n<!--\r\nInsert an empty named anchor here to allow jumping to this section with a fragment URL\r\n(e.g. https:\/\/github.com\/mlflow\/mlflow\/pull\/123#user-content-release-note-category).\r\nNote that GitHub prefixes anchor names in markdown with \"user-content-\".\r\n-->\r\n<a name=\"release-note-category\"><\/a>\r\n### How should the PR be classified in the release notes? Choose one:\r\n\r\n- [ ] `rn\/breaking-change` - The PR will be mentioned in the \"Breaking Changes\" section\r\n- [x] `rn\/none` - No description will be included. The PR will be mentioned only by the PR number in the \"Small Bugfixes and Documentation Updates\" section\r\n- [ ] `rn\/feature` - A new user-facing feature worth mentioning in the release notes\r\n- [ ] `rn\/bug-fix` - A user-facing bug fix worth mentioning in the release notes\r\n- [ ] `rn\/documentation` - A user-facing documentation change worth mentioning in the release notes\r\n","pull_request":{"url":"https:\/\/api.github.com\/repos\/mlflow\/mlflow\/pulls\/5421","html_url":"https:\/\/github.com\/mlflow\/mlflow\/pull\/5421","diff_url":"https:\/\/github.com\/mlflow\/mlflow\/pull\/5421.diff","patch_url":"https:\/\/github.com\/mlflow\/mlflow\/pull\/5421.patch","merged_at":null},"labels":["rn\/none","area\/models"],"labels_description":["List under Small Changes in Changelogs.","MLmodel format, model serialization\/deserialization, flavors"],"entities":["chore:","pickle","default","scikit-learn"]},{"html_url":"https:\/\/github.com\/mlflow\/mlflow\/issues\/5410","id":1150136673,"number":5410,"title":"[feature request] html support image in artifacts","state":"open","locked":false,"assignee":null,"assignees":[],"comments":1,"created_at":1645774636000,"updated_at":1646121944000,"closed_at":null,"body":"Current Artifact support preview HTML and image,\r\nBut \"Html\" seem no support load image in Artifact, ex:\r\n<img src=\"image_in_artifacts.png\"\/>\r\n\r\nCan we support it?","pull_request":null,"labels":["enhancement"],"labels_description":["New feature or request"],"entities":["[feature","request]","html","support","image"]},{"html_url":"https:\/\/github.com\/mlflow\/mlflow\/issues\/5397","id":1145166171,"number":5397,"title":"[feature request] show multiple plots in tracking ui","state":"open","locked":false,"assignee":null,"assignees":[],"comments":1,"created_at":1645399546000,"updated_at":1645757608000,"closed_at":null,"body":"## Willingness to contribute\r\n- [ ] Yes. I can contribute this feature independently.\r\n- [ ] Yes. I would be willing to contribute this feature with guidance from the MLflow community.\r\n- [x] No. I cannot contribute this feature at this time.\r\n\r\n## Proposal Summary\r\n\r\nUser Interface (`mlflow ui`)  displaying multiple plots instead of just one.\r\n\r\n## Motivation\r\n\r\nCurrently the Tracking UI seems to allow 1 plot only.\r\nMultiple metrics can be plotted there, but in cases where metrics are on very different scales (e.g. different losses, learning rate, accuracy, etc.) the visualization is poor.\r\n\r\nSwitching being different metrics of interest, back-and-forth, manually, is not very user friendly.\r\n\r\nAllowing multiple plots would improve the experiment tracking immensely.\r\n\r\n### What component(s), interfaces, languages, and integrations does this feature affect?\r\nComponents \r\n- [ ] `area\/server-infra`: MLflow Tracking server backend\r\n- [x] `area\/tracking`: Tracking Service, tracking client APIs, autologging\r\n\r\nInterfaces\r\n- [x] `area\/uiux`: Front-end, user experience, plotting, JavaScript, JavaScript dev server\r\n","pull_request":null,"labels":["enhancement","area\/uiux","area\/tracking"],"labels_description":["New feature or request","Front-end, user experience, plotting, JavaScript, JavaScript dev server","Tracking service, tracking client APIs, autologging"],"entities":["[feature"]},{"html_url":"https:\/\/github.com\/mlflow\/mlflow\/issues\/5392","id":1142897742,"number":5392,"title":"mlflow & vertex ai integration","state":"open","locked":false,"assignee":null,"assignees":[],"comments":3,"created_at":1645184432000,"updated_at":1646358510000,"closed_at":null,"body":"Can we integrate vertex AI with mlflow ? If yes, how ? ","pull_request":null,"labels":[],"labels_description":[],"entities":["vertex","integration"]},{"html_url":"https:\/\/github.com\/mlflow\/mlflow\/issues\/5388","id":1141583729,"number":5388,"title":"tracking experiments on both file and db backed tracking servers.","state":"open","locked":false,"assignee":null,"assignees":[],"comments":2,"created_at":1645116188000,"updated_at":1646197901000,"closed_at":null,"body":"Hello,\r\nI am currently trying to develop a solution for registry of the MLmodels. Already had a tracking server running in a filebased tracking system and wanted to get the registry going aswell, so trying to use a mysql db. \r\nThe question is: is there a way to maintain the tracking of the experiments on both servers? (save the mlflow entities on both file:\/\/ and mysql:\/\/ servers, so that if the DB goes down, the user can continue it's experiments\/tracking on the file based, without any historic info lost).\r\nFrom what I tried, i'm only being able to track on the DB server or on the filebased server. ","pull_request":null,"labels":[],"labels_description":[],"entities":["file","db","tracking","servers."]},{"html_url":"https:\/\/github.com\/mlflow\/mlflow\/issues\/5383","id":1139285427,"number":5383,"title":"[fr] allow user to download metrics data as csv","state":"open","locked":false,"assignee":null,"assignees":[],"comments":5,"created_at":1644964620000,"updated_at":1646461371000,"closed_at":null,"body":"Thank you for submitting a feature request. **Before proceeding, please review MLflow's [Issue Policy for feature requests](https:\/\/www.github.com\/mlflow\/mlflow\/blob\/master\/ISSUE_POLICY.md#feature-requests) and the [MLflow Contributing Guide](https:\/\/github.com\/mlflow\/mlflow\/blob\/master\/CONTRIBUTING.rst)**.\r\n\r\n**Please fill in this feature request template to ensure a timely and thorough response.**\r\n\r\n## Willingness to contribute\r\nThe MLflow Community encourages new feature contributions. Would you or another member of your organization be willing to contribute an implementation of this feature (either as an MLflow Plugin or an enhancement to the MLflow code base)?\r\n\r\n- [ ] Yes. I can contribute this feature independently.\r\n- [ ] Yes. I would be willing to contribute this feature with guidance from the MLflow community.\r\n- [X] No. I cannot contribute this feature at this time.\r\n\r\n## Proposal Summary\r\n\r\n(In a few sentences, provide a clear, high-level description of the feature request)\r\n\r\n## Motivation\r\n- What is the use case for this feature?\r\nTo be able to download time-series of metrics logged to mlflow as a csv. Whatever data can be visualized on the line plot in the mlflow experiment tracker Metrics pane will able to be download as csv for more detailed analysis or visualization. \r\n- Why is this use case valuable to support for MLflow users in general?\r\nMetrics time series can be very useful for model selection and diagnosis. By viewing the time-corrected covariation between metrics, it will allow users to gain further insight into the model. Additionally, having a csv allows users flexibility to visualize and analyze these valuable metrics in whichever method they choose.\r\n- Why is this use case valuable to support for your project(s) or organization?\r\nGaining further insight into the model selection process. Additionally to help understand what metrics may be redundant or misleading.\r\n- Why is it currently difficult to achieve this use case? (please be as specific as possible about why related MLflow features and components are insufficient)\r\nThe data is already stored somewhere in mlflow, but it is just not easily accessible to the user.\r\n\r\n### What component(s), interfaces, languages, and integrations does this feature affect?\r\nComponents \r\n- [ ] `area\/artifacts`: Artifact stores and artifact logging\r\n- [ ] `area\/build`: Build and test infrastructure for MLflow\r\n- [ ] `area\/docs`: MLflow documentation pages\r\n- [ ] `area\/examples`: Example code\r\n- [ ] `area\/model-registry`: Model Registry service, APIs, and the fluent client calls for Model Registry\r\n- [ ] `area\/models`: MLmodel format, model serialization\/deserialization, flavors\r\n- [ ] `area\/projects`: MLproject format, project running backends\r\n- [ ] `area\/scoring`: MLflow Model server, model deployment tools, Spark UDFs\r\n- [ ] `area\/server-infra`: MLflow Tracking server backend\r\n- [X ] `area\/tracking`: Tracking Service, tracking client APIs, autologging\r\n\r\nInterfaces\r\n- [ ] `area\/uiux`: Front-end, user experience, plotting, JavaScript, JavaScript dev server\r\n- [ ] `area\/docker`: Docker use across MLflow's components, such as MLflow Projects and MLflow Models\r\n- [ ] `area\/sqlalchemy`: Use of SQLAlchemy in the Tracking Service or Model Registry\r\n- [ ] `area\/windows`: Windows support\r\n\r\nLanguages \r\n- [ ] `language\/r`: R APIs and clients\r\n- [ ] `language\/java`: Java APIs and clients\r\n- [ ] `language\/new`: Proposals for new client languages\r\n\r\nIntegrations\r\n- [ ] `integrations\/azure`: Azure and Azure ML integrations\r\n- [ ] `integrations\/sagemaker`: SageMaker integrations\r\n- [ ] `integrations\/databricks`: Databricks integrations\r\n\r\n## Details\r\n\r\n(Use this section to include any additional information about the feature. If you have a proposal for how to implement this feature, please include it here. For implementation guidelines, please refer to the [Contributing Guide](https:\/\/github.com\/mlflow\/mlflow\/blob\/master\/CONTRIBUTING.rst#contribution-guidelines).)\r\n\r\nJust make whatever data is used to create existing visualization available to the user for download. I think it would be a very small adjustment. ","pull_request":null,"labels":["enhancement","area\/tracking"],"labels_description":["New feature or request","Tracking service, tracking client APIs, autologging"],"entities":["user","csv"]},{"html_url":"https:\/\/github.com\/mlflow\/mlflow\/issues\/5381","id":1138571630,"number":5381,"title":"[fr] add a timeout argument in model serve (scoring server)","state":"open","locked":false,"assignee":null,"assignees":[],"comments":2,"created_at":1644925151000,"updated_at":1645433664000,"closed_at":null,"body":"## Willingness to contribute\r\nThe MLflow Community encourages new feature contributions. Would you or another member of your organization be willing to contribute an implementation of this feature (either as an MLflow Plugin or an enhancement to the MLflow code base)?\r\n\r\n- [ ] Yes. I can contribute this feature independently.\r\n- [x] Yes. I would be willing to contribute this feature with guidance from the MLflow community.\r\n- [ ] No. I cannot contribute this feature at this time.\r\n\r\n## Proposal Summary\r\n\r\nFor cases where POST request latency is not the case or a custom inference logic is implemented, the default timeout (60s) of scoring server is not sufficient and a client asking for a response will lead to the following errors.\r\n```\r\nProtocolError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\r\nConnectionError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\r\n```\r\n\r\n## Motivation\r\n- What is the use case for this feature? Timeout is hardcoded in scoring server. [link to code](https:\/\/github.com\/mlflow\/mlflow\/blob\/master\/mlflow\/pyfunc\/scoring_server\/__init__.py#L358)\r\n- Why is this use case valuable to support for MLflow users in general? We would probably wish to set a timeout for the scoring server\r\n- Why is this use case valuable to support for your project(s) or organization? We have implemented a pyfunc model with custom inference logic and stateful model serving architecture. For applications where latency is not the case, it would be nice to opt for the desired scoring server timeout.\r\n- Why is it currently difficult to achieve this use case? (please be as specific as possible about why related MLflow features and components are insufficient) It's not implemented, though we have kept a fork MLFlow version and set a timeout argument to handle this case \r\n\r\n### What component(s), interfaces, languages, and integrations does this feature affect?\r\nComponents \r\n- [ ] `area\/artifacts`: Artifact stores and artifact logging\r\n- [ ] `area\/build`: Build and test infrastructure for MLflow\r\n- [x] `area\/docs`: MLflow documentation pages\r\n- [ ] `area\/examples`: Example code\r\n- [ ] `area\/model-registry`: Model Registry service, APIs, and the fluent client calls for Model Registry\r\n- [ ] `area\/models`: MLmodel format, model serialization\/deserialization, flavors\r\n- [ ] `area\/projects`: MLproject format, project running backends\r\n- [x] `area\/scoring`: MLflow Model server, model deployment tools, Spark UDFs\r\n- [ ] `area\/server-infra`: MLflow Tracking server backend\r\n- [ ] `area\/tracking`: Tracking Service, tracking client APIs, autologging\r\n\r\nInterfaces\r\n- [ ] `area\/uiux`: Front-end, user experience, plotting, JavaScript, JavaScript dev server\r\n- [ ] `area\/docker`: Docker use across MLflow's components, such as MLflow Projects and MLflow Models\r\n- [ ] `area\/sqlalchemy`: Use of SQLAlchemy in the Tracking Service or Model Registry\r\n- [ ] `area\/windows`: Windows support\r\n\r\nLanguages \r\n- [ ] `language\/r`: R APIs and clients\r\n- [ ] `language\/java`: Java APIs and clients\r\n- [ ] `language\/new`: Proposals for new client languages\r\n\r\nIntegrations\r\n- [ ] `integrations\/azure`: Azure and Azure ML integrations\r\n- [ ] `integrations\/sagemaker`: SageMaker integrations\r\n- [ ] `integrations\/databricks`: Databricks integrations\r\n\r\n## Details\r\n\r\nWe can probably \r\n1. create a new `--timeout` argument in `models serve` [command](https:\/\/github.com\/mlflow\/mlflow\/blob\/master\/mlflow\/models\/cli.py#L35)\r\n2. remove the hardcoded timeout and create a new parser for timeout argument in scoring server [init](https:\/\/github.com\/mlflow\/mlflow\/blob\/master\/mlflow\/pyfunc\/scoring_server\/__init__.py#L358)","pull_request":null,"labels":["enhancement","area\/docs","area\/scoring"],"labels_description":["New feature or request","Documentation issues","MLflow Model server, model deployment tools, Spark UDFs"],"entities":["timeout","argument","model","serve"]},{"html_url":"https:\/\/github.com\/mlflow\/mlflow\/pull\/5380","id":1138248024,"number":5380,"title":"add pyenv + virtualenv support for mlflow models","state":"open","locked":false,"assignee":null,"assignees":[],"comments":1,"created_at":1644906905000,"updated_at":1647510186000,"closed_at":null,"body":"Signed-off-by: harupy <17039389+harupy@users.noreply.github.com>\r\n\r\n## What changes are proposed in this pull request?\r\n\r\nThis PR makes the following changes to support pyenv + virtualenv for `mlflow models`. I'm happy to split the PR to facilitate code review.\r\n\r\n- [x] Add `--env-manager` option.\r\n- [x] Deprecate `--no-conda` and encourage using `--env-manager` instead.\r\n- [x] Raise an experimental warning if `virtualenv` is chosen.\r\n- [x] Indicate conda is discouraged now and virtualenv is available as an alternative.\r\n- [x] Support for serving models that don't have `python-env.yaml`.\r\n- [x] Clean up the environment if `pip install` fails.\r\n- [ ] Log `python-env.yaml` in all flavors.\r\n- [x] Restore a model environment using pyenv + virtualenv.\r\n- [ ] Update documentation.\r\n\r\n## Follow-up tasks\r\n\r\n- Replace `--no-conda` with `--env-manager` in tests.\r\n- Add pyenv + virtualenv support for MLflow Projects.\r\n\r\n## How is this patch tested?\r\n\r\n- Unit tests\r\n- Manual tests using the script below\r\n\r\n```python\r\nimport mlflow\r\nimport subprocess\r\n\r\nfrom sklearn.linear_model import LogisticRegression\r\nfrom sklearn.datasets import load_iris\r\nfrom click.testing import CliRunner\r\n\r\n\r\nX, y = load_iris(return_X_y=True, as_frame=True)\r\nmodel = LogisticRegression().fit(X, y)\r\n\r\nwith mlflow.start_run():\r\n    model_info = mlflow.sklearn.log_model(model, \"model\")\r\n\r\nsample_path = \"sample.csv\"\r\nX.sample(frac=0.1, random_state=0).to_csv(sample_path, index=False)\r\n\r\n\r\ndef run(args):\r\n    args = [\"mlflow\", \"models\", *args]\r\n    print(\"========== Running\", args, \"==========\")\r\n    subprocess.run(args, check=True)\r\n\r\n\r\ncommon_args = [\"--model-uri\", model_info.model_uri, \"--env-manager\", \"virtualenv\"]\r\nrun([\"prepare-env\", *common_args])\r\nrun([\"predict\", *common_args, \"--input-path\", sample_path, \"--content-type\", \"csv\"])\r\nrun([\"serve\", *common_args])\r\n\r\n# Wait until the server is up and running, and then run this command in another terminal\r\nr\"\"\"\r\ncurl -X POST -d \"{\\\"data\\\":[[1,2,3,4]]}\" \\\r\n-H \"Content-Type: application\/json; format=pandas-split\" \\\r\nhttp:\/\/localhost:5000\/invocations\r\n\"\"\"\r\n```\r\n\r\n## Does this PR change the documentation?\r\n\r\n- [ ] No. You can skip the rest of this section.\r\n- [ ] Yes. Make sure the changed pages \/ sections render correctly by following the steps below.\r\n\r\n1. Check the status of the `ci\/circleci: build_doc` check. If it's successful, proceed to the\r\n   next step, otherwise fix it.\r\n2. Click `Details` on the right to open the job page of CircleCI.\r\n3. Click the `Artifacts` tab.\r\n4. Click `docs\/build\/html\/index.html`.\r\n5. Find the changed pages \/ sections and make sure they render correctly.\r\n\r\n## Release Notes\r\n\r\n### Is this a user-facing change?\r\n\r\n- [ ] No. You can skip the rest of this section.\r\n- [x] Yes. Give a description of this change to be included in the release notes for MLflow users.\r\n\r\n(Details in 1-2 sentences. You can just refer to another PR with a description if this PR is part of a larger change.)\r\n\r\n### What component(s), interfaces, languages, and integrations does this PR affect?\r\nComponents \r\n- [ ] `area\/artifacts`: Artifact stores and artifact logging\r\n- [ ] `area\/build`: Build and test infrastructure for MLflow\r\n- [ ] `area\/docs`: MLflow documentation pages\r\n- [ ] `area\/examples`: Example code\r\n- [ ] `area\/model-registry`: Model Registry service, APIs, and the fluent client calls for Model Registry\r\n- [ ] `area\/models`: MLmodel format, model serialization\/deserialization, flavors\r\n- [ ] `area\/projects`: MLproject format, project running backends\r\n- [ ] `area\/scoring`: MLflow Model server, model deployment tools, Spark UDFs\r\n- [ ] `area\/server-infra`: MLflow Tracking server backend\r\n- [ ] `area\/tracking`: Tracking Service, tracking client APIs, autologging\r\n\r\nInterface \r\n- [ ] `area\/uiux`: Front-end, user experience, plotting, JavaScript, JavaScript dev server\r\n- [ ] `area\/docker`: Docker use across MLflow's components, such as MLflow Projects and MLflow Models\r\n- [ ] `area\/sqlalchemy`: Use of SQLAlchemy in the Tracking Service or Model Registry\r\n- [ ] `area\/windows`: Windows support\r\n\r\nLanguage \r\n- [ ] `language\/r`: R APIs and clients\r\n- [ ] `language\/java`: Java APIs and clients\r\n- [ ] `language\/new`: Proposals for new client languages\r\n\r\nIntegrations\r\n- [ ] `integrations\/azure`: Azure and Azure ML integrations\r\n- [ ] `integrations\/sagemaker`: SageMaker integrations\r\n- [ ] `integrations\/databricks`: Databricks integrations\r\n\r\n<!--\r\nInsert an empty named anchor here to allow jumping to this section with a fragment URL\r\n(e.g. https:\/\/github.com\/mlflow\/mlflow\/pull\/123#user-content-release-note-category).\r\nNote that GitHub prefixes anchor names in markdown with \"user-content-\".\r\n-->\r\n<a name=\"release-note-category\"><\/a>\r\n### How should the PR be classified in the release notes? Choose one:\r\n\r\n- [ ] `rn\/breaking-change` - The PR will be mentioned in the \"Breaking Changes\" section\r\n- [x] `rn\/none` - No description will be included. The PR will be mentioned only by the PR number in the \"Small Bugfixes and Documentation Updates\" section\r\n- [ ] `rn\/feature` - A new user-facing feature worth mentioning in the release notes\r\n- [ ] `rn\/bug-fix` - A user-facing bug fix worth mentioning in the release notes\r\n- [ ] `rn\/documentation` - A user-facing documentation change worth mentioning in the release notes\r\n","pull_request":{"url":"https:\/\/api.github.com\/repos\/mlflow\/mlflow\/pulls\/5380","html_url":"https:\/\/github.com\/mlflow\/mlflow\/pull\/5380","diff_url":"https:\/\/github.com\/mlflow\/mlflow\/pull\/5380.diff","patch_url":"https:\/\/github.com\/mlflow\/mlflow\/pull\/5380.patch","merged_at":null},"labels":["rn\/none"],"labels_description":["List under Small Changes in Changelogs."],"entities":["pyenv","virtualenv","support","mlflow"]},{"html_url":"https:\/\/github.com\/mlflow\/mlflow\/pull\/5378","id":1137855297,"number":5378,"title":"introducing abstract endpoint methods","state":"open","locked":false,"assignee":null,"assignees":[],"comments":1,"created_at":1644873442000,"updated_at":1646959026000,"closed_at":null,"body":"## What changes are proposed in this pull request?\r\n\r\n(Please fill in changes proposed in this fix)\r\n\r\n## How is this patch tested?\r\n\r\n(Details)\r\n\r\n## Does this PR change the documentation?\r\n\r\n- [ ] No. You can skip the rest of this section.\r\n- [ ] Yes. Make sure the changed pages \/ sections render correctly by following the steps below.\r\n\r\n1. Check the status of the `ci\/circleci: build_doc` check. If it's successful, proceed to the\r\n   next step, otherwise fix it.\r\n2. Click `Details` on the right to open the job page of CircleCI.\r\n3. Click the `Artifacts` tab.\r\n4. Click `docs\/build\/html\/index.html`.\r\n5. Find the changed pages \/ sections and make sure they render correctly.\r\n\r\n## Release Notes\r\n\r\n### Is this a user-facing change?\r\n\r\n- [ ] No. You can skip the rest of this section.\r\n- [ ] Yes. Give a description of this change to be included in the release notes for MLflow users.\r\n\r\n(Details in 1-2 sentences. You can just refer to another PR with a description if this PR is part of a larger change.)\r\n\r\n### What component(s), interfaces, languages, and integrations does this PR affect?\r\nComponents \r\n- [ ] `area\/artifacts`: Artifact stores and artifact logging\r\n- [ ] `area\/build`: Build and test infrastructure for MLflow\r\n- [ ] `area\/docs`: MLflow documentation pages\r\n- [ ] `area\/examples`: Example code\r\n- [ ] `area\/model-registry`: Model Registry service, APIs, and the fluent client calls for Model Registry\r\n- [ ] `area\/models`: MLmodel format, model serialization\/deserialization, flavors\r\n- [ ] `area\/projects`: MLproject format, project running backends\r\n- [ ] `area\/scoring`: MLflow Model server, model deployment tools, Spark UDFs\r\n- [ ] `area\/server-infra`: MLflow Tracking server backend\r\n- [ ] `area\/tracking`: Tracking Service, tracking client APIs, autologging\r\n\r\nInterface \r\n- [ ] `area\/uiux`: Front-end, user experience, plotting, JavaScript, JavaScript dev server\r\n- [ ] `area\/docker`: Docker use across MLflow's components, such as MLflow Projects and MLflow Models\r\n- [ ] `area\/sqlalchemy`: Use of SQLAlchemy in the Tracking Service or Model Registry\r\n- [ ] `area\/windows`: Windows support\r\n\r\nLanguage \r\n- [ ] `language\/r`: R APIs and clients\r\n- [ ] `language\/java`: Java APIs and clients\r\n- [ ] `language\/new`: Proposals for new client languages\r\n\r\nIntegrations\r\n- [ ] `integrations\/azure`: Azure and Azure ML integrations\r\n- [ ] `integrations\/sagemaker`: SageMaker integrations\r\n- [ ] `integrations\/databricks`: Databricks integrations\r\n\r\n<!--\r\nInsert an empty named anchor here to allow jumping to this section with a fragment URL\r\n(e.g. https:\/\/github.com\/mlflow\/mlflow\/pull\/123#user-content-release-note-category).\r\nNote that GitHub prefixes anchor names in markdown with \"user-content-\".\r\n-->\r\n<a name=\"release-note-category\"><\/a>\r\n### How should the PR be classified in the release notes? Choose one:\r\n\r\n- [ ] `rn\/breaking-change` - The PR will be mentioned in the \"Breaking Changes\" section\r\n- [ ] `rn\/none` - No description will be included. The PR will be mentioned only by the PR number in the \"Small Bugfixes and Documentation Updates\" section\r\n- [ ] `rn\/feature` - A new user-facing feature worth mentioning in the release notes\r\n- [ ] `rn\/bug-fix` - A user-facing bug fix worth mentioning in the release notes\r\n- [ ] `rn\/documentation` - A user-facing documentation change worth mentioning in the release notes\r\n","pull_request":{"url":"https:\/\/api.github.com\/repos\/mlflow\/mlflow\/pulls\/5378","html_url":"https:\/\/github.com\/mlflow\/mlflow\/pull\/5378","diff_url":"https:\/\/github.com\/mlflow\/mlflow\/pull\/5378.diff","patch_url":"https:\/\/github.com\/mlflow\/mlflow\/pull\/5378.patch","merged_at":null},"labels":[],"labels_description":[],"entities":["endpoint"]},{"html_url":"https:\/\/github.com\/mlflow\/mlflow\/issues\/5376","id":1137183207,"number":5376,"title":"modelinfo should contain registered model version[fr]","state":"open","locked":false,"assignee":null,"assignees":[],"comments":8,"created_at":1644839086000,"updated_at":1645470341000,"closed_at":null,"body":"## Willingness to contribute\r\nThe MLflow Community encourages new feature contributions. Would you or another member of your organization be willing to contribute an implementation of this feature (either as an MLflow Plugin or an enhancement to the MLflow code base)?\r\n\r\n- [ ] Yes. I can contribute this feature independently.\r\n- [x] Yes. I would be willing to contribute this feature with guidance from the MLflow community.\r\n- [ ] No. I cannot contribute this feature at this time.\r\n\r\n## Proposal Summary\r\nI apologise in advance as I am probably just using MLFlow wrong, so please educate me if there are better ways of achieving my goal :)\r\n\r\nMy goal is to register a Model with both Schema and custom Tags.\r\n\r\nI have been using `create_model_version` using the example from [here](https:\/\/www.mlflow.org\/docs\/latest\/python_api\/mlflow.tracking.html#mlflow.tracking.MlflowClient.create_model_version). The problem is I do not get the Schema even though I set signature in `mlflow.sklearn.log_model`.\r\n\r\nI am able to get the Schema set when I register the model using `mlflow.sklearn.log_model` by setting `registered_model_name`, however then I cannot find an elegant way of setting the tags.\r\nWhen running log_model it prints the resulting version number, but it does not return it in the modelInfo object, so I cannot find an elegant solution to how to set the tags afterwards using `set_model_version_tag`, I currently use `get_registered_model` and just assume the latest is the one I just registered.\r\n\r\nIf this is just not me using MLFlow wrong I believe this can be solved in two ways:\r\n- tags added as a parameter to all log_model functions\r\n- registered_model_version returned as part of ModelInfo when setting `registered_model_name` in `log_model`\r\n\r\n## Motivation\r\n- What is the use case for this feature?\r\nAbility to elegantly both set both Schema and tags on a registered model\r\n- Why is this use case valuable to support for MLflow users in general?\r\n- Why is this use case valuable to support for your project(s) or organization?\r\n- Why is it currently difficult to achieve this use case? (please be as specific as possible about why related MLflow features and components are insufficient)\r\n\r\n### What component(s), interfaces, languages, and integrations does this feature affect?\r\nComponents \r\n- [ ] `area\/artifacts`: Artifact stores and artifact logging\r\n- [ ] `area\/build`: Build and test infrastructure for MLflow\r\n- [ ] `area\/docs`: MLflow documentation pages\r\n- [ ] `area\/examples`: Example code\r\n- [ ] `area\/model-registry`: Model Registry service, APIs, and the fluent client calls for Model Registry\r\n- [x] `area\/models`: MLmodel format, model serialization\/deserialization, flavors\r\n- [ ] `area\/projects`: MLproject format, project running backends\r\n- [ ] `area\/scoring`: MLflow Model server, model deployment tools, Spark UDFs\r\n- [ ] `area\/server-infra`: MLflow Tracking server backend\r\n- [x] `area\/tracking`: Tracking Service, tracking client APIs, autologging\r\n\r\nInterfaces\r\n- [ ] `area\/uiux`: Front-end, user experience, plotting, JavaScript, JavaScript dev server\r\n- [ ] `area\/docker`: Docker use across MLflow's components, such as MLflow Projects and MLflow Models\r\n- [ ] `area\/sqlalchemy`: Use of SQLAlchemy in the Tracking Service or Model Registry\r\n- [ ] `area\/windows`: Windows support\r\n\r\nLanguages \r\n- [ ] `language\/r`: R APIs and clients\r\n- [ ] `language\/java`: Java APIs and clients\r\n- [ ] `language\/new`: Proposals for new client languages\r\n\r\nIntegrations\r\n- [ ] `integrations\/azure`: Azure and Azure ML integrations\r\n- [ ] `integrations\/sagemaker`: SageMaker integrations\r\n- [ ] `integrations\/databricks`: Databricks integrations\r\n\r\n## Details\r\n\r\n(Use this section to include any additional information about the feature. If you have a proposal for how to implement this feature, please include it here. For implementation guidelines, please refer to the [Contributing Guide](https:\/\/github.com\/mlflow\/mlflow\/blob\/master\/CONTRIBUTING.rst#contribution-guidelines).)\r\n","pull_request":null,"labels":["enhancement","area\/models","area\/tracking"],"labels_description":["New feature or request","MLmodel format, model serialization\/deserialization, flavors","Tracking service, tracking client APIs, autologging"],"entities":["modelinfo","model"]},{"html_url":"https:\/\/github.com\/mlflow\/mlflow\/issues\/5372","id":1133230534,"number":5372,"title":"[bug] mlflow.sklearn.log_model raises typeerror: expected string or bytes-like object","state":"open","locked":false,"assignee":null,"assignees":[],"comments":4,"created_at":1644619622000,"updated_at":1645826474000,"closed_at":null,"body":"### Willingness to contribute\r\n- [ ] Yes. I can contribute a fix for this bug independently.\r\n- [X] Yes. I would be willing to contribute a fix for this bug with guidance from the MLflow community.\r\n- [ ] No. I cannot contribute a bug fix at this time.\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in MLflow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Microsoft Windows 10 Home Version 21H2\r\n- **MLflow installed from (source or binary)**: binary\r\n- **MLflow version (run ``mlflow --version``)**: 1.23.1\r\n- **Python version**: 3.9.4\r\n- **npm version, if running the dev UI**: N\/A\r\n- **Exact command to reproduce**: see below\r\n\r\n### Describe the problem\r\nCalling mlflow.sklearn.log_model function raises `TypeError: expected string or bytes-like object`.\r\n\r\n### Code to reproduce issue\r\n```\r\nimport mlflow\r\nmlflow.set_tracking_uri(MLFLOW_TRACKING_URI)\r\nmlflow.set_experiment(MLFLOW_EXPERIMENT)\r\nfrom sklearn.datasets import load_iris\r\nfrom sklearn import tree\r\niris = load_iris()\r\nsk_model = tree.DecisionTreeClassifier()\r\nsk_model = sk_model.fit(iris.data, iris.target)\r\nmlflow.sklearn.log_model(sk_model, str(\"sk_models\"))\r\n```\r\n\r\n### Other info \/ logs\r\nThis seems to be due to `importlib_metadata.version` function, which tries to get the installed version of scikit-learn, but it returns None. I have installed scikit-learn 1.0.2 through `pip install scikit-learn`, and the following code correctly prints 1.0.2:\r\n```\r\nimport sklearn\r\nprint(sklearn.__version__)\r\n```\r\n\r\n### What component(s), interfaces, languages, and integrations does this bug affect?\r\nComponents \r\n- [ ] `area\/artifacts`: Artifact stores and artifact logging\r\n- [ ] `area\/build`: Build and test infrastructure for MLflow\r\n- [ ] `area\/docs`: MLflow documentation pages\r\n- [X] `area\/examples`: Example code\r\n- [ ] `area\/model-registry`: Model Registry service, APIs, and the fluent client calls for Model Registry\r\n- [ ] `area\/models`: MLmodel format, model serialization\/deserialization, flavors\r\n- [ ] `area\/projects`: MLproject format, project running backends\r\n- [ ] `area\/scoring`: MLflow Model server, model deployment tools, Spark UDFs\r\n- [ ] `area\/server-infra`: MLflow Tracking server backend\r\n- [X] `area\/tracking`: Tracking Service, tracking client APIs, autologging\r\n\r\nInterface \r\n- [ ] `area\/uiux`: Front-end, user experience, plotting, JavaScript, JavaScript dev server\r\n- [ ] `area\/docker`: Docker use across MLflow's components, such as MLflow Projects and MLflow Models\r\n- [ ] `area\/sqlalchemy`: Use of SQLAlchemy in the Tracking Service or Model Registry\r\n- [ ] `area\/windows`: Windows support\r\n\r\nLanguage \r\n- [ ] `language\/r`: R APIs and clients\r\n- [ ] `language\/java`: Java APIs and clients\r\n- [ ] `language\/new`: Proposals for new client languages\r\n\r\nIntegrations\r\n- [ ] `integrations\/azure`: Azure and Azure ML integrations\r\n- [ ] `integrations\/sagemaker`: SageMaker integrations\r\n- [ ] `integrations\/databricks`: Databricks integrations\r\n","pull_request":null,"labels":["bug","area\/examples","area\/tracking"],"labels_description":["Something isn't working","Example code","Tracking service, tracking client APIs, autologging"],"entities":["string","object"]},{"html_url":"https:\/\/github.com\/mlflow\/mlflow\/pull\/5371","id":1132744593,"number":5371,"title":"inject mlproject environment variables to the docker image when running on kubernetes","state":"open","locked":false,"assignee":null,"assignees":[],"comments":2,"created_at":1644592913000,"updated_at":1645827285000,"closed_at":null,"body":"## What changes are proposed in this pull request?\r\n\r\nWhen running on Kubernetes, the environment variables provided in the MLproject file are not taken into account.\r\n\r\n```\r\ndocker_env:\r\n  image: mlflow-docker-example-environment\r\n  environment: [[\"NEW_ENV_VAR\", \"new_var_value\"], \"VAR_TO_COPY_FROM_HOST_ENVIRONMENT\"]\r\n```\r\n\r\nfrom: https:\/\/www.mlflow.org\/docs\/latest\/projects.html#specifying-an-environment\r\n\r\n## How is this patch tested?\r\n\r\nManually.\r\n\r\n## Does this PR change the documentation?\r\n\r\n- [x] No. You can skip the rest of this section.\r\n- [ ] Yes. Make sure the changed pages \/ sections render correctly by following the steps below.\r\n\r\n1. Check the status of the `ci\/circleci: build_doc` check. If it's successful, proceed to the\r\n   next step, otherwise fix it.\r\n2. Click `Details` on the right to open the job page of CircleCI.\r\n3. Click the `Artifacts` tab.\r\n4. Click `docs\/build\/html\/index.html`.\r\n5. Find the changed pages \/ sections and make sure they render correctly.\r\n\r\n## Release Notes\r\n\r\n### Is this a user-facing change?\r\n\r\n- [ ] No. You can skip the rest of this section.\r\n- [x] Yes. Give a description of this change to be included in the release notes for MLflow users.\r\n\r\nInject the environment variables provided in the MLproject into the Docker image when running on Kubernetes\r\n\r\n### What component(s), interfaces, languages, and integrations does this PR affect?\r\nComponents \r\n- [ ] `area\/artifacts`: Artifact stores and artifact logging\r\n- [ ] `area\/build`: Build and test infrastructure for MLflow\r\n- [ ] `area\/docs`: MLflow documentation pages\r\n- [ ] `area\/examples`: Example code\r\n- [ ] `area\/model-registry`: Model Registry service, APIs, and the fluent client calls for Model Registry\r\n- [ ] `area\/models`: MLmodel format, model serialization\/deserialization, flavors\r\n- [ ] `area\/projects`: MLproject format, project running backends\r\n- [ ] `area\/scoring`: MLflow Model server, model deployment tools, Spark UDFs\r\n- [ ] `area\/server-infra`: MLflow Tracking server backend\r\n- [ ] `area\/tracking`: Tracking Service, tracking client APIs, autologging\r\n\r\nInterface \r\n- [ ] `area\/uiux`: Front-end, user experience, plotting, JavaScript, JavaScript dev server\r\n- [x] `area\/docker`: Docker use across MLflow's components, such as MLflow Projects and MLflow Models\r\n- [ ] `area\/sqlalchemy`: Use of SQLAlchemy in the Tracking Service or Model Registry\r\n- [ ] `area\/windows`: Windows support\r\n\r\nLanguage \r\n- [ ] `language\/r`: R APIs and clients\r\n- [ ] `language\/java`: Java APIs and clients\r\n- [ ] `language\/new`: Proposals for new client languages\r\n\r\nIntegrations\r\n- [ ] `integrations\/azure`: Azure and Azure ML integrations\r\n- [ ] `integrations\/sagemaker`: SageMaker integrations\r\n- [ ] `integrations\/databricks`: Databricks integrations\r\n\r\n<!--\r\nInsert an empty named anchor here to allow jumping to this section with a fragment URL\r\n(e.g. https:\/\/github.com\/mlflow\/mlflow\/pull\/123#user-content-release-note-category).\r\nNote that GitHub prefixes anchor names in markdown with \"user-content-\".\r\n-->\r\n<a name=\"release-note-category\"><\/a>\r\n### How should the PR be classified in the release notes? Choose one:\r\n\r\n- [ ] `rn\/breaking-change` - The PR will be mentioned in the \"Breaking Changes\" section\r\n- [ ] `rn\/none` - No description will be included. The PR will be mentioned only by the PR number in the \"Small Bugfixes and Documentation Updates\" section\r\n- [ ] `rn\/feature` - A new user-facing feature worth mentioning in the release notes\r\n- [x] `rn\/bug-fix` - A user-facing bug fix worth mentioning in the release notes\r\n- [ ] `rn\/documentation` - A user-facing documentation change worth mentioning in the release notes\r\n","pull_request":{"url":"https:\/\/api.github.com\/repos\/mlflow\/mlflow\/pulls\/5371","html_url":"https:\/\/github.com\/mlflow\/mlflow\/pull\/5371","diff_url":"https:\/\/github.com\/mlflow\/mlflow\/pull\/5371.diff","patch_url":"https:\/\/github.com\/mlflow\/mlflow\/pull\/5371.patch","merged_at":null},"labels":["rn\/bug-fix","area\/docker"],"labels_description":["Mention under Bug Fixes in Changelogs.","Docker use anywhere, such as MLprojects and MLmodels"],"entities":["mlproject","environment","docker","image"]},{"html_url":"https:\/\/github.com\/mlflow\/mlflow\/issues\/5365","id":1129273287,"number":5365,"title":"[bug] extra run results (past 100), loaded with the \"load more\" button, disappear as soon as you sort runs by a column.","state":"open","locked":false,"assignee":null,"assignees":[],"comments":0,"created_at":1644454595000,"updated_at":1644454638000,"closed_at":null,"body":"- [ ] Yes. I can contribute a fix for this bug independently.\r\n- [x] Yes. I would be willing to contribute a fix for this bug with guidance from the MLflow community.\r\n- [ ] No. I cannot contribute a bug fix at this time.\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in MLflow)**: no\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Microsoft Windows 10 Version 20H2\r\n- **MLflow installed from (source or binary)**: pip\r\n- **MLflow version (run ``mlflow --version``)**: 1.22.0\r\n- **Python version**: 3.8.5\r\n- **npm version, if running the dev UI**: 7.11.0\r\n- **Exact command to reproduce**: \"mlflow ui\" -> navigate to an experiment with over 100 run results\r\n\r\n### Describe the problem\r\nMLFlow UI only displays 100 run results for 100+ run experiments. Clicking \"Load more\" in the UI loads additional runs. However, those additional runs immediately disappear as soon as you try to sort by a column in the table, thus defeating the point of loading more results into the table. \r\n\r\n### What component(s), interfaces, languages, and integrations does this bug affect?\r\nComponents \r\n- [ ] `area\/artifacts`: Artifact stores and artifact logging\r\n- [ ] `area\/build`: Build and test infrastructure for MLflow\r\n- [ ] `area\/docs`: MLflow documentation pages\r\n- [ ] `area\/examples`: Example code\r\n- [ ] `area\/model-registry`: Model Registry service, APIs, and the fluent client calls for Model Registry\r\n- [ ] `area\/models`: MLmodel format, model serialization\/deserialization, flavors\r\n- [ ] `area\/projects`: MLproject format, project running backends\r\n- [ ] `area\/scoring`: MLflow Model server, model deployment tools, Spark UDFs\r\n- [ ] `area\/server-infra`: MLflow Tracking server backend\r\n- [ ] `area\/tracking`: Tracking Service, tracking client APIs, autologging\r\n\r\nInterface \r\n- [x] `area\/uiux`: Front-end, user experience, plotting, JavaScript, JavaScript dev server\r\n- [ ] `area\/docker`: Docker use across MLflow's components, such as MLflow Projects and MLflow Models\r\n- [ ] `area\/sqlalchemy`: Use of SQLAlchemy in the Tracking Service or Model Registry\r\n- [ ] `area\/windows`: Windows support\r\n\r\nLanguage \r\n- [ ] `language\/r`: R APIs and clients\r\n- [ ] `language\/java`: Java APIs and clients\r\n- [ ] `language\/new`: Proposals for new client languages\r\n\r\nIntegrations\r\n- [ ] `integrations\/azure`: Azure and Azure ML integrations\r\n- [ ] `integrations\/sagemaker`: SageMaker integrations\r\n- [ ] `integrations\/databricks`: Databricks integrations\r\n","pull_request":null,"labels":["bug","area\/uiux"],"labels_description":["Something isn't working","Front-end, user experience, plotting, JavaScript, JavaScript dev server"],"entities":["run","(past","\"load","more\"","button,","column."]},{"html_url":"https:\/\/github.com\/mlflow\/mlflow\/issues\/5364","id":1128321921,"number":5364,"title":"[bug] the scatter plot is empty when this is downloaded.","state":"open","locked":false,"assignee":null,"assignees":[],"comments":1,"created_at":1644401276000,"updated_at":1644401510000,"closed_at":null,"body":"### Willingness to contribute\r\nThe MLflow Community encourages bug fix contributions. Would you or another member of your organization be willing to contribute a fix for this bug to the MLflow code base?\r\n\r\n- [ ] Yes. I can contribute a fix for this bug independently.\r\n- [ ] Yes. I would be willing to contribute a fix for this bug with guidance from the MLflow community.\r\n- [X] No. I cannot contribute a bug fix at this time.\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in MLflow)**:\r\nNo\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n[ Python 3.9 Dockerimage](https:\/\/hub.docker.com\/layers\/python\/library\/python\/3.9\/images\/sha256-cffe05d2e4135f5893fa96754fd672d314ae48b160e072f9c4d42843daeedfc4?context=explore)\r\n- **MLflow installed from (source or binary)**:\r\nPip\r\n- **MLflow version (run ``mlflow --version``)**:\r\n1.22.0\r\n- **Python version**:\r\n3.9\r\n- **npm version, if running the dev UI**:\r\n\/\r\n- **Exact command to reproduce**:\r\nDownload a scatter plot\r\n\r\n### Describe the problem\r\nWhen I compare multiple runs in the tracking UI and create a scatter plot, it displays correctly. Then when I download the chart, the data points are missing from the chart.\r\n\r\n### Code to reproduce issue\r\n\/\r\n\r\n### Other info \/ logs\r\nI got a ``Failed to load resource`` error in the console in Safari's development tools when I pressed the plot's download button. A base64 data string (probably from the plot) is also displayed. \r\n\r\n\r\n### What component(s), interfaces, languages, and integrations does this bug affect?\r\nComponents \r\n- [ ] `area\/artifacts`: Artifact stores and artifact logging\r\n- [ ] `area\/build`: Build and test infrastructure for MLflow\r\n- [ ] `area\/docs`: MLflow documentation pages\r\n- [ ] `area\/examples`: Example code\r\n- [ ] `area\/model-registry`: Model Registry service, APIs, and the fluent client calls for Model Registry\r\n- [ ] `area\/models`: MLmodel format, model serialization\/deserialization, flavors\r\n- [ ] `area\/projects`: MLproject format, project running backends\r\n- [ ] `area\/scoring`: MLflow Model server, model deployment tools, Spark UDFs\r\n- [ ] `area\/server-infra`: MLflow Tracking server backend\r\n- [X] `area\/tracking`: Tracking Service, tracking client APIs, autologging\r\n\r\nInterface \r\n- [ ] `area\/uiux`: Front-end, user experience, plotting, JavaScript, JavaScript dev server\r\n- [ ] `area\/docker`: Docker use across MLflow's components, such as MLflow Projects and MLflow Models\r\n- [ ] `area\/sqlalchemy`: Use of SQLAlchemy in the Tracking Service or Model Registry\r\n- [ ] `area\/windows`: Windows support\r\n\r\nLanguage \r\n- [ ] `language\/r`: R APIs and clients\r\n- [ ] `language\/java`: Java APIs and clients\r\n- [ ] `language\/new`: Proposals for new client languages\r\n\r\nIntegrations\r\n- [ ] `integrations\/azure`: Azure and Azure ML integrations\r\n- [ ] `integrations\/sagemaker`: SageMaker integrations\r\n- [ ] `integrations\/databricks`: Databricks integrations\r\n","pull_request":null,"labels":["bug","area\/tracking"],"labels_description":["Something isn't working","Tracking service, tracking client APIs, autologging"],"entities":["scatter","plot"]},{"html_url":"https:\/\/github.com\/mlflow\/mlflow\/issues\/5361","id":1128071877,"number":5361,"title":"not able to tag experiment to user in cloud jupyter notebook for running transformer models","state":"open","locked":false,"assignee":null,"assignees":[],"comments":0,"created_at":1644384450000,"updated_at":1644384450000,"closed_at":null,"body":"I am using `mlflow version 1.22.0.` \r\nHow can I pass namespace to mlflow experiment in PAZ notebook such that I can see experiment from ai platform.\r\nCurrently I am using this: \r\n\r\n```\r\nimport os\r\nimport mlflow\r\nos.environ['GIT_PYTHON_REFRESH']='quiet'\r\nmlflow.set_tracking_uri(os.environ['MLFLOW_TRACKING_URL'])\r\nprint(os.environ['MLFLOW_TRACKING_URL'])\r\nexperiment_id = mlflow.create_experiment(\"Experiment\")\r\nexperiment = mlflow.get_experiment(experiment_id)\r\nprint(\"Name: {}\".format(experiment.name))\r\nprint(\"Experiment_id: {}\".format(experiment.experiment_id))\r\nprint(\"Artifact Location: {}\".format(experiment.artifact_location))\r\nprint(\"Tags: {}\".format(experiment.tags))\r\nprint(\"Lifecycle_stage: {}\".format(experiment.lifecycle_stage))\r\nmlflow.set_experiment('Experiment')\r\n```\r\n\r\nBut I want to tag it to to user namespace something like: \r\n\r\n`mlflow.set_experiment('Experiment',os.environ['USER'])`\r\n\r\n`Error: MlflowException: Must specify exactly one of: `experiment_id` or `experiment_name`.`\r\n\r\n**I want to use same version of mlflow. Is there another way to tag USER?**\r\n\r\nAlso in MLFLOW UI, I see blank `Run Name`, `Version`, `Models`. How to se these values for different runs?\r\n\r\nI want to do something like `mlflow.run_name('RUN1')`, `mlflow.Models('Model1')`\r\n\r\nI am not using `mlflow.start(...)` as hugging face automatically  tracks it.\r\n\r\n\r\n\r\n\r\n\r\n","pull_request":null,"labels":["bug"],"labels_description":["Something isn't working"],"entities":["experiment","user","cloud","jupyter","notebook","transformer"]},{"html_url":"https:\/\/github.com\/mlflow\/mlflow\/issues\/5354","id":1126041737,"number":5354,"title":"[bug] impossible to see run names in experiments comparision mode","state":"open","locked":false,"assignee":null,"assignees":[],"comments":0,"created_at":1644242821000,"updated_at":1644242880000,"closed_at":null,"body":"\r\n![image](https:\/\/user-images.githubusercontent.com\/27057946\/152803441-cdb73a60-2603-4686-91df-ceb99633727f.png)\r\n\r\n\r\nWhen a run name is quite long it is impossible to see it even from page sources. ","pull_request":null,"labels":["bug"],"labels_description":["Something isn't working"],"entities":["comparision","mode"]},{"html_url":"https:\/\/github.com\/mlflow\/mlflow\/issues\/5351","id":1125407749,"number":5351,"title":"[bug] unable to load when there is more than 1 model","state":"open","locked":false,"assignee":null,"assignees":[],"comments":2,"created_at":1644198069000,"updated_at":1646217751000,"closed_at":null,"body":"I have 2 keras model store locally, so i registerd both of them as 1 combined model. I am able to register the model into AML workspace as well as creating a endpoint by simply specifying my score.py to call both model to return one results. \r\nCalling it as an endpoint works perfectly fine\r\n\r\nHowever for retraining purpose, i want to load the model from mdoel registery but hit multiple issue.\r\nModel Name : Neural-model\r\nVersion: 3\r\nrun_id : xxxx\r\nFolder path for model 1: \/model\/type\/\r\nFolder path for model 2: \/model\/classification\/\r\n\r\n\r\n**I tried:**\r\n\r\nrun_id='xxxx'\r\nrun=\"runs:\/{}\".format(run_id)\r\nkeras_model = mlflow.keras.load_model(run + \"\/model\/type\/\/\")\r\n\r\nError:\r\nMlflowException: Could not find an \"MLmodel\" configuration file at \"\/tmp\/tmpv9jiqje9\/model\/type\/\"\r\n\r\n**i tried using model URI as well:**\r\n\r\n#model_uri='models:\/Neural-model\/3'\r\nclassifier_model = mlflow.keras.load_model(model_uri=model_uri,compile=False)\r\n\r\nError:\r\n\r\nRestException: BAD_REQUEST: Response: {'Error': {'Code': 'UserError', 'Severity': None, 'Message': 'Model source model must be a directory containing an mlflow MLmodel.', 'MessageFormat': None, 'MessageParameters': None, 'ReferenceCode': None, 'DetailsUri': None, 'Target': None, 'Details': [], 'InnerError': None, 'DebugInfo': None, 'AdditionalInfo': None}, 'Correlation': {'operation': '481ee7c36a8d5b44a4114a4cc9de7cf3', 'request': 'f7a15b107e89454e'}, 'Environment': 'southeastasia', 'Location': 'southeastasia', 'Time': '2022-02-04T02:33:51.0933512+00:00', 'ComponentName': 'mlflow', 'error_code': 'BAD_REQUEST'}\r\n\r\n\r\nThe first method cannot find my model while the second cannot point to the model i want that is registerd.\r\nAny idea how to overcome multiple model deployment issue?\r\n\r\n","pull_request":null,"labels":["bug"],"labels_description":["Something isn't working"],"entities":["model"]},{"html_url":"https:\/\/github.com\/mlflow\/mlflow\/issues\/5349","id":1124686556,"number":5349,"title":"[fr] implement a way to filter experiments by tags","state":"open","locked":false,"assignee":null,"assignees":[],"comments":0,"created_at":1644015043000,"updated_at":1644015058000,"closed_at":null,"body":"## Willingness to contribute\r\nThe MLflow Community encourages new feature contributions. Would you or another member of your organization be willing to contribute an implementation of this feature (either as an MLflow Plugin or an enhancement to the MLflow code base)?\r\n\r\n- [ ] Yes. I can contribute this feature independently.\r\n- [X] Yes. I would be willing to contribute this feature with guidance from the MLflow community.\r\n- [ ] No. I cannot contribute this feature at this time.\r\n\r\n## Proposal Summary\r\n`CreateExperiments` permits attaching tags to an experiment.  But I can't find a way to filter experiments using those tags:\r\n- `ListExperiments` has no parameters for filtering\r\n- there isn't a `SearchExperiments` endpoint\r\n\r\n## Motivation\r\n- What is the use case for this feature?  I want to be able to organize experiments from different users and organizations.  Tags are often promising, but without a way to retrieve experiments based on tags, I don't see a way to use them.\r\n- Why is this use case valuable to support for MLflow users in general?  I'm probably not the only person who'd like to enable different users and organizations to be able to filter the experiments.\r\n- Why is this use case valuable to support for your project(s) or organization?  Different users may share the same MLflow installation, and want to look at their own experiments.\r\n- Why is it currently difficult to achieve this use case? (please be as specific as possible about why related MLflow features and components are insufficient) Not sure how to answer this: it's not implemented, so it's hard to do :)  Client side filtering of the API responses is possible so long as the number of experiments and users are small.  But as those grow over time, I expect it to be unwieldy and slow.\r\n\r\n### What component(s), interfaces, languages, and integrations does this feature affect?\r\nComponents \r\n- [ ] `area\/artifacts`: Artifact stores and artifact logging\r\n- [ ] `area\/build`: Build and test infrastructure for MLflow\r\n- [ ] `area\/docs`: MLflow documentation pages\r\n- [ ] `area\/examples`: Example code\r\n- [ ] `area\/model-registry`: Model Registry service, APIs, and the fluent client calls for Model Registry\r\n- [ ] `area\/models`: MLmodel format, model serialization\/deserialization, flavors\r\n- [ ] `area\/projects`: MLproject format, project running backends\r\n- [ ] `area\/scoring`: MLflow Model server, model deployment tools, Spark UDFs\r\n- [ ] `area\/server-infra`: MLflow Tracking server backend\r\n- [X] `area\/tracking`: Tracking Service, tracking client APIs, autologging\r\n\r\nInterfaces\r\n- [ ] `area\/uiux`: Front-end, user experience, plotting, JavaScript, JavaScript dev server\r\n- [ ] `area\/docker`: Docker use across MLflow's components, such as MLflow Projects and MLflow Models\r\n- [X] `area\/sqlalchemy`: Use of SQLAlchemy in the Tracking Service or Model Registry\r\n- [ ] `area\/windows`: Windows support\r\n\r\nLanguages \r\n- [ ] `language\/r`: R APIs and clients\r\n- [ ] `language\/java`: Java APIs and clients\r\n- [ ] `language\/new`: Proposals for new client languages\r\n\r\nIntegrations\r\n- [ ] `integrations\/azure`: Azure and Azure ML integrations\r\n- [ ] `integrations\/sagemaker`: SageMaker integrations\r\n- [ ] `integrations\/databricks`: Databricks integrations\r\n\r\n## Details\r\n\r\nCurrently, I'm focused only on the API - there's some new UI possibility here, but I'm going to ignore those issues for now.\r\n\r\nTwo approaches occur to me.\r\n\r\n### `SearchExperiments`\r\n\r\n`SearchExperiments` could be added, by analogy with e.g. `SearchRuns`.  This would involve implementing the search functionality at [1] for experiments.  This way, experiment search has a similar developer experience to other search endpoints.\r\n\r\nThis would allow callers to get a filtered list by tag using an expression like: `tags.<key> = <value>`.  Since the search syntax already supports boolean combinations, it'd be easy to implement a filtered view of experiments.\r\n\r\nPros:\r\n- suffices for my use-case\r\n- extends existing search features\r\n- the `Search...` endpoints use `POST`, so no encoding is needed\r\n\r\nCons:\r\n- more powerful than I need\r\n- some preferences exist against this design, e.g. [2]\r\n\r\n### `ListExperiments`\r\n\r\n`ListExperiments` could be extended with a `tags` parameter that accepts a (default empty) list of key-value pairs.  Those pair would *all* be applied as filters on the sqlalchemy query.\r\n\r\nThis would allow callers to get a filtered list by using an argument like `[<key>=<value>]`.  Since filtering by multiple tags is often useful, this should be a list so no expression parsing is required.  Each key-value pair would translate to a sqlalchemy filter on the underlying query.\r\n\r\nPros:\r\n- suffices for my use-case\r\n- simpler interface and probably implementation\r\n- searching by tag conjunction is a common paradigm\r\n\r\nCons:\r\n- `ListExpressions` uses GET, so the key value pairs need to be url-encoded\r\n- different developer experience when filtering experiments vs e.g. runs\r\n\r\n\r\n[1] - https:\/\/mlflow.org\/docs\/latest\/search-syntax.html\r\n[2] - https:\/\/github.com\/mlflow\/mlflow\/pull\/3881#issuecomment-754053632\r\n","pull_request":null,"labels":["enhancement","area\/tracking","area\/sqlalchemy"],"labels_description":["New feature or request","Tracking service, tracking client APIs, autologging","Use of SQL alchemy in tracking service or model registry"],"entities":["way"]},{"html_url":"https:\/\/github.com\/mlflow\/mlflow\/issues\/5341","id":1121895746,"number":5341,"title":"mlflow does not return an error when sorting on non-existing metrics","state":"open","locked":false,"assignee":null,"assignees":[],"comments":1,"created_at":1643806669000,"updated_at":1647338095000,"closed_at":null,"body":"### Willingness to contribute\r\n\r\n- [x] Yes. I can contribute a fix for this bug independently.\r\n\r\n### System information\r\n- **MLflow installed from (source or binary)**: binary\r\n- **MLflow version (run ``mlflow --version``)**: 1.22.0\r\n\r\n### Describe the problem\r\n\r\nWhen searching for runs using the `search_runs` API:\r\n\r\n```\r\nmlflow.search_runs([experiment_id], order_by=[\"metrics.m DESC\"])\r\n```\r\n\r\nThe given metric name in the `order_by` clause is not validated in any way. If it starts with `metrics.` and is properly quoted then using a non-existing metric name will happily return the list sorted by the default ordering (date).\r\n\r\n### Code to reproduce issue\r\n\r\n```\r\nmlflow.search_runs([experiment_id], order_by=[\"metrics.this_is_an_invalid_metric DESC\"])\r\n```\r\n\r\n### Other info \/ logs\r\n\r\nThese seem to be the only places where the search clauses are validated and the current code only checks the syntax of clauses about `metrics`:\r\n\r\nhttps:\/\/github.com\/mlflow\/mlflow\/blob\/HEAD\/mlflow\/utils\/search_utils.py#L153-L170\r\nhttps:\/\/github.com\/mlflow\/mlflow\/blob\/HEAD\/mlflow\/utils\/search_utils.py#L461-L480\r\n\r\nIf testing the available metrics is difficult then maybe it's worth validating the results to make sure the at least one of the returned rows has a metric with the given name.\r\n\r\n### What component(s), interfaces, languages, and integrations does this bug affect?\r\nComponents \r\n- [x] `area\/tracking`: Tracking Service, tracking client APIs, autologging\r\n\r\nInterface \r\n- [x] `area\/sqlalchemy`: Use of SQLAlchemy in the Tracking Service or Model Registry","pull_request":null,"labels":["area\/tracking","area\/sqlalchemy"],"labels_description":["Tracking service, tracking client APIs, autologging","Use of SQL alchemy in tracking service or model registry"],"entities":["error"]},{"html_url":"https:\/\/github.com\/mlflow\/mlflow\/issues\/5340","id":1121877219,"number":5340,"title":"[fr] make the azureml entry script accept more data types","state":"open","locked":false,"assignee":null,"assignees":[],"comments":3,"created_at":1643805607000,"updated_at":1643918704000,"closed_at":null,"body":"Thank you for submitting a feature request. **Before proceeding, please review MLflow's [Issue Policy for feature requests](https:\/\/www.github.com\/mlflow\/mlflow\/blob\/master\/ISSUE_POLICY.md#feature-requests) and the [MLflow Contributing Guide](https:\/\/github.com\/mlflow\/mlflow\/blob\/master\/CONTRIBUTING.rst)**.\r\n\r\n**Please fill in this feature request template to ensure a timely and thorough response.**\r\n\r\n## Willingness to contribute\r\nThe MLflow Community encourages new feature contributions. Would you or another member of your organization be willing to contribute an implementation of this feature (either as an MLflow Plugin or an enhancement to the MLflow code base)?\r\n\r\n- [x] Yes. I can contribute this feature independently.\r\n- [x] Yes. I would be willing to contribute this feature with guidance from the MLflow community.\r\n- [ ] No. I cannot contribute this feature at this time.\r\n\r\n## Proposal Summary\r\n\r\nCurrently, the required entry script for deployment of models onto the `azureml` ecosystem from `mlflow` is hardwired as a text string, and configurable using the `mlflow.pyfunc` functions for each model flavour. However, the current function to decoding JSON input data only supports Schema types associated with Pandas Dataframe tabular data, and does not for example, support the use **TFServing type tensor structures** which are important for image based applications.\r\n\r\nThe current string which is hard coded into the `mlflow.azureml.__init__.py` file is as follows:\r\n\r\n```python\r\nimport pandas as pd\r\nfrom azureml.core.model import Model\r\nfrom mlflow.pyfunc import load_model\r\nfrom mlflow.pyfunc.scoring_server import parse_json_input, _get_jsonable_obj\r\n\r\ndef init():\r\n    global model\r\n    model_path = Model.get_model_path(model_name=\"{model_name}\", version={model_version})\r\n    model = load_model(model_path)\r\n\r\ndef run(json_input):\r\n    input_df = parse_json_input(json_input=json_input, orient=\"split\")\r\n    return _get_jsonable_obj(model.predict(input_df), pandas_orient=\"records\")\r\n```\r\n\r\nThe [`parse_json_input`](https:\/\/github.com\/mlflow\/mlflow\/blob\/1335164d20a6521dd4ebfe3f361d19fe9672efcf\/mlflow\/pyfunc\/scoring_server\/__init__.py#L116) function, which comes form the module `mlflow.pyfunc.scoring_server`, has only the ability to handle Pandas Dataframe tabular data, and thus does not convert input JSON content to tensor (numpy ndarray) type objects. It doesn't support the TFServing tensor format, which would allow for decoding of these types of structures.\r\n\r\n\r\n## Motivation\r\n- What is the use case for this feature?\r\n\r\nA simple extension of a use case here is the ability to tensor type inputs as defined by the Tensorspec Schema type, and support the deserialization of numpy ndarrays, such as images. This opens up the potential for any type of application which uses ndarray objects as inputs, such as CNN applications.\r\n\r\n- Why is this use case valuable to support for MLflow users in general?\r\n\r\nIt broadens the scope of the types of models that can be deployed on AzureML using the model agnostic infrastructure developed through the `mlflow.pyfunc` flavours.\r\n\r\n- Why is this use case valuable to support for your project(s) or organization?\r\n\r\nPotential for many different types of models, not just Pandas dataframe tabular based data structures as input.\r\n\r\n- Why is it currently difficult to achieve this use case? (please be as specific as possible about why related MLflow features and components are insufficient)\r\n\r\nThe current implementation of the entry script is defined as a hard coded string in the `mlflow.azureml` module and cannot be overwritten or modified by any functional call. Therefore, the limitations of what data types can be deserialized by the JSON deserialization function used cannot be circumvented without changes to the said the module.\r\n\r\n### What component(s), interfaces, languages, and integrations does this feature affect?\r\nComponents \r\n- [ ] `area\/artifacts`: Artifact stores and artifact logging\r\n- [ ] `area\/build`: Build and test infrastructure for MLflow\r\n- [ ] `area\/docs`: MLflow documentation pages\r\n- [ ] `area\/examples`: Example code\r\n- [ ] `area\/model-registry`: Model Registry service, APIs, and the fluent client calls for Model Registry\r\n- [x] `area\/models`: MLmodel format, model serialization\/deserialization, flavors\r\n- [ ] `area\/projects`: MLproject format, project running backends\r\n- [x] `area\/scoring`: MLflow Model server, model deployment tools, Spark UDFs\r\n- [ ] `area\/server-infra`: MLflow Tracking server backend\r\n- [ ] `area\/tracking`: Tracking Service, tracking client APIs, autologging\r\n\r\nInterfaces\r\n- [ ] `area\/uiux`: Front-end, user experience, plotting, JavaScript, JavaScript dev server\r\n- [ ] `area\/docker`: Docker use across MLflow's components, such as MLflow Projects and MLflow Models\r\n- [ ] `area\/sqlalchemy`: Use of SQLAlchemy in the Tracking Service or Model Registry\r\n- [ ] `area\/windows`: Windows support\r\n\r\nLanguages \r\n- [ ] `language\/r`: R APIs and clients\r\n- [ ] `language\/java`: Java APIs and clients\r\n- [ ] `language\/new`: Proposals for new client languages\r\n\r\nIntegrations\r\n- [x] `integrations\/azure`: Azure and Azure ML integrations\r\n- [ ] `integrations\/sagemaker`: SageMaker integrations\r\n- [ ] `integrations\/databricks`: Databricks integrations\r\n\r\n## Details\r\n\r\n(Use this section to include any additional information about the feature. If you have a proposal for how to implement this feature, please include it here. For implementation guidelines, please refer to the [Contributing Guide](https:\/\/github.com\/mlflow\/mlflow\/blob\/master\/CONTRIBUTING.rst#contribution-guidelines).)\r\n\r\nOne potential solution to the problem is to use existing functionality that has been developed for other deployment types in the `mlflow` package. A number of JSON deserialization functions are contained within the `mlflow.pyfunc.scoring_serving` module which do have the ability to decode TFServing type tensor formats, and convert these to numpy ndarrays from the specified `mlflow.types.Schema`.\r\n\r\nIf an input of a numpy ndarray representing an image (e.g. shape of `image` is `(3,800,600)` array) is encapsulated and serialized in the following way (using the TFServing definintion):\r\n\r\n```python\r\npayload = {\r\n            'instances' : [\r\n                image.tolist()\r\n                ]\r\n        }\r\npayload = str.encode(json.dumps(payload)))\r\n```\r\n\r\nThen using the `mlflow.pyfunc.scoring_server` module function [`infer_and_parse_json_input()`](https:\/\/github.com\/mlflow\/mlflow\/blob\/1335164d20a6521dd4ebfe3f361d19fe9672efcf\/mlflow\/pyfunc\/scoring_server\/__init__.py#L76) in the entry script does result in successful decoding of the above JSON serialized numpy ndarray back to the correct sized object, which can then be passed to loaded model function for inference.\r\n\r\nA notional modification to the entry script could potentially be as simple as:\r\n\r\n```python\r\nimport pandas as pd\r\nfrom azureml.core.model import Model\r\nfrom mlflow.pyfunc import load_model\r\nfrom mlflow.pyfunc.scoring_server import infer_and_parse_json_input, _get_jsonable_obj\r\n\r\ndef init():\r\n    global model\r\n    model_path = Model.get_model_path(model_name=\"{model_name}\", version={model_version})\r\n    model = load_model(model_path)\r\n\r\ndef run(json_input):\r\n    input = infer_and_parse_json_input(json_input=json_input, orient=\"split\")\r\n    return _get_jsonable_obj(model.predict(input), pandas_orient=\"records\")\r\n```\r\n\r\nLocal testing of this functionality appears to be successful in decoding the JSON serialized image back to a `numpy ndarray` and was successfully accepted by a `PyTorch ONNX` model that takes `numpy ndarray` image representations as input.\r\n\r\nThere are a few issues to iron out, including the additional dimension returned by the `infer_and_parse_json_input()` function, that is a shape of `(1,3,nx,ny)` instead of the transmitted `(3,nx,ny)`. A simple `numpy.squeeze()` application solves this issue, however, `input` may not be only be a `numpy ndarray` so some digging into the `infer_and_parse_json_input()` function is required to see why.\r\n","pull_request":null,"labels":["enhancement","area\/models","area\/scoring","integrations\/azure"],"labels_description":["New feature or request","MLmodel format, model serialization\/deserialization, flavors","MLflow Model server, model deployment tools, Spark UDFs","Azure and Azure ML integrations"],"entities":["azureml","entry","script","data"]},{"html_url":"https:\/\/github.com\/mlflow\/mlflow\/issues\/5339","id":1121336299,"number":5339,"title":"[bug] tensorflow autolog doesn't capture batch size correct for tf dataset input","state":"open","locked":false,"assignee":null,"assignees":[],"comments":2,"created_at":1643761058000,"updated_at":1644343443000,"closed_at":null,"body":"### Willingness to contribute\r\nThe MLflow Community encourages bug fix contributions. Would you or another member of your organization be willing to contribute a fix for this bug to the MLflow code base?\r\n\r\n- [ ] Yes. I can contribute a fix for this bug independently.\r\n- [x] Yes. I would be willing to contribute a fix for this bug with guidance from the MLflow community.\r\n- [ ] No. I cannot contribute a bug fix at this time.\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in MLflow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **MLflow installed from (source or binary)**: pip?\r\n- **MLflow version (run ``mlflow --version``)**: 1.23.1\r\n- **Python version**: 3.8\r\n- **npm version, if running the dev UI**: n\/a\r\n- **Exact command to reproduce**: see code below\r\n\r\n\r\n### Describe the problem\r\n\r\nWhen calling `fit` with a tensorflow dataset with a batch operation applied, the mlflow tracking ui records batch size of None, not the correct batch size. Note that [the fit docs](https:\/\/keras.io\/api\/models\/model_training_apis\/#fit-method) instruct to not supply `batch_size` when the input data is a dataset:\r\n> batch_size: ... Do not specify the batch_size if your data is in the form of datasets, generators, or keras.utils.Sequence instances\r\n\r\nThe cause of this appears to be [this line](https:\/\/github.com\/mlflow\/mlflow\/blob\/1335164d20a6521dd4ebfe3f361d19fe9672efcf\/mlflow\/tensorflow\/__init__.py#L860) in the patched fit function for auto-logging, which logs the batch_size param, regardless of whether the input to fit was a batched dataset.\r\n\r\n### Code to reproduce issue\r\n\r\n```python\r\nimport mlflow\r\nimport tensorflow as tf\r\n\r\n\r\ndef gen():\r\n    for i in range(10):\r\n        yield i, -i\r\n\r\ndataset = tf.data.Dataset.from_generator(\r\n    gen,\r\n    output_signature=(\r\n        tf.TensorSpec(shape=(), dtype=tf.float32),\r\n        tf.TensorSpec(shape=(), dtype=tf.float32),\r\n    )\r\n)\r\ndataset = dataset.batch(2)\r\n\r\nmlflow.tensorflow.autolog()\r\n\r\nmodel = tf.keras.Sequential()\r\nmodel.add(tf.keras.layers.Dense(1, input_shape=(1,)))\r\nmodel.compile(loss=\"mse\")\r\nmodel.fit(dataset, epochs=10)\r\n```\r\n### Other info \/ logs\r\n\r\nN\/A\r\n\r\n### What component(s), interfaces, languages, and integrations does this bug affect?\r\nComponents \r\n- [ ] `area\/artifacts`: Artifact stores and artifact logging\r\n- [ ] `area\/build`: Build and test infrastructure for MLflow\r\n- [ ] `area\/docs`: MLflow documentation pages\r\n- [ ] `area\/examples`: Example code\r\n- [ ] `area\/model-registry`: Model Registry service, APIs, and the fluent client calls for Model Registry\r\n- [ ] `area\/models`: MLmodel format, model serialization\/deserialization, flavors\r\n- [ ] `area\/projects`: MLproject format, project running backends\r\n- [ ] `area\/scoring`: MLflow Model server, model deployment tools, Spark UDFs\r\n- [ ] `area\/server-infra`: MLflow Tracking server backend\r\n- [x] `area\/tracking`: Tracking Service, tracking client APIs, autologging\r\n\r\nInterface \r\n- [ ] `area\/uiux`: Front-end, user experience, plotting, JavaScript, JavaScript dev server\r\n- [ ] `area\/docker`: Docker use across MLflow's components, such as MLflow Projects and MLflow Models\r\n- [ ] `area\/sqlalchemy`: Use of SQLAlchemy in the Tracking Service or Model Registry\r\n- [ ] `area\/windows`: Windows support\r\n\r\nLanguage \r\n- [ ] `language\/r`: R APIs and clients\r\n- [ ] `language\/java`: Java APIs and clients\r\n- [ ] `language\/new`: Proposals for new client languages\r\n\r\nIntegrations\r\n- [ ] `integrations\/azure`: Azure and Azure ML integrations\r\n- [ ] `integrations\/sagemaker`: SageMaker integrations\r\n- [ ] `integrations\/databricks`: Databricks integrations\r\n","pull_request":null,"labels":["bug","area\/tracking"],"labels_description":["Something isn't working","Tracking service, tracking client APIs, autologging"],"entities":["tensorflow","autolog","batch","size","dataset","input"]},{"html_url":"https:\/\/github.com\/mlflow\/mlflow\/issues\/5338","id":1121243942,"number":5338,"title":"[fr] parallelize modelsartifactrepository.download_artifacts() file access","state":"open","locked":false,"assignee":{"login":"ankit-db","id":52183359,"node_id":"MDQ6VXNlcjUyMTgzMzU5","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/52183359?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/ankit-db","html_url":"https:\/\/github.com\/ankit-db","followers_url":"https:\/\/api.github.com\/users\/ankit-db\/followers","following_url":"https:\/\/api.github.com\/users\/ankit-db\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/ankit-db\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/ankit-db\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/ankit-db\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/ankit-db\/orgs","repos_url":"https:\/\/api.github.com\/users\/ankit-db\/repos","events_url":"https:\/\/api.github.com\/users\/ankit-db\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/ankit-db\/received_events","type":"User","site_admin":false},"assignees":[{"login":"ankit-db","id":52183359,"node_id":"MDQ6VXNlcjUyMTgzMzU5","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/52183359?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/ankit-db","html_url":"https:\/\/github.com\/ankit-db","followers_url":"https:\/\/api.github.com\/users\/ankit-db\/followers","following_url":"https:\/\/api.github.com\/users\/ankit-db\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/ankit-db\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/ankit-db\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/ankit-db\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/ankit-db\/orgs","repos_url":"https:\/\/api.github.com\/users\/ankit-db\/repos","events_url":"https:\/\/api.github.com\/users\/ankit-db\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/ankit-db\/received_events","type":"User","site_admin":false}],"comments":0,"created_at":1643752921000,"updated_at":1643753305000,"closed_at":null,"body":"Thank you for submitting a feature request. **Before proceeding, please review MLflow's [Issue Policy for feature requests](https:\/\/www.github.com\/mlflow\/mlflow\/blob\/master\/ISSUE_POLICY.md#feature-requests) and the [MLflow Contributing Guide](https:\/\/github.com\/mlflow\/mlflow\/blob\/master\/CONTRIBUTING.rst)**.\r\n\r\n**Please fill in this feature request template to ensure a timely and thorough response.**\r\n\r\n## Willingness to contribute\r\nThe MLflow Community encourages new feature contributions. Would you or another member of your organization be willing to contribute an implementation of this feature (either as an MLflow Plugin or an enhancement to the MLflow code base)?\r\n\r\n- [ ] Yes. I can contribute this feature independently.\r\n- [ ] Yes. I would be willing to contribute this feature with guidance from the MLflow community.\r\n- [x] No. I cannot contribute this feature at this time.\r\n\r\n## Proposal Summary\r\n\r\nParallelize the download of artifact files from ModelsArtifactRepository.\r\n\r\n## Motivation\r\n- What is the use case for this feature?\r\n\r\nModelsArtifactRepository.download_artifacts() currently access files in a single-threaded way.  For models with only a few artifacts, this is ok, however there are models out there with a great number of files.  This causes the initial download of the model to take much longer than it ideally should.\r\n\r\n- Why is this use case valuable to support for MLflow users in general?\r\n\r\nFaster download times for large models.\r\n\r\n- Why is this use case valuable to support for your project(s) or organization?\r\n\r\nIf our customer can download their large models faster, it will allow them to train\/develop models faster and distribute them to a wider user base.\r\n\r\n- Why is it currently difficult to achieve this use case? (please be as specific as possible about why related MLflow features and components are insufficient)\r\n\r\nThe existing MLFlow implementation currently doesn't parallelize these operations, causing the file I\/O to become a bottleneck.\r\n\r\n### What component(s), interfaces, languages, and integrations does this feature affect?\r\nComponents \r\n- [x] `area\/model-registry`: Model Registry service, APIs, and the fluent client calls for Model Registry\r\n- [x] `area\/models`: MLmodel format, model serialization\/deserialization, flavors\r\n\r\n\r\nInterfaces\r\n\r\n\r\nLanguages \r\n\r\n\r\nIntegrations\r\n\r\n\r\n## Details\r\n\r\n(Use this section to include any additional information about the feature. If you have a proposal for how to implement this feature, please include it here. For implementation guidelines, please refer to the [Contributing Guide](https:\/\/github.com\/mlflow\/mlflow\/blob\/master\/CONTRIBUTING.rst#contribution-guidelines).)\r\n","pull_request":null,"labels":["enhancement","good first issue","Acknowledged","area\/model-registry","area\/models","priority\/important-soon"],"labels_description":["New feature or request","Good for newcomers","This issue has been read and acknowledged by the MLflow admins.","Model registry, model registry APIs, and the fluent client calls for model registry","MLmodel format, model serialization\/deserialization, flavors","The issue is worked on by the community currently or will be very soon, ideally in time for the"],"entities":["file","access"]},{"html_url":"https:\/\/github.com\/mlflow\/mlflow\/issues\/5337","id":1121055907,"number":5337,"title":"[fr] use custom `predict_proba` with default evaluators","state":"open","locked":false,"assignee":null,"assignees":[],"comments":0,"created_at":1643739970000,"updated_at":1643740463000,"closed_at":null,"body":"Thank you for submitting a feature request. **Before proceeding, please review MLflow's [Issue Policy for feature requests](https:\/\/www.github.com\/mlflow\/mlflow\/blob\/master\/ISSUE_POLICY.md#feature-requests) and the [MLflow Contributing Guide](https:\/\/github.com\/mlflow\/mlflow\/blob\/master\/CONTRIBUTING.rst)**.\r\n\r\n**Please fill in this feature request template to ensure a timely and thorough response.**\r\n\r\n## Willingness to contribute\r\nThe MLflow Community encourages new feature contributions. Would you or another member of your organization be willing to contribute an implementation of this feature (either as an MLflow Plugin or an enhancement to the MLflow code base)?\r\n\r\n- [ ] Yes. I can contribute this feature independently.\r\n- [x] Yes. I would be willing to contribute this feature with guidance from the MLflow community.\r\n- [ ] No. I cannot contribute this feature at this time.\r\n\r\n## Proposal Summary\r\n\r\nBeing able to identify a custom `predict_proba` function from a custom pyfunc model to be able to fully use the \"default\" evaluators with pyfunc models and not only with sklearn models.\r\n \r\n## Motivation\r\n- What is the use case for this feature?\r\nIncrease the range of models that can use the \"default\" evaluators when running `mlflow.evaluate`\r\n- Why is this use case valuable to support for MLflow users in general?\r\nIt is I believe something easy to unlock that could be used by anyone building their own custom model with pyfunc\r\n- Why is this use case valuable to support for your project(s) or organization?\r\nWe would like to use all the evaluators in the group of \"default\" evaluators with our custom pyfunc classifier.\r\n- Why is it currently difficult to achieve this use case? (please be as specific as possible about why related MLflow features and components are insufficient)\r\nThis is currently not easy to achieve as we would have to define our own evaluators.\r\n\r\n### What component(s), interfaces, languages, and integrations does this feature affect?\r\nComponents \r\n- [ ] `area\/artifacts`: Artifact stores and artifact logging\r\n- [ ] `area\/build`: Build and test infrastructure for MLflow\r\n- [ ] `area\/docs`: MLflow documentation pages\r\n- [ ] `area\/examples`: Example code\r\n- [ ] `area\/model-registry`: Model Registry service, APIs, and the fluent client calls for Model Registry\r\n- [ ] `area\/models`: MLmodel format, model serialization\/deserialization, flavors\r\n- [ ] `area\/projects`: MLproject format, project running backends\r\n- [x] `area\/scoring`: MLflow Model server, model deployment tools, Spark UDFs\r\n- [ ] `area\/server-infra`: MLflow Tracking server backend\r\n- [ ] `area\/tracking`: Tracking Service, tracking client APIs, autologging\r\n\r\nInterfaces\r\n- [ ] `area\/uiux`: Front-end, user experience, plotting, JavaScript, JavaScript dev server\r\n- [ ] `area\/docker`: Docker use across MLflow's components, such as MLflow Projects and MLflow Models\r\n- [ ] `area\/sqlalchemy`: Use of SQLAlchemy in the Tracking Service or Model Registry\r\n- [ ] `area\/windows`: Windows support\r\n\r\nLanguages \r\n- [ ] `language\/r`: R APIs and clients\r\n- [ ] `language\/java`: Java APIs and clients\r\n- [ ] `language\/new`: Proposals for new client languages\r\n\r\nIntegrations\r\n- [ ] `integrations\/azure`: Azure and Azure ML integrations\r\n- [ ] `integrations\/sagemaker`: SageMaker integrations\r\n- [ ] `integrations\/databricks`: Databricks integrations\r\n\r\n## Details\r\n\r\nI propose to modify [this function](https:\/\/github.com\/mlflow\/mlflow\/blob\/af1df139aa107830fbfbc79e515df15f8f6e9f55\/mlflow\/models\/evaluation\/default_evaluator.py#L40) in https:\/\/github.com\/mlflow\/mlflow\/blob\/master\/mlflow\/models\/evaluation\/default_evaluator.py to just detect a `predict_proba` within a pyfunc model.\r\n\r\n(Use this section to include any additional information about the feature. If you have a proposal for how to implement this feature, please include it here. For implementation guidelines, please refer to the [Contributing Guide](https:\/\/github.com\/mlflow\/mlflow\/blob\/master\/CONTRIBUTING.rst#contribution-guidelines).)\r\n","pull_request":null,"labels":["enhancement","area\/scoring"],"labels_description":["New feature or request","MLflow Model server, model deployment tools, Spark UDFs"],"entities":["custom","`predict_proba`","default"]},{"html_url":"https:\/\/github.com\/mlflow\/mlflow\/issues\/5335","id":1120926253,"number":5335,"title":"[bug] pyspark required when using mlflow.evaluate","state":"open","locked":false,"assignee":null,"assignees":[],"comments":1,"created_at":1643733021000,"updated_at":1644612597000,"closed_at":null,"body":"Thank you for submitting an issue. Please refer to our [issue policy](https:\/\/www.github.com\/mlflow\/mlflow\/blob\/master\/ISSUE_POLICY.md) for additional information about bug reports. For help with debugging your code, please refer to [Stack Overflow](https:\/\/stackoverflow.com\/questions\/tagged\/mlflow).\r\n\r\n**Please fill in this bug report template to ensure a timely and thorough response.**\r\n\r\n### Willingness to contribute\r\nThe MLflow Community encourages bug fix contributions. Would you or another member of your organization be willing to contribute a fix for this bug to the MLflow code base?\r\n\r\n- [ ] Yes. I can contribute a fix for this bug independently.\r\n- [x] Yes. I would be willing to contribute a fix for this bug with guidance from the MLflow community.\r\n- [ ] No. I cannot contribute a bug fix at this time.\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in MLflow)**: yes -- custom Pyfunc model\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 18.04\r\n- **MLflow installed from (source or binary)**: source\r\n- **MLflow version (run ``mlflow --version``)**: 1.22.0\r\n- **Python version**: 3.9\r\n- **npm version, if running the dev UI**:\r\n- **Exact command to reproduce**: \r\n\r\n### Describe the problem\r\nI am running an evaluation of my model using `mlflow.evaluate` function. Here is the command used:\r\n```\r\nmlflow.evaluate(\r\n    model=model,  # custom pyfunc model\r\n    data=data.head(), # pandas dataframe\r\n    targets=\"label\",\r\n    model_type=\"classifier\",\r\n    dataset_name=\"test\",\r\n    dataset_path=args.data_uri,\r\n    evaluators=[\"default\"],\r\n)\r\n```\r\nAnd I run on the following error:\r\n```\r\nTraceback (most recent call last):\r\n  File \"evaluate.py\", line 42, in <module>\r\n    feature_names=[\"text\"]\r\n  File \"\/opt\/conda\/lib\/python3.7\/site-packages\/mlflow\/models\/evaluation\/base.py\", line 807, in evaluate\r\n    feature_names=feature_names,\r\n  File \"\/opt\/conda\/lib\/python3.7\/site-packages\/mlflow\/models\/evaluation\/base.py\", line 317, in __init__\r\n    if isinstance(data, spark_df_type):\r\nTypeError: isinstance() arg 2 must be a type or tuple of types\r\n```\r\nThe origin of this error seems to come from this part of the code in https:\/\/github.com\/mlflow\/mlflow\/blob\/master\/mlflow\/models\/evaluation\/base.py in `EvaluationDataset`:\r\n```\r\nif \"pyspark\" in sys.modules:\r\n    from pyspark.sql import DataFrame as SparkDataFrame\r\n\r\n    supported_dataframe_types = (pd.DataFrame, SparkDataFrame)\r\n    spark_df_type = SparkDataFrame\r\nelse:\r\n    supported_dataframe_types = (pd.DataFrame,)\r\n    spark_df_type = None\r\n```\r\nSo if pyspark is not installed in your modules it will throw the above error because `spark_df_type` is `None`. I believe that pyspark should not be a requirement here and that we should have a check before running `isinstance(data, spark_df_type)`\r\n\r\n### What component(s), interfaces, languages, and integrations does this bug affect?\r\nComponents \r\n- [ ] `area\/artifacts`: Artifact stores and artifact logging\r\n- [ ] `area\/build`: Build and test infrastructure for MLflow\r\n- [ ] `area\/docs`: MLflow documentation pages\r\n- [ ] `area\/examples`: Example code\r\n- [ ] `area\/model-registry`: Model Registry service, APIs, and the fluent client calls for Model Registry\r\n- [ ] `area\/models`: MLmodel format, model serialization\/deserialization, flavors\r\n- [ ] `area\/projects`: MLproject format, project running backends\r\n- [x] `area\/scoring`: MLflow Model server, model deployment tools, Spark UDFs\r\n- [ ] `area\/server-infra`: MLflow Tracking server backend\r\n- [ ] `area\/tracking`: Tracking Service, tracking client APIs, autologging\r\n\r\nInterface \r\n- [ ] `area\/uiux`: Front-end, user experience, plotting, JavaScript, JavaScript dev server\r\n- [ ] `area\/docker`: Docker use across MLflow's components, such as MLflow Projects and MLflow Models\r\n- [ ] `area\/sqlalchemy`: Use of SQLAlchemy in the Tracking Service or Model Registry\r\n- [ ] `area\/windows`: Windows support\r\n\r\nLanguage \r\n- [ ] `language\/r`: R APIs and clients\r\n- [ ] `language\/java`: Java APIs and clients\r\n- [ ] `language\/new`: Proposals for new client languages\r\n\r\nIntegrations\r\n- [ ] `integrations\/azure`: Azure and Azure ML integrations\r\n- [ ] `integrations\/sagemaker`: SageMaker integrations\r\n- [ ] `integrations\/databricks`: Databricks integrations\r\n","pull_request":null,"labels":["bug","area\/scoring","priority\/important-soon"],"labels_description":["Something isn't working","MLflow Model server, model deployment tools, Spark UDFs","The issue is worked on by the community currently or will be very soon, ideally in time for the"],"entities":["pyspark","mlflow.evaluate"]},{"html_url":"https:\/\/github.com\/mlflow\/mlflow\/issues\/5334","id":1120373446,"number":5334,"title":"xgb.predict() and  mlflow.predict() time difference for model prediction on same machine[bug]","state":"open","locked":false,"assignee":null,"assignees":[],"comments":0,"created_at":1643704480000,"updated_at":1643704581000,"closed_at":null,"body":"Thank you for submitting an issue. Please refer to our [issue policy](https:\/\/www.github.com\/mlflow\/mlflow\/blob\/master\/ISSUE_POLICY.md) for additional information about bug reports. For help with debugging your code, please refer to [Stack Overflow](https:\/\/stackoverflow.com\/questions\/tagged\/mlflow).\r\n\r\n**Please fill in this bug report template to ensure a timely and thorough response.**\r\n\r\n### Willingness to contribute\r\nThe MLflow Community encourages bug fix contributions. Would you or another member of your organization be willing to contribute a fix for this bug to the MLflow code base?\r\n\r\n- [ ] Yes. I can contribute a fix for this bug independently.\r\n- [ ] Yes. I would be willing to contribute a fix for this bug with guidance from the MLflow community.\r\n- [*] No. I cannot contribute a bug fix at this time.\r\n\r\n### System information\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: 4.14.243-185.433.amzn2.x86_64\r\n- **MLflow installed from (source or binary)**: source\r\n- **MLflow version (run ``mlflow --version``)**:1.17.0\r\n- **Python version**: 3.7.9\r\n\r\n### Describe the problem\r\n![image](https:\/\/user-images.githubusercontent.com\/61279516\/151935647-361f40a8-c516-48f4-8c45-34b5f590528a.png)\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/61279516\/151936434-0839443e-cd30-4a0c-9b26-239d4dcb23ec.png)\r\n\r\n\r\n\r\nGetting different inference time from the same model on same input when I run it independently and when I run it by wrapping in mlflow\r\n\r\nI am getting 2ms\/prediction from xgb when i run it independently on a input of shape (100,14) and when i wrap it in mlflow my response time from model gets increased to 20-60ms per prediction for same input size.\r\n\r\n@lichenran1234 @maitre-matt \r\n\r\n\r\n","pull_request":null,"labels":["bug"],"labels_description":["Something isn't working"],"entities":["mlflow.predict()","time","difference","model","prediction","machine[bug]"]},{"html_url":"https:\/\/github.com\/mlflow\/mlflow\/issues\/5332","id":1118683314,"number":5332,"title":"[bug] memory usage 400mb (very high) - can it be reduced?","state":"open","locked":false,"assignee":null,"assignees":[],"comments":1,"created_at":1643571358000,"updated_at":1643593430000,"closed_at":null,"body":"Hi,\r\n\r\nI installed MLflow in my one node Kubernetes cluster. It needs 400MB of RAM after fresh install\r\nand without any load. That is a lot...\r\n\r\nA fresh Postgres install needs 34Mi and MinIO only 115Mi.\r\n\r\nI am using MLflow 1-23-1 with external Postgres and MinIO.\r\nI am starting it with:\r\n\r\n```text\r\n        command:\r\n          - mlflow\r\n          - server\r\n          - -h\r\n          - \"0.0.0.0\"\r\n          - --backend-store-uri\r\n          - \"postgresql:\/\/$(DB_NAME):$(DB_PASSWORD)@$(DB_HOST)\/$(DB_NAME)\"\r\n          - --default-artifact-root\r\n          - \"s3:\/\/mlflow\/\"\r\n```\r\n\r\nIs there any way you could reduce memory usage or tell me how I can tune it?","pull_request":null,"labels":["bug"],"labels_description":["Something isn't working"],"entities":["memory","usage","high)"]},{"html_url":"https:\/\/github.com\/mlflow\/mlflow\/issues\/5331","id":1117933615,"number":5331,"title":"[fr] improve logarithm labels within metric plots with standard scientific notation","state":"open","locked":false,"assignee":null,"assignees":[],"comments":0,"created_at":1643412338000,"updated_at":1643412354000,"closed_at":null,"body":"Thank you for submitting a feature request. **Before proceeding, please review MLflow's [Issue Policy for feature requests](https:\/\/www.github.com\/mlflow\/mlflow\/blob\/master\/ISSUE_POLICY.md#feature-requests) and the [MLflow Contributing Guide](https:\/\/github.com\/mlflow\/mlflow\/blob\/master\/CONTRIBUTING.rst)**.\r\n\r\n**Please fill in this feature request template to ensure a timely and thorough response.**\r\n\r\n## Willingness to contribute\r\nThe MLflow Community encourages new feature contributions. Would you or another member of your organization be willing to contribute an implementation of this feature (either as an MLflow Plugin or an enhancement to the MLflow code base)?\r\n\r\n- [ ] Yes. I can contribute this feature independently.\r\n- [X] Yes. I would be willing to contribute this feature with guidance from the MLflow community.\r\n- [ ] No. I cannot contribute this feature at this time.\r\n\r\n## Proposal Summary\r\n\r\nRequest improvement to log-scale labels within metric plots by converting to standard scientific notation.\r\n\r\n## Motivation\r\n- What is the use case for this feature?\r\n- Interactive plotting of machine learning runs. Improvement obviates need for UI's like Tensorboard.\r\n- Why is this use case valuable to support for MLflow users in general?\r\n- Enhances dynamic analysis of metrics.\r\n- Why is this use case valuable to support for your project(s) or organization?\r\n- Helps keep my sanity and better conveys plot information\r\n- Why is it currently difficult to achieve this use case? (please be as specific as possible about why related MLflow features and components are insufficient)\r\n- Do not expect this fix to be difficult. However, personally unable to determine where plot labels are generated in source files. Not fluent in JS. \r\n\r\n### What component(s), interfaces, languages, and integrations does this feature affect?\r\nComponents \r\n- [ ] `area\/artifacts`: Artifact stores and artifact logging\r\n- [ ] `area\/build`: Build and test infrastructure for MLflow\r\n- [ ] `area\/docs`: MLflow documentation pages\r\n- [ ] `area\/examples`: Example code\r\n- [ ] `area\/model-registry`: Model Registry service, APIs, and the fluent client calls for Model Registry\r\n- [ ] `area\/models`: MLmodel format, model serialization\/deserialization, flavors\r\n- [ ] `area\/projects`: MLproject format, project running backends\r\n- [ ] `area\/scoring`: MLflow Model server, model deployment tools, Spark UDFs\r\n- [ ] `area\/server-infra`: MLflow Tracking server backend\r\n- [ ] `area\/tracking`: Tracking Service, tracking client APIs, autologging\r\n\r\nInterfaces\r\n- [X] `area\/uiux`: Front-end, user experience, plotting, JavaScript, JavaScript dev server\r\n- [ ] `area\/docker`: Docker use across MLflow's components, such as MLflow Projects and MLflow Models\r\n- [ ] `area\/sqlalchemy`: Use of SQLAlchemy in the Tracking Service or Model Registry\r\n- [ ] `area\/windows`: Windows support\r\n\r\nLanguages \r\n- [ ] `language\/r`: R APIs and clients\r\n- [ ] `language\/java`: Java APIs and clients\r\n- [ ] `language\/new`: Proposals for new client languages\r\n\r\nIntegrations\r\n- [ ] `integrations\/azure`: Azure and Azure ML integrations\r\n- [ ] `integrations\/sagemaker`: SageMaker integrations\r\n- [ ] `integrations\/databricks`: Databricks integrations\r\n\r\n## Details\r\n\r\nAt present, it is difficult to read plots that are log-scaled due to the chosen formatting of text units (e.g. \\mu for micro 10^-6). This request is to convert log scaling according to standard convention (e.g. 4e-3) as more commonly found in plotting applications such Tensorboard. In addition, I think the style choice of larger text for integer exponent bases is non-standard. Uniform font desired.\r\n\r\nMLFlow log scaling (non-standard):\r\n![image](https:\/\/user-images.githubusercontent.com\/50338490\/151634400-23a73e5d-5db5-4a7a-a4ca-7b4b4a3d8026.png)\r\n\r\nTensorboard log scaling (standard):\r\n![image](https:\/\/user-images.githubusercontent.com\/50338490\/151634510-21d4ee84-4f11-4126-85df-0dfc31fd7570.png)\r\n\r\nThanks!\r\n","pull_request":null,"labels":["enhancement","area\/uiux"],"labels_description":["New feature or request","Front-end, user experience, plotting, JavaScript, JavaScript dev server"],"entities":["logarithm","notation"]},{"html_url":"https:\/\/github.com\/mlflow\/mlflow\/issues\/5330","id":1117297569,"number":5330,"title":"[bug] mlflow + wasabi + prophet not working","state":"open","locked":false,"assignee":null,"assignees":[],"comments":0,"created_at":1643369298000,"updated_at":1644603133000,"closed_at":null,"body":"### Willingness to contribute\r\n\r\n- [ ] Yes. I can contribute a fix for this bug independently.\r\n- [ ] Yes. I would be willing to contribute a fix for this bug with guidance from the MLflow community.\r\n- [x] No. I cannot contribute a bug fix at this time.\r\n\r\n### System information\r\nOS: Fedora 31\r\nMLFlow version: 1.22.0\r\nPython: 3.7.9\r\n\r\nTrain a model and store it in a\r\n\r\n### Describe the problem\r\nI've trained a Prophet model and store it in a Wasabi bucket without a problem using MLFlow's Python API. When I load it, I get an exception. I have no problem when using a personal S3 bucket, but using a Wasabi server gives problems. \r\n\r\n### Logs\r\n```\r\nTraceback (most recent call last):\r\n  File \"run_mlflow_predictions.py\", line 93, in <module>\r\n    df_forecast = load_model(run_id)\r\n  File \"run_mlflow_predictions.py\", line 73, in load_model\r\n    loaded_model = prophet.load_model(model_path)\r\n  File \"xxx\/mlflow\/venv\/lib64\/python3.7\/site-packages\/mlflow\/prophet.py\", line 285, in load_model\r\n    local_model_path = _download_artifact_from_uri(artifact_uri=model_uri, output_path=dst_path)\r\n  File \"xxx\/mlflow\/venv\/lib64\/python3.7\/site-packages\/mlflow\/tracking\/artifact_utils.py\", line 96, in _download_artifact_from_uri\r\n    artifact_path=artifact_path, dst_path=output_path\r\n  File \"xxx\/venv\/lib64\/python3.7\/site-packages\/mlflow\/store\/artifact\/runs_artifact_repo.py\", line 125, in download_artifacts\r\n    return self.repo.download_artifacts(artifact_path, dst_path)\r\n  File \"xxx\/venv\/lib64\/python3.7\/site-packages\/mlflow\/store\/artifact\/artifact_repo.py\", line 179, in download_artifacts\r\n    if self._is_directory(artifact_path):\r\n  File \"xxx\/venv\/lib64\/python3.7\/site-packages\/mlflow\/store\/artifact\/artifact_repo.py\", line 61, in _is_directory\r\n    listing = self.list_artifacts(artifact_path)\r\n  File \"xxx\/venv\/lib64\/python3.7\/site-packages\/mlflow\/store\/artifact\/http_artifact_repo.py\", line 47, in list_artifacts\r\n    head, tail = self.artifact_uri.split(sep, maxsplit=1)\r\nValueError: not enough values to unpack (expected 2, got 1)\r\n```\r\n\r\n\r\n### What component(s), interfaces, languages, and integrations does this bug affect?\r\nComponents \r\n- [x] `area\/artifacts`: Artifact stores and artifact logging\r\n- [ ] `area\/build`: Build and test infrastructure for MLflow\r\n- [ ] `area\/docs`: MLflow documentation pages\r\n- [ ] `area\/examples`: Example code\r\n- [ ] `area\/model-registry`: Model Registry service, APIs, and the fluent client calls for Model Registry\r\n- [x] `area\/models`: MLmodel format, model serialization\/deserialization, flavors\r\n- [ ] `area\/projects`: MLproject format, project running backends\r\n- [ ] `area\/scoring`: MLflow Model server, model deployment tools, Spark UDFs\r\n- [ ] `area\/server-infra`: MLflow Tracking server backend\r\n- [x] `area\/tracking`: Tracking Service, tracking client APIs, autologging\r\n\r\nInterface \r\n- [ ] `area\/uiux`: Front-end, user experience, plotting, JavaScript, JavaScript dev server\r\n- [x] `area\/docker`: Docker use across MLflow's components, such as MLflow Projects and MLflow Models\r\n- [ ] `area\/sqlalchemy`: Use of SQLAlchemy in the Tracking Service or Model Registry\r\n- [ ] `area\/windows`: Windows support","pull_request":null,"labels":["bug","area\/artifacts","area\/models","area\/tracking"],"labels_description":["Something isn't working","Artifact stores and artifact logging","MLmodel format, model serialization\/deserialization, flavors","Tracking service, tracking client APIs, autologging"],"entities":["prophet"]},{"html_url":"https:\/\/github.com\/mlflow\/mlflow\/issues\/5327","id":1116034963,"number":5327,"title":"[fr] allow `stage` as version value in `set_model_version_tag`","state":"open","locked":false,"assignee":null,"assignees":[],"comments":0,"created_at":1643279235000,"updated_at":1643279250000,"closed_at":null,"body":"Thank you for submitting a feature request. **Before proceeding, please review MLflow's [Issue Policy for feature requests](https:\/\/www.github.com\/mlflow\/mlflow\/blob\/master\/ISSUE_POLICY.md#feature-requests) and the [MLflow Contributing Guide](https:\/\/github.com\/mlflow\/mlflow\/blob\/master\/CONTRIBUTING.rst)**.\r\n\r\n**Please fill in this feature request template to ensure a timely and thorough response.**\r\n\r\nCurrently, the method `client.set_model_version_tag` only accepts an integer value for the parameter `version`. However, downstream jobs that refer to a model in the model registry often refer to it by `stage` instead of `version`. It would simplify things if the `set_model_version_tag` method would accept the stage name in addition to the version as well.\r\n\r\nExample:\r\n```python\r\nclient.set_model_version_tag(name=\"my-model\", version=\"staging\", key=\"mykey\", value=\"myvalue\")\r\n```\r\n\r\nCurrently, if someone now wants to set a tag for a `production` model, the person would first have to look up the `model version` for that model before it can set the tag. \r\n\r\n## Willingness to contribute\r\nThe MLflow Community encourages new feature contributions. Would you or another member of your organization be willing to contribute an implementation of this feature (either as an MLflow Plugin or an enhancement to the MLflow code base)?\r\n\r\n- [ ] Yes. I can contribute this feature independently.\r\n- [x] Yes. I would be willing to contribute this feature with guidance from the MLflow community.\r\n- [ ] No. I cannot contribute this feature at this time.\r\n\r\n## Proposal Summary\r\n\r\nAllow `client.set_model_version_tag` to accept `staging,production,archived` as values for the `version` parameter as well, or add an additional parameter called `stage` that can be used instead of `version`.\r\n\r\n## Motivation\r\n- What is the use case for this feature? _It makes MLflow simpler to use_\r\n- Why is this use case valuable to support for MLflow users in general?\r\n- Why is this use case valuable to support for your project(s) or organization?\r\n- Why is it currently difficult to achieve this use case? (please be as specific as possible about why related MLflow features and components are insufficient)\r\n\r\n### What component(s), interfaces, languages, and integrations does this feature affect?\r\nComponents \r\n- [ ] `area\/artifacts`: Artifact stores and artifact logging\r\n- [ ] `area\/build`: Build and test infrastructure for MLflow\r\n- [ ] `area\/docs`: MLflow documentation pages\r\n- [ ] `area\/examples`: Example code\r\n- [x] `area\/model-registry`: Model Registry service, APIs, and the fluent client calls for Model Registry\r\n- [ ] `area\/models`: MLmodel format, model serialization\/deserialization, flavors\r\n- [ ] `area\/projects`: MLproject format, project running backends\r\n- [ ] `area\/scoring`: MLflow Model server, model deployment tools, Spark UDFs\r\n- [ ] `area\/server-infra`: MLflow Tracking server backend\r\n- [ ] `area\/tracking`: Tracking Service, tracking client APIs, autologging\r\n\r\nInterfaces\r\n- [ ] `area\/uiux`: Front-end, user experience, plotting, JavaScript, JavaScript dev server\r\n- [ ] `area\/docker`: Docker use across MLflow's components, such as MLflow Projects and MLflow Models\r\n- [ ] `area\/sqlalchemy`: Use of SQLAlchemy in the Tracking Service or Model Registry\r\n- [ ] `area\/windows`: Windows support\r\n\r\nLanguages \r\n- [ ] `language\/r`: R APIs and clients\r\n- [ ] `language\/java`: Java APIs and clients\r\n- [ ] `language\/new`: Proposals for new client languages\r\n\r\nIntegrations\r\n- [ ] `integrations\/azure`: Azure and Azure ML integrations\r\n- [ ] `integrations\/sagemaker`: SageMaker integrations\r\n- [ ] `integrations\/databricks`: Databricks integrations\r\n\r\n## Details\r\n\r\n(Use this section to include any additional information about the feature. If you have a proposal for how to implement this feature, please include it here. For implementation guidelines, please refer to the [Contributing Guide](https:\/\/github.com\/mlflow\/mlflow\/blob\/master\/CONTRIBUTING.rst#contribution-guidelines).)\r\n","pull_request":null,"labels":["enhancement","area\/model-registry"],"labels_description":["New feature or request","Model registry, model registry APIs, and the fluent client calls for model registry"],"entities":["`stage`","version","value"]},{"html_url":"https:\/\/github.com\/mlflow\/mlflow\/issues\/5301","id":1112400135,"number":5301,"title":"running mlprojects using old commit [bug]","state":"open","locked":false,"assignee":null,"assignees":[],"comments":1,"created_at":1643017429000,"updated_at":1647437385000,"closed_at":null,"body":"Thank you for submitting an issue. Please refer to our [issue policy](https:\/\/www.github.com\/mlflow\/mlflow\/blob\/master\/ISSUE_POLICY.md) for additional information about bug reports. For help with debugging your code, please refer to [Stack Overflow](https:\/\/stackoverflow.com\/questions\/tagged\/mlflow).\r\n\r\n**Please fill in this bug report template to ensure a timely and thorough response.**\r\n\r\n### Willingness to contribute\r\nThe MLflow Community encourages bug fix contributions. Would you or another member of your organization be willing to contribute a fix for this bug to the MLflow code base?\r\n\r\n- [ ] Yes. I can contribute a fix for this bug independently.\r\n- [ ] Yes. I would be willing to contribute a fix for this bug with guidance from the MLflow community.\r\n- [x] No. I cannot contribute a bug fix at this time.\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in MLflow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux\r\n- **MLflow installed from (source or binary)**: binary \r\n- **MLflow version (run ``mlflow --version``)**: 1.23.0\r\n- **Python version**: 3.8\r\n- **npm version, if running the dev UI**:\r\n- **Exact command to reproduce**: `mlflow run <Repo> -v <OldCommitID>`\r\n\r\n### Describe the problem\r\nMLflow projects is used for packaging data science code in a reusable and reproducible way. However, I tried to use MLflow projects to run old commit and it did not work.\r\n\r\n### Code to reproduce issue\r\n`mlflow run <Repo> -v <OldCommitID>`\r\n\r\n### Other info \/ logs\r\n`ERROR mlflow.cli: === Unable to checkout version <OldCommitID> of git repo <Repo> - please ensure that the version exists in the repo. `\r\n\r\n` stderr: 'fatal: reference is not a tree: <OldCommitID>' ===`\r\n\r\n\r\n### What component(s), interfaces, languages, and integrations does this bug affect?\r\nComponents \r\n- [ ] `area\/artifacts`: Artifact stores and artifact logging\r\n- [ ] `area\/build`: Build and test infrastructure for MLflow\r\n- [ ] `area\/docs`: MLflow documentation pages\r\n- [ ] `area\/examples`: Example code\r\n- [ ] `area\/model-registry`: Model Registry service, APIs, and the fluent client calls for Model Registry\r\n- [ ] `area\/models`: MLmodel format, model serialization\/deserialization, flavors\r\n- [x] `area\/projects`: MLproject format, project running backends\r\n- [ ] `area\/scoring`: MLflow Model server, model deployment tools, Spark UDFs\r\n- [ ] `area\/server-infra`: MLflow Tracking server backend\r\n- [ ] `area\/tracking`: Tracking Service, tracking client APIs, autologging\r\n\r\nInterface \r\n- [ ] `area\/uiux`: Front-end, user experience, plotting, JavaScript, JavaScript dev server\r\n- [ ] `area\/docker`: Docker use across MLflow's components, such as MLflow Projects and MLflow Models\r\n- [ ] `area\/sqlalchemy`: Use of SQLAlchemy in the Tracking Service or Model Registry\r\n- [ ] `area\/windows`: Windows support\r\n\r\nLanguage \r\n- [ ] `language\/r`: R APIs and clients\r\n- [ ] `language\/java`: Java APIs and clients\r\n- [ ] `language\/new`: Proposals for new client languages\r\n\r\nIntegrations\r\n- [ ] `integrations\/azure`: Azure and Azure ML integrations\r\n- [ ] `integrations\/sagemaker`: SageMaker integrations\r\n- [ ] `integrations\/databricks`: Databricks integrations\r\n","pull_request":null,"labels":["bug","area\/projects"],"labels_description":["Something isn't working","MLproject format, project running backends"],"entities":["commit"]},{"html_url":"https:\/\/github.com\/mlflow\/mlflow\/issues\/5297","id":1110670818,"number":5297,"title":"[bug] --serve-artifacts uris on separate server","state":"open","locked":false,"assignee":null,"assignees":[],"comments":1,"created_at":1642782778000,"updated_at":1646234670000,"closed_at":null,"body":"Thank you for submitting an issue. Please refer to our [issue policy](https:\/\/www.github.com\/mlflow\/mlflow\/blob\/master\/ISSUE_POLICY.md) for additional information about bug reports. For help with debugging your code, please refer to [Stack Overflow](https:\/\/stackoverflow.com\/questions\/tagged\/mlflow).\r\n\r\n**Please fill in this bug report template to ensure a timely and thorough response.**\r\n\r\n### Willingness to contribute\r\nThe MLflow Community encourages bug fix contributions. Would you or another member of your organization be willing to contribute a fix for this bug to the MLflow code base?\r\n\r\n- [ ] Yes. I can contribute a fix for this bug independently.\r\n- [ ] Yes. I would be willing to contribute a fix for this bug with guidance from the MLflow community.\r\n- [x] No. I cannot contribute a bug fix at this time.\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in MLflow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:  Server running on Ubuntu 20.04, attempt to save model from Windows 10\r\n- **MLflow installed from (source or binary)**: `pip install mlflow`\r\n- **MLflow version (run ``mlflow --version``)**: 1.23\r\n- **Python version**: 3.8.10\r\n- **npm version, if running the dev UI**: N\/A\r\n- **Exact command to reproduce**: \r\n  - On server side: `mlflow server  --serve-artifacts --host 0.0.0.0 -p 5020    --artifacts-destination .\/mlartifacts --gunicorn-opts \"--log-level debug\"\r\n  - On client side, mlflow installed via `git clone` and `pip install -e .\/`. Use the `mlflow_artifacts` example, so command is `python example.py`\r\n\r\n### Describe the problem\r\nI expect to be able to save artifacts from the `mlflow_artifacts` example to the server running on a separate machine. Instead I get errors relating to the URI. \r\n\r\nOnly running the example code from the same machine works. \r\n\r\n### Code to reproduce issue\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\nOn an ubuntu machine, run the server command:\r\n```bash\r\nmlflow server  --serve-artifacts --host 0.0.0.0 -p 5020    --artifacts-destination .\/mlartifacts     --gunicorn-opts \"--log-level debug\"\r\n```\r\n(Note that adding the option `--default-artifact-root http:\/\/<server-ip>:5020\/api\/2.0\/mlflow-artifacts\/artifacts\/experiments` doesn't change a thing)\r\n\r\nOn a windows machine:\r\n```powershell\r\n$env:MLFLOW_TRACKING_URI=\"http:\/\/<ip-of-server>:5020\/\"\r\ncd examples\/mlflow_artifacts\/\r\npython example.py\r\n```\r\n\r\nThe result is the following error:\r\n\r\n```python\r\nConnectionError: HTTPConnectionPool(host='0.0.0.0', port=5020): Max retries exceeded with url: \/api\/2.0\/mlflow-artifacts\/artifacts\/experiments\/0\/ef5c9d97f4504cc494413c4d5cf048f7\/artifacts\/a.txt (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000002464D5AC700>: Failed to establish a new connection: [WinError 10049] The requested address is not valid in its context'))\r\n```\r\n\r\nI think the hint is in the HTTPConnectionPool, which seems to be looking for a localhost (0.0.0.0) rather than the server name. Since this is being passed to the client on a different machine, which isn't running a server, the client can't save the artifact. \r\n\r\n\r\n### Other info \/ logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\n```bash\r\n---------------------------------------------------------------------------\r\nOSError                                   Traceback (most recent call last)\r\nFile ~\\Miniconda3\\envs\\mlflow\\lib\\site-packages\\urllib3\\connection.py:174, in HTTPConnection._new_conn(self)\r\n    173 try:\r\n--> 174     conn = connection.create_connection(\r\n    175         (self._dns_host, self.port), self.timeout, **extra_kw\r\n    176     )\r\n    178 except SocketTimeout:\r\n\r\nFile ~\\Miniconda3\\envs\\mlflow\\lib\\site-packages\\urllib3\\util\\connection.py:96, in create_connection(address, timeout, source_address, socket_options)\r\n     95 if err is not None:\r\n---> 96     raise err\r\n     98 raise socket.error(\"getaddrinfo returns an empty list\")\r\n\r\nFile ~\\Miniconda3\\envs\\mlflow\\lib\\site-packages\\urllib3\\util\\connection.py:86, in create_connection(address, timeout, source_address, socket_options)\r\n     85     sock.bind(source_address)\r\n---> 86 sock.connect(sa)\r\n     87 return sock\r\n\r\nOSError: [WinError 10049] The requested address is not valid in its context\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nNewConnectionError                        Traceback (most recent call last)\r\nFile ~\\Miniconda3\\envs\\mlflow\\lib\\site-packages\\urllib3\\connectionpool.py:699, in HTTPConnectionPool.urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\r\n    698 # Make the request on the httplib connection object.\r\n--> 699 httplib_response = self._make_request(\r\n    700     conn,\r\n    701     method,\r\n    702     url,\r\n    703     timeout=timeout_obj,\r\n    704     body=body,\r\n    705     headers=headers,\r\n    706     chunked=chunked,\r\n    707 )\r\n    709 # If we're going to release the connection in ``finally:``, then\r\n    710 # the response doesn't need to know about the connection. Otherwise\r\n    711 # it will also try to release it and we'll have a double-release\r\n    712 # mess.\r\n\r\nFile ~\\Miniconda3\\envs\\mlflow\\lib\\site-packages\\urllib3\\connectionpool.py:394, in HTTPConnectionPool._make_request(self, conn, method, url, timeout, chunked, **httplib_request_kw)\r\n    393     else:\r\n--> 394         conn.request(method, url, **httplib_request_kw)\r\n    396 # We are swallowing BrokenPipeError (errno.EPIPE) since the server is\r\n    397 # legitimately able to close the connection after sending a valid response.\r\n    398 # With this behaviour, the received response is still readable.\r\n\r\nFile ~\\Miniconda3\\envs\\mlflow\\lib\\site-packages\\urllib3\\connection.py:239, in HTTPConnection.request(self, method, url, body, headers)\r\n    238     headers[\"User-Agent\"] = _get_default_user_agent()\r\n--> 239 super(HTTPConnection, self).request(method, url, body=body, headers=headers)\r\n\r\nFile ~\\Miniconda3\\envs\\mlflow\\lib\\http\\client.py:1279, in HTTPConnection.request(self, method, url, body, headers, encode_chunked)\r\n   1278 \"\"\"Send a complete request to the server.\"\"\"\r\n-> 1279 self._send_request(method, url, body, headers, encode_chunked)\r\n\r\nFile ~\\Miniconda3\\envs\\mlflow\\lib\\http\\client.py:1325, in HTTPConnection._send_request(self, method, url, body, headers, encode_chunked)\r\n   1324     body = _encode(body, 'body')\r\n-> 1325 self.endheaders(body, encode_chunked=encode_chunked)\r\n\r\nFile ~\\Miniconda3\\envs\\mlflow\\lib\\http\\client.py:1274, in HTTPConnection.endheaders(self, message_body, encode_chunked)\r\n   1273     raise CannotSendHeader()\r\n-> 1274 self._send_output(message_body, encode_chunked=encode_chunked)\r\n\r\nFile ~\\Miniconda3\\envs\\mlflow\\lib\\http\\client.py:1034, in HTTPConnection._send_output(self, message_body, encode_chunked)\r\n   1033 del self._buffer[:]\r\n-> 1034 self.send(msg)\r\n   1036 if message_body is not None:\r\n   1037\r\n   1038     # create a consistent interface to message_body\r\n\r\nFile ~\\Miniconda3\\envs\\mlflow\\lib\\http\\client.py:974, in HTTPConnection.send(self, data)\r\n    973 if self.auto_open:\r\n--> 974     self.connect()\r\n    975 else:\r\n\r\nFile ~\\Miniconda3\\envs\\mlflow\\lib\\site-packages\\urllib3\\connection.py:205, in HTTPConnection.connect(self)\r\n    204 def connect(self):\r\n--> 205     conn = self._new_conn()\r\n    206     self._prepare_conn(conn)\r\n\r\nFile ~\\Miniconda3\\envs\\mlflow\\lib\\site-packages\\urllib3\\connection.py:186, in HTTPConnection._new_conn(self)\r\n    185 except SocketError as e:\r\n--> 186     raise NewConnectionError(\r\n    187         self, \"Failed to establish a new connection: %s\" % e\r\n    188     )\r\n    190 return conn\r\n\r\nNewConnectionError: <urllib3.connection.HTTPConnection object at 0x000002464D5AC700>: Failed to establish a new connection: [WinError 10049] The requested address is not valid in its context\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nMaxRetryError                             Traceback (most recent call last)\r\nFile ~\\Miniconda3\\envs\\mlflow\\lib\\site-packages\\requests\\adapters.py:439, in HTTPAdapter.send(self, request, stream, timeout, verify, cert, proxies)\r\n    438 if not chunked:\r\n--> 439     resp = conn.urlopen(\r\n    440         method=request.method,\r\n    441         url=url,\r\n    442         body=request.body,\r\n    443         headers=request.headers,\r\n    444         redirect=False,\r\n    445         assert_same_host=False,\r\n    446         preload_content=False,\r\n    447         decode_content=False,\r\n    448         retries=self.max_retries,\r\n    449         timeout=timeout\r\n    450     )\r\n    452 # Send the request.\r\n    453 else:\r\n\r\nFile ~\\Miniconda3\\envs\\mlflow\\lib\\site-packages\\urllib3\\connectionpool.py:755, in HTTPConnectionPool.urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\r\n    753     e = ProtocolError(\"Connection aborted.\", e)\r\n--> 755 retries = retries.increment(\r\n    756     method, url, error=e, _pool=self, _stacktrace=sys.exc_info()[2]\r\n    757 )\r\n    758 retries.sleep()\r\n\r\nFile ~\\Miniconda3\\envs\\mlflow\\lib\\site-packages\\urllib3\\util\\retry.py:574, in Retry.increment(self, method, url, response, error, _pool, _stacktrace)\r\n    573 if new_retry.is_exhausted():\r\n--> 574     raise MaxRetryError(_pool, url, error or ResponseError(cause))\r\n    576 log.debug(\"Incremented Retry for (url='%s'): %r\", url, new_retry)\r\n\r\nMaxRetryError: HTTPConnectionPool(host='0.0.0.0', port=5020): Max retries exceeded with url: \/api\/2.0\/mlflow-artifacts\/artifacts\/experiments\/0\/ef5c9d97f4504cc494413c4d5cf048f7\/artifacts\/a.txt (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000002464D5AC700>: Failed to establish a new connection: [WinError 10049] The requested address is not valid in its context'))\r\n```\r\n\r\n### What component(s), interfaces, languages, and integrations does this bug affect?\r\nComponents \r\n- [x] `area\/artifacts`: Artifact stores and artifact logging\r\n- [ ] `area\/build`: Build and test infrastructure for MLflow\r\n- [ ] `area\/docs`: MLflow documentation pages\r\n- [x] `area\/examples`: Example code\r\n- [ ] `area\/model-registry`: Model Registry service, APIs, and the fluent client calls for Model Registry\r\n- [ ] `area\/models`: MLmodel format, model serialization\/deserialization, flavors\r\n- [ ] `area\/projects`: MLproject format, project running backends\r\n- [ ] `area\/scoring`: MLflow Model server, model deployment tools, Spark UDFs\r\n- [ ] `area\/server-infra`: MLflow Tracking server backend\r\n- [x] `area\/tracking`: Tracking Service, tracking client APIs, autologging\r\n\r\nInterface \r\n- [ ] `area\/uiux`: Front-end, user experience, plotting, JavaScript, JavaScript dev server\r\n- [ ] `area\/docker`: Docker use across MLflow's components, such as MLflow Projects and MLflow Models\r\n- [ ] `area\/sqlalchemy`: Use of SQLAlchemy in the Tracking Service or Model Registry\r\n- [ ] `area\/windows`: Windows support\r\n\r\nLanguage \r\n- [ ] `language\/r`: R APIs and clients\r\n- [ ] `language\/java`: Java APIs and clients\r\n- [ ] `language\/new`: Proposals for new client languages\r\n\r\nIntegrations\r\n- [ ] `integrations\/azure`: Azure and Azure ML integrations\r\n- [ ] `integrations\/sagemaker`: SageMaker integrations\r\n- [ ] `integrations\/databricks`: Databricks integrations\r\n","pull_request":null,"labels":["bug","area\/artifacts","area\/examples","area\/tracking"],"labels_description":["Something isn't working","Artifact stores and artifact logging","Example code","Tracking service, tracking client APIs, autologging"],"entities":["server"]},{"html_url":"https:\/\/github.com\/mlflow\/mlflow\/pull\/5296","id":1110163405,"number":5296,"title":"add signed url return value when getting gcs download uri","state":"open","locked":false,"assignee":null,"assignees":[],"comments":3,"created_at":1642751764000,"updated_at":1644915646000,"closed_at":null,"body":"## What changes are proposed in this pull request?\r\n\r\nthe way that model serve, prepare-env, etc get the download uri if we store the models in GCS. instead of returning the normal GCS path, we are going to return the signed url so we dont need to use SA to get the models for model serve.\r\n\r\n## How is this patch tested?\r\n\r\n- deploy mlflow service\r\n- test the endpoint using curl\r\n```\r\ncurl 'https:\/\/<mlflow-url>\/api\/2.0\/preview\/mlflow\/model-versions\/get-download-uri?name=<model_name>&version=<model_version>&use_signed_url=True'\r\n```\r\n- or deploy model service with additional parameter`--use-signed-url`\r\n```\r\nmlflow models serve --model-uri models:\/<model_name>\/<model_stage> -h 0.0.0.0 -p 3021 --use-signed-url\r\n```\r\n\r\n## Does this PR change the documentation?\r\n\r\n- [X] No. You can skip the rest of this section.\r\n- [ ] Yes. Make sure the changed pages \/ sections render correctly by following the steps below.\r\n\r\n1. Check the status of the `ci\/circleci: build_doc` check. If it's successful, proceed to the\r\n   next step, otherwise fix it.\r\n2. Click `Details` on the right to open the job page of CircleCI.\r\n3. Click the `Artifacts` tab.\r\n4. Click `docs\/build\/html\/index.html`.\r\n5. Find the changed pages \/ sections and make sure they render correctly.\r\n\r\n## Release Notes\r\n\r\n### Is this a user-facing change?\r\n\r\n- [X] No. You can skip the rest of this section.\r\n- [ ] Yes. Give a description of this change to be included in the release notes for MLflow users.\r\n\r\n(Details in 1-2 sentences. You can just refer to another PR with a description if this PR is part of a larger change.)\r\n\r\n### What component(s), interfaces, languages, and integrations does this PR affect?\r\nComponents \r\n- [X] `area\/artifacts`: Artifact stores and artifact logging\r\n- [ ] `area\/build`: Build and test infrastructure for MLflow\r\n- [ ] `area\/docs`: MLflow documentation pages\r\n- [ ] `area\/examples`: Example code\r\n- [X] `area\/model-registry`: Model Registry service, APIs, and the fluent client calls for Model Registry\r\n- [X] `area\/models`: MLmodel format, model serialization\/deserialization, flavors\r\n- [ ] `area\/projects`: MLproject format, project running backends\r\n- [ ] `area\/scoring`: MLflow Model server, model deployment tools, Spark UDFs\r\n- [ ] `area\/server-infra`: MLflow Tracking server backend\r\n- [X] `area\/tracking`: Tracking Service, tracking client APIs, autologging\r\n\r\nInterface \r\n- [ ] `area\/uiux`: Front-end, user experience, plotting, JavaScript, JavaScript dev server\r\n- [ ] `area\/docker`: Docker use across MLflow's components, such as MLflow Projects and MLflow Models\r\n- [X] `area\/sqlalchemy`: Use of SQLAlchemy in the Tracking Service or Model Registry\r\n- [ ] `area\/windows`: Windows support\r\n\r\nLanguage \r\n- [ ] `language\/r`: R APIs and clients\r\n- [ ] `language\/java`: Java APIs and clients\r\n- [ ] `language\/new`: Proposals for new client languages\r\n\r\nIntegrations\r\n- [ ] `integrations\/azure`: Azure and Azure ML integrations\r\n- [ ] `integrations\/sagemaker`: SageMaker integrations\r\n- [ ] `integrations\/databricks`: Databricks integrations\r\n\r\n<!--\r\nInsert an empty named anchor here to allow jumping to this section with a fragment URL\r\n(e.g. https:\/\/github.com\/mlflow\/mlflow\/pull\/123#user-content-release-note-category).\r\nNote that GitHub prefixes anchor names in markdown with \"user-content-\".\r\n-->\r\n<a name=\"release-note-category\"><\/a>\r\n### How should the PR be classified in the release notes? Choose one:\r\n\r\n- [ ] `rn\/breaking-change` - The PR will be mentioned in the \"Breaking Changes\" section\r\n- [ ] `rn\/none` - No description will be included. The PR will be mentioned only by the PR number in the \"Small Bugfixes and Documentation Updates\" section\r\n- [X] `rn\/feature` - A new user-facing feature worth mentioning in the release notes\r\n- [ ] `rn\/bug-fix` - A user-facing bug fix worth mentioning in the release notes\r\n- [ ] `rn\/documentation` - A user-facing documentation change worth mentioning in the release notes\r\n","pull_request":{"url":"https:\/\/api.github.com\/repos\/mlflow\/mlflow\/pulls\/5296","html_url":"https:\/\/github.com\/mlflow\/mlflow\/pull\/5296","diff_url":"https:\/\/github.com\/mlflow\/mlflow\/pull\/5296.diff","patch_url":"https:\/\/github.com\/mlflow\/mlflow\/pull\/5296.patch","merged_at":null},"labels":["rn\/feature","area\/artifacts","area\/model-registry","area\/models","area\/tracking","area\/sqlalchemy"],"labels_description":["Mention under Features in Changelogs.","Artifact stores and artifact logging","Model registry, model registry APIs, and the fluent client calls for model registry","MLmodel format, model serialization\/deserialization, flavors","Tracking service, tracking client APIs, autologging","Use of SQL alchemy in tracking service or model registry"],"entities":["url","return","value","download","uri"]},{"html_url":"https:\/\/github.com\/mlflow\/mlflow\/issues\/5295","id":1109835190,"number":5295,"title":"[bug] filtering registered models by tag results in invalid_parameter_value error","state":"open","locked":false,"assignee":null,"assignees":[],"comments":0,"created_at":1642718478000,"updated_at":1642718660000,"closed_at":null,"body":"Thank you for submitting an issue. Please refer to our [issue policy](https:\/\/www.github.com\/mlflow\/mlflow\/blob\/master\/ISSUE_POLICY.md) for additional information about bug reports. For help with debugging your code, please refer to [Stack Overflow](https:\/\/stackoverflow.com\/questions\/tagged\/mlflow).\r\n\r\n**Please fill in this bug report template to ensure a timely and thorough response.**\r\n\r\n### Willingness to contribute\r\nThe MLflow Community encourages bug fix contributions. Would you or another member of your organization be willing to contribute a fix for this bug to the MLflow code base?\r\n\r\n- [ ] Yes. I can contribute a fix for this bug independently.\r\n- [ ] Yes. I would be willing to contribute a fix for this bug with guidance from the MLflow community.\r\n- [x] No. I cannot contribute a bug fix at this time.\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in MLflow)**: no\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux\r\n- **MLflow installed from (source or binary)**: source\r\n- **MLflow version (run ``mlflow --version``)**: 1.22.0\r\n- **Python version**: 3.9.1\r\n- **npm version, if running the dev UI**: Don't know\r\n- **Exact command to reproduce**: see below\r\n\r\n### Describe the problem\r\n\r\nTrying to search by tag by following the example in the tags field results in the error message\r\n\r\n`INVALID_PARAMETER_VALUE: Search filter 'name ilike '%%' AND tags.key='value'' contains multiple expressions. Expected search filter with single comparison operator. e.g. name='myModelName'`\r\n\r\n![Screen Shot 2022-01-20 at 5 39 21 PM](https:\/\/user-images.githubusercontent.com\/800945\/150433434-1f258779-6b1b-4574-b530-30537cd11d77.png)\r\n\r\n### Code to reproduce issue\r\nN\/A\r\n\r\n### Other info \/ logs\r\n`INVALID_PARAMETER_VALUE: Search filter 'name ilike '%%' AND tags.key='value'' contains multiple expressions. Expected search filter with single comparison operator. e.g. name='myModelName'` is displayed in the UI.\r\n\r\nWe're using MySQL.\r\n\r\n### What component(s), interfaces, languages, and integrations does this bug affect?\r\nComponents \r\n- [ ] `area\/artifacts`: Artifact stores and artifact logging\r\n- [ ] `area\/build`: Build and test infrastructure for MLflow\r\n- [ ] `area\/docs`: MLflow documentation pages\r\n- [ ] `area\/examples`: Example code\r\n- [x] `area\/model-registry`: Model Registry service, APIs, and the fluent client calls for Model Registry\r\n- [ ] `area\/models`: MLmodel format, model serialization\/deserialization, flavors\r\n- [ ] `area\/projects`: MLproject format, project running backends\r\n- [ ] `area\/scoring`: MLflow Model server, model deployment tools, Spark UDFs\r\n- [ ] `area\/server-infra`: MLflow Tracking server backend\r\n- [ ] `area\/tracking`: Tracking Service, tracking client APIs, autologging\r\n\r\nInterface \r\n- [x] `area\/uiux`: Front-end, user experience, plotting, JavaScript, JavaScript dev server\r\n- [ ] `area\/docker`: Docker use across MLflow's components, such as MLflow Projects and MLflow Models\r\n- [ ] `area\/sqlalchemy`: Use of SQLAlchemy in the Tracking Service or Model Registry\r\n- [ ] `area\/windows`: Windows support\r\n\r\nLanguage \r\n- [ ] `language\/r`: R APIs and clients\r\n- [ ] `language\/java`: Java APIs and clients\r\n- [ ] `language\/new`: Proposals for new client languages\r\n\r\nIntegrations\r\n- [ ] `integrations\/azure`: Azure and Azure ML integrations\r\n- [ ] `integrations\/sagemaker`: SageMaker integrations\r\n- [ ] `integrations\/databricks`: Databricks integrations\r\n","pull_request":null,"labels":["bug","area\/uiux","area\/model-registry"],"labels_description":["Something isn't working","Front-end, user experience, plotting, JavaScript, JavaScript dev server","Model registry, model registry APIs, and the fluent client calls for model registry"],"entities":["tag","invalid_parameter_value","error"]},{"html_url":"https:\/\/github.com\/mlflow\/mlflow\/pull\/5290","id":1106810734,"number":5290,"title":"remove deprecated `mlflow.pyfunc.load_pyfunc`","state":"open","locked":false,"assignee":null,"assignees":[],"comments":0,"created_at":1642506657000,"updated_at":1642744068000,"closed_at":null,"body":"Signed-off-by: harupy <hkawamura0130@gmail.com>\r\n\r\n## What changes are proposed in this pull request?\r\n\r\nRemove deprecated `mlflow.pyfunc.load_pyfunc`.\r\n\r\n## How is this patch tested?\r\n\r\nExisting tests\r\n\r\n## Does this PR change the documentation?\r\n\r\n- [x] No. You can skip the rest of this section.\r\n- [ ] Yes. Make sure the changed pages \/ sections render correctly by following the steps below.\r\n\r\n1. Check the status of the `ci\/circleci: build_doc` check. If it's successful, proceed to the\r\n   next step, otherwise fix it.\r\n2. Click `Details` on the right to open the job page of CircleCI.\r\n3. Click the `Artifacts` tab.\r\n4. Click `docs\/build\/html\/index.html`.\r\n5. Find the changed pages \/ sections and make sure they render correctly.\r\n\r\n## Release Notes\r\n\r\n### Is this a user-facing change?\r\n\r\n- [ ] No. You can skip the rest of this section.\r\n- [x] Yes. Give a description of this change to be included in the release notes for MLflow users.\r\n\r\n`mlflow.pyfunc.load_pyfunc` has been removed.\r\n\r\n\r\n### What component(s), interfaces, languages, and integrations does this PR affect?\r\nComponents \r\n- [ ] `area\/artifacts`: Artifact stores and artifact logging\r\n- [ ] `area\/build`: Build and test infrastructure for MLflow\r\n- [ ] `area\/docs`: MLflow documentation pages\r\n- [ ] `area\/examples`: Example code\r\n- [ ] `area\/model-registry`: Model Registry service, APIs, and the fluent client calls for Model Registry\r\n- [ ] `area\/models`: MLmodel format, model serialization\/deserialization, flavors\r\n- [ ] `area\/projects`: MLproject format, project running backends\r\n- [ ] `area\/scoring`: MLflow Model server, model deployment tools, Spark UDFs\r\n- [ ] `area\/server-infra`: MLflow Tracking server backend\r\n- [ ] `area\/tracking`: Tracking Service, tracking client APIs, autologging\r\n\r\nInterface \r\n- [ ] `area\/uiux`: Front-end, user experience, plotting, JavaScript, JavaScript dev server\r\n- [ ] `area\/docker`: Docker use across MLflow's components, such as MLflow Projects and MLflow Models\r\n- [ ] `area\/sqlalchemy`: Use of SQLAlchemy in the Tracking Service or Model Registry\r\n- [ ] `area\/windows`: Windows support\r\n\r\nLanguage \r\n- [ ] `language\/r`: R APIs and clients\r\n- [ ] `language\/java`: Java APIs and clients\r\n- [ ] `language\/new`: Proposals for new client languages\r\n\r\nIntegrations\r\n- [ ] `integrations\/azure`: Azure and Azure ML integrations\r\n- [ ] `integrations\/sagemaker`: SageMaker integrations\r\n- [ ] `integrations\/databricks`: Databricks integrations\r\n\r\n<!--\r\nInsert an empty named anchor here to allow jumping to this section with a fragment URL\r\n(e.g. https:\/\/github.com\/mlflow\/mlflow\/pull\/123#user-content-release-note-category).\r\nNote that GitHub prefixes anchor names in markdown with \"user-content-\".\r\n-->\r\n<a name=\"release-note-category\"><\/a>\r\n### How should the PR be classified in the release notes? Choose one:\r\n\r\n- [x] `rn\/breaking-change` - The PR will be mentioned in the \"Breaking Changes\" section\r\n- [ ] `rn\/none` - No description will be included. The PR will be mentioned only by the PR number in the \"Small Bugfixes and Documentation Updates\" section\r\n- [ ] `rn\/feature` - A new user-facing feature worth mentioning in the release notes\r\n- [ ] `rn\/bug-fix` - A user-facing bug fix worth mentioning in the release notes\r\n- [ ] `rn\/documentation` - A user-facing documentation change worth mentioning in the release notes\r\n","pull_request":{"url":"https:\/\/api.github.com\/repos\/mlflow\/mlflow\/pulls\/5290","html_url":"https:\/\/github.com\/mlflow\/mlflow\/pull\/5290","diff_url":"https:\/\/github.com\/mlflow\/mlflow\/pull\/5290.diff","patch_url":"https:\/\/github.com\/mlflow\/mlflow\/pull\/5290.patch","merged_at":null},"labels":["rn\/breaking-change"],"labels_description":["Mention under Breaking Changes in Changelogs."],"entities":[]},{"html_url":"https:\/\/github.com\/mlflow\/mlflow\/issues\/5274","id":1104817257,"number":5274,"title":"[bug] error on create_registered_model from mlflowclient","state":"open","locked":false,"assignee":null,"assignees":[],"comments":1,"created_at":1642273060000,"updated_at":1642394946000,"closed_at":null,"body":"Thank you for submitting an issue. Please refer to our [issue policy](https:\/\/www.github.com\/mlflow\/mlflow\/blob\/master\/ISSUE_POLICY.md) for additional information about bug reports. For help with debugging your code, please refer to [Stack Overflow](https:\/\/stackoverflow.com\/questions\/tagged\/mlflow).\r\n\r\n**Please fill in this bug report template to ensure a timely and thorough response.**\r\n\r\n### Willingness to contribute\r\nThe MLflow Community encourages bug fix contributions. Would you or another member of your organization be willing to contribute a fix for this bug to the MLflow code base?\r\n\r\n- [ ] Yes. I can contribute a fix for this bug independently.\r\n- [ ] Yes. I would be willing to contribute a fix for this bug with guidance from the MLflow community.\r\n- [x] No. I cannot contribute a bug fix at this time.\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in MLflow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 20.04.3\r\n- **MLflow installed from (source or binary)**: binary\r\n- **MLflow version (run ``mlflow --version``)**: 1.22.0\r\n- **Python version**: 3.8\r\n- **npm version, if running the dev UI**:\r\n- **Exact command to reproduce**: `client.create_registered_model('Test')\r\n`\r\n### Describe the problem\r\nDescribe the problem clearly here. Include descriptions of the expected behavior and the actual behavior.\r\nMlflow client throws error every time I try to create_registered_model if tracking_uri and registry_uri are passed as parameters to MlflowClient instance. However if tracking_uri and registry_uri are set as env variables there is no error.\r\n\r\n### Code to reproduce issue\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n```\r\nimport mlflow\r\n\r\nMLFLOW_S3_ENDPOINT_URL = 'example'\r\nMLFLOW_TRACKING_URI = 'example'\r\n\r\nclient = mlflow.tracking.MlflowClient(tracking_uri=MLFLOW_TRACKING_URI, registry_uri=MLFLOW_S3_ENDPOINT_URL)\r\n\r\nclient.create_registered_model('test')\r\n```\r\n\r\n### Other info \/ logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n```\r\n---------------------------------------------------------------------------\r\nMlflowException                           Traceback (most recent call last)\r\n\/tmp\/ipykernel_24697\/2664497627.py in <module>\r\n----> 1 client.create_registered_model('Test')\r\n\r\n~\/.local\/share\/virtualenvs\/mt-demand-forecast-Zq_G97Q2\/lib\/python3.8\/site-packages\/mlflow\/tracking\/client.py in create_registered_model(self, name, tags, description)\r\n   1656             description: This sentiment analysis model classifies the tone-happy, sad, angry.\r\n   1657         \"\"\"\r\n-> 1658         return self._get_registry_client().create_registered_model(name, tags, description)\r\n   1659 \r\n   1660     def rename_registered_model(self, name: str, new_name: str) -> RegisteredModel:\r\n\r\n~\/.local\/share\/virtualenvs\/mt-demand-forecast-Zq_G97Q2\/lib\/python3.8\/site-packages\/mlflow\/tracking\/_model_registry\/client.py in create_registered_model(self, name, tags, description)\r\n     57         tags = tags if tags else {}\r\n     58         tags = [RegisteredModelTag(key, str(value)) for key, value in tags.items()]\r\n---> 59         return self.store.create_registered_model(name, tags, description)\r\n     60 \r\n     61     def update_registered_model(self, name, description):\r\n\r\n~\/.local\/share\/virtualenvs\/mt-demand-forecast-Zq_G97Q2\/lib\/python3.8\/site-packages\/mlflow\/store\/model_registry\/rest_store.py in create_registered_model(self, name, tags, description)\r\n     81             CreateRegisteredModel(name=name, tags=proto_tags, description=description)\r\n     82         )\r\n---> 83         response_proto = self._call_endpoint(CreateRegisteredModel, req_body)\r\n     84         return RegisteredModel.from_proto(response_proto.registered_model)\r\n     85 \r\n\r\n~\/.local\/share\/virtualenvs\/mt-demand-forecast-Zq_G97Q2\/lib\/python3.8\/site-packages\/mlflow\/store\/model_registry\/rest_store.py in _call_endpoint(self, api, json_body, call_all_endpoints)\r\n     62         else:\r\n     63             endpoint, method = _METHOD_TO_INFO[api]\r\n---> 64             return call_endpoint(self.get_host_creds(), endpoint, method, json_body, response_proto)\r\n     65 \r\n     66     # CRUD API for RegisteredModel objects\r\n\r\n~\/.local\/share\/virtualenvs\/mt-demand-forecast-Zq_G97Q2\/lib\/python3.8\/site-packages\/mlflow\/utils\/rest_utils.py in call_endpoint(host_creds, endpoint, method, json_body, response_proto)\r\n    227             host_creds=host_creds, endpoint=endpoint, method=method, json=json_body\r\n    228         )\r\n--> 229     response = verify_rest_response(response, endpoint)\r\n    230     js_dict = json.loads(response.text)\r\n    231     parse_dict(js_dict=js_dict, message=response_proto)\r\n\r\n~\/.local\/share\/virtualenvs\/mt-demand-forecast-Zq_G97Q2\/lib\/python3.8\/site-packages\/mlflow\/utils\/rest_utils.py in verify_rest_response(response, endpoint)\r\n    173                 response.status_code,\r\n    174             )\r\n--> 175             raise MlflowException(\"%s. Response body: '%s'\" % (base_msg, response.text))\r\n    176 \r\n    177     # Skip validation for endpoints (e.g. DBFS file-download API) which may return a non-JSON\r\n\r\nMlflowException: API request to endpoint \/api\/2.0\/preview\/mlflow\/registered-models\/create failed with error code 405 != 200. Response body: '<?xml version=\"1.0\" encoding=\"UTF-8\"?>\r\n<Error><Code>MethodNotAllowed<\/Code><Message>The specified method is not allowed against this resource.<\/Message><Resource>\/api\/2.0\/preview\/mlflow\/registered-models\/create<\/Resource><RequestId>109ef7057479979a<\/RequestId><Method>POST<\/Method><\/Error>'\r\n```\r\n\r\n\r\n### What component(s), interfaces, languages, and integrations does this bug affect?\r\nComponents \r\n- [ ] `area\/artifacts`: Artifact stores and artifact logging\r\n- [ ] `area\/build`: Build and test infrastructure for MLflow\r\n- [ ] `area\/docs`: MLflow documentation pages\r\n- [ ] `area\/examples`: Example code\r\n- [x] `area\/model-registry`: Model Registry service, APIs, and the fluent client calls for Model Registry\r\n- [ ] `area\/models`: MLmodel format, model serialization\/deserialization, flavors\r\n- [ ] `area\/projects`: MLproject format, project running backends\r\n- [ ] `area\/scoring`: MLflow Model server, model deployment tools, Spark UDFs\r\n- [ ] `area\/server-infra`: MLflow Tracking server backend\r\n- [ ] `area\/tracking`: Tracking Service, tracking client APIs, autologging\r\n","pull_request":null,"labels":["bug","area\/model-registry"],"labels_description":["Something isn't working","Model registry, model registry APIs, and the fluent client calls for model registry"],"entities":["error","create_registered_model","mlflowclient"]},{"html_url":"https:\/\/github.com\/mlflow\/mlflow\/issues\/5269","id":1103486711,"number":5269,"title":"[fr] log an array of metrics in one call.","state":"open","locked":false,"assignee":null,"assignees":[],"comments":1,"created_at":1642158882000,"updated_at":1642641280000,"closed_at":null,"body":"## Willingness to contribute\r\nThe MLflow Community encourages new feature contributions. Would you or another member of your organization be willing to contribute an implementation of this feature (either as an MLflow Plugin or an enhancement to the MLflow code base)?\r\n\r\n- [X] Yes. I can contribute this feature independently.\r\n- [ ] Yes. I would be willing to contribute this feature with guidance from the MLflow community.\r\n- [ ] No. I cannot contribute this feature at this time.\r\n\r\n## Proposal Summary\r\n\r\nIn the mlflow package, implemnt a `log_array_metric(name, array)` function. It will be equivalent to:\r\n```\r\nfor i, val in enumerate(array):\r\n     mlflow.log_metric(name, val, step=i) \r\n```\r\nThe implementation should rely on the log_batch function of MlflowClient object in order to reduce the number of calls to the mlflow server\r\n\r\n## Motivation\r\n\r\nIt is a common use case to log a loss after a training. Currently, it is necessary to write the loop. This FR would remove some boiler plate code.\r\nThe loop generates N remote calls to the mlflow server (N = size of the array). For my personal use-case, logging 1000 points takes 90 s. If I use log_batch method from the mflowclient, it takes 4s.\r\n\r\n\r\n### What component(s), interfaces, languages, and integrations does this feature affect?\r\nComponents \r\n- [ ] `area\/artifacts`: Artifact stores and artifact logging\r\n- [ ] `area\/build`: Build and test infrastructure for MLflow\r\n- [ ] `area\/docs`: MLflow documentation pages\r\n- [ ] `area\/examples`: Example code\r\n- [ ] `area\/model-registry`: Model Registry service, APIs, and the fluent client calls for Model Registry\r\n- [ ] `area\/models`: MLmodel format, model serialization\/deserialization, flavors\r\n- [ ] `area\/projects`: MLproject format, project running backends\r\n- [ ] `area\/scoring`: MLflow Model server, model deployment tools, Spark UDFs\r\n- [ ] `area\/server-infra`: MLflow Tracking server backend\r\n- [X] `area\/tracking`: Tracking Service, tracking client APIs, autologging\r\n\r\nInterfaces\r\n- [ ] `area\/uiux`: Front-end, user experience, plotting, JavaScript, JavaScript dev server\r\n- [ ] `area\/docker`: Docker use across MLflow's components, such as MLflow Projects and MLflow Models\r\n- [ ] `area\/sqlalchemy`: Use of SQLAlchemy in the Tracking Service or Model Registry\r\n- [ ] `area\/windows`: Windows support\r\n\r\nLanguages \r\n- [ ] `language\/r`: R APIs and clients\r\n- [ ] `language\/java`: Java APIs and clients\r\n- [ ] `language\/new`: Proposals for new client languages\r\n\r\nIntegrations\r\n- [ ] `integrations\/azure`: Azure and Azure ML integrations\r\n- [ ] `integrations\/sagemaker`: SageMaker integrations\r\n- [ ] `integrations\/databricks`: Databricks integrations\r\n\r\n## Details\r\n\r\n(Use this section to include any additional information about the feature. If you have a proposal for how to implement this feature, please include it here. For implementation guidelines, please refer to the [Contributing Guide](https:\/\/github.com\/mlflow\/mlflow\/blob\/master\/CONTRIBUTING.rst#contribution-guidelines).)\r\n","pull_request":null,"labels":["enhancement","area\/tracking"],"labels_description":["New feature or request","Tracking service, tracking client APIs, autologging"],"entities":["array","call."]},{"html_url":"https:\/\/github.com\/mlflow\/mlflow\/issues\/5266","id":1102177107,"number":5266,"title":"incorrect error message when user is not using databricks cli","state":"open","locked":false,"assignee":null,"assignees":[],"comments":0,"created_at":1642100176000,"updated_at":1643355768000,"closed_at":null,"body":"https:\/\/github.com\/mlflow\/mlflow\/blob\/dab955c02f762852a0e4ec8cf01c7baaffdf598d\/mlflow\/utils\/databricks_utils.py#L322\r\n\r\n**Issue:**\r\n\r\nIf user is setting secret scopes in Databricks for their remote registry (rather than the Databricks CLI), and configures them incorrectly, the error message incorrectly says it is the result of a malformed Databricks CLI profile.\r\n\r\n**Steps to reproduce:**\r\n\r\n1) Set registry_uri mlflow.set_registry_uri(registry_uri) to \"databricks:\/\/non-existent-scope:prefix\" (i.e. to a scope that does not exist)\r\n2) Attempt to register model.\r\n\r\n**Expected results:**\r\n\r\nAn error message indicating the scope doesn't exist, scope secrets are not correctly configured, could not reach workspace-id, etc.\r\n\r\n**Actual results:**\r\n\r\nGot malformed Databricks CLI profile 'MLFlowRegistry'\r\n\r\n","pull_request":null,"labels":["bug","area\/model-registry"],"labels_description":["Something isn't working","Model registry, model registry APIs, and the fluent client calls for model registry"],"entities":["error","message","user","cli"]},{"html_url":"https:\/\/github.com\/mlflow\/mlflow\/issues\/5265","id":1101756357,"number":5265,"title":"[bug]mlflow fails to create conda environments which is caused by pip dependency failure","state":"open","locked":false,"assignee":null,"assignees":[],"comments":5,"created_at":1642081488000,"updated_at":1642449504000,"closed_at":null,"body":"### Willingness to contribute\r\nThe MLflow Community encourages bug fix contributions. Would you or another member of your organization be willing to contribute a fix for this bug to the MLflow code base?\r\n\r\n- [ ] Yes. I can contribute a fix for this bug independently.\r\n- [x] Yes. I would be willing to contribute a fix for this bug with guidance from the MLflow community.\r\n- [ ] No. I cannot contribute a bug fix at this time.\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in MLflow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 18.04)**: Linux(Ubuntu)\r\n- **MLflow installed from (source or binary)**:pip\r\n- **MLflow version (run ``mlflow --version``)**:1.22.0\r\n- **Python version**:3.8 and 3.9\r\n- **Exact command to reproduce**: ~\/Documents\/git_cloned\/Github\/Machine-Learning-Engineering-with-MLflow\/Chapter01\/stockpred > mlflow models serve -m runs:\/c3fe1649dc284a9d82fbdb037666fc03\/model_random_forest\r\n\r\n### Describe the problem\r\nAnytime I try to serve the model locally with the command above which creates a new conda environment and serves the model within that particular environment the environment build fails because the pip a\/some dependencies fails to install. and I can't tell exactly why this dependencies fail \r\n\r\n### Code to reproduce issue\r\n`conda.yaml`\r\n\r\n```\r\nchannels:\r\n- conda-forge\r\ndependencies:\r\n- python=3.9.7\r\n- pip\r\n- pip:\r\n  - mlflow==1.22.0\r\n  - cloudpickle==2.0.0\r\n  - scikit-learn==1.0.2\r\nname: mlflow-env\r\n```\r\n`requirements.txt`\r\n```\r\nmlflow==1.22.0\r\ncloudpickle==2.0.0\r\nscikit-learn==1.0.2\r\n```\r\n### Other info \/ logs\r\n ```\r\n2022\/01\/13 11:52:01 INFO mlflow.models.cli: Selected backend for flavor 'python_function'\r\n2022\/01\/13 11:52:03 INFO mlflow.utils.conda: === Creating conda environment mlflow-04d87ea036b44c4189dc1f3a9f0d282b85dfcf87 ===\r\nCollecting package metadata (repodata.json): done\r\nSolving environment: done\r\nPreparing transaction: done\r\nVerifying transaction: done\r\nExecuting transaction: done\r\nInstalling pip dependencies: failed\r\n\r\n# >>>>>>>>>>>>>>>>>>>>>> ERROR REPORT <<<<<<<<<<<<<<<<<<<<<<\r\n\r\n    Traceback (most recent call last):\r\n      File \"\/home\/nwoke\/anaconda3\/lib\/python3.9\/site-packages\/conda\/exceptions.py\", line 1080, in __call__\r\n        return func(*args, **kwargs)\r\n      File \"\/home\/nwoke\/anaconda3\/lib\/python3.9\/site-packages\/conda_env\/cli\/main.py\", line 80, in do_call\r\n        exit_code = getattr(module, func_name)(args, parser)\r\n      File \"\/home\/nwoke\/anaconda3\/lib\/python3.9\/site-packages\/conda_env\/cli\/main_create.py\", line 141, in execute\r\n        result[installer_type] = installer.install(prefix, pkg_specs, args, env)\r\n      File \"\/home\/nwoke\/anaconda3\/lib\/python3.9\/site-packages\/conda_env\/installers\/pip.py\", line 70, in install\r\n        return _pip_install_via_requirements(*args, **kwargs)\r\n      File \"\/home\/nwoke\/anaconda3\/lib\/python3.9\/site-packages\/conda_env\/installers\/pip.py\", line 44, in _pip_install_via_requirements\r\n        requirements = Utf8NamedTemporaryFile(mode='w',\r\n      File \"\/home\/nwoke\/anaconda3\/lib\/python3.9\/site-packages\/conda\/auxlib\/compat.py\", line 88, in Utf8NamedTemporaryFile\r\n        return NamedTemporaryFile(\r\n      File \"\/home\/nwoke\/anaconda3\/lib\/python3.9\/tempfile.py\", line 541, in NamedTemporaryFile\r\n        (fd, name) = _mkstemp_inner(dir, prefix, suffix, flags, output_type)\r\n      File \"\/home\/nwoke\/anaconda3\/lib\/python3.9\/tempfile.py\", line 251, in _mkstemp_inner\r\n        fd = _os.open(file, flags, 0o600)\r\n    PermissionError: [Errno 13] Permission denied: '\/home\/nwoke\/Documents\/git_cloned\/Github\/Machine-Learning-Engineering-with-MLflow\/Chapter01\/stockpred\/mlruns\/0\/c3fe1649dc284a9d82fbdb037666fc03\/artifacts\/model_random_forest\/condaenv.89qb6n18.requirements.txt'\r\n\r\n`$ \/home\/nwoke\/anaconda3\/bin\/conda-env create -n mlflow-04d87ea036b44c4189dc1f3a9f0d282b85dfcf87 --file \/home\/nwoke\/Documents\/git_cloned\/Github\/Machine-Learning-Engineering-with-MLflow\/Chapter01\/stockpred\/mlruns\/0\/c3fe1649dc284a9d82fbdb037666fc03\/artifacts\/model_random_forest\/conda.yaml`\r\n\r\n  environment variables:\r\n                 CIO_TEST=<not set>\r\n  CONDA_AUTO_UPDATE_CONDA=false\r\n                CONDA_EXE=\/home\/nwoke\/anaconda3\/bin\/conda\r\n         CONDA_PYTHON_EXE=\/home\/nwoke\/anaconda3\/bin\/python\r\n               CONDA_ROOT=\/home\/nwoke\/anaconda3\r\n              CONDA_SHLVL=0\r\n           CURL_CA_BUNDLE=<not set>\r\n            DEFAULTS_PATH=\/usr\/share\/gconf\/ubuntu.default.path\r\n       LIBVA_DRIVERS_PATH=\/opt\/intel\/mediasdk\/lib64\r\n           MANDATORY_PATH=\/usr\/share\/gconf\/ubuntu.mandatory.path\r\n                     PATH=\/home\/nwoke\/anaconda3\/condabin:\/home\/nwoke\/.local\/bin:\/usr\/local\/sbin:\r\n                          \/usr\/local\/bin:\/usr\/sbin:\/usr\/bin:\/sbin:\/bin:\/usr\/games:\/usr\/local\/gam\r\n                          es:\/snap\/bin:\/home\/nwoke\/.dotnet\/tools\r\n       REQUESTS_CA_BUNDLE=<not set>\r\n            SSL_CERT_FILE=<not set>\r\n               WINDOWPATH=2\r\n\r\n     active environment : None\r\n            shell level : 0\r\n       user config file : \/home\/nwoke\/.condarc\r\n populated config files : \/home\/nwoke\/.condarc\r\n          conda version : 4.11.0\r\n    conda-build version : 3.21.5\r\n         python version : 3.9.7.final.0\r\n       virtual packages : __linux=5.4.0=0\r\n                          __glibc=2.27=0\r\n                          __unix=0=0\r\n                          __archspec=1=x86_64\r\n       base environment : \/home\/nwoke\/anaconda3  (writable)\r\n      conda av data dir : \/home\/nwoke\/anaconda3\/etc\/conda\r\n  conda av metadata url : None\r\n           channel URLs : https:\/\/conda.anaconda.org\/conda-forge\/linux-64\r\n                          https:\/\/conda.anaconda.org\/conda-forge\/noarch\r\n                          https:\/\/repo.anaconda.com\/pkgs\/main\/linux-64\r\n                          https:\/\/repo.anaconda.com\/pkgs\/main\/noarch\r\n                          https:\/\/repo.anaconda.com\/pkgs\/r\/linux-64\r\n                          https:\/\/repo.anaconda.com\/pkgs\/r\/noarch\r\n          package cache : \/home\/nwoke\/anaconda3\/pkgs\r\n                          \/home\/nwoke\/.conda\/pkgs\r\n       envs directories : \/home\/nwoke\/anaconda3\/envs\r\n                          \/home\/nwoke\/opt\/anaconda3\/envs\r\n                          \/home\/nwoke\/.conda\/envs\r\n               platform : linux-64\r\n             user-agent : conda\/4.11.0 requests\/2.26.0 CPython\/3.9.7 Linux\/5.4.0-94-generic ubuntu\/18.04.6 glibc\/2.27\r\n                UID:GID : 1000:1000\r\n             netrc file : None\r\n           offline mode : False\r\n\r\n\r\nAn unexpected error has occurred. Conda has prepared the above report.\r\n\r\nTraceback (most recent call last):\r\n  File \"\/home\/nwoke\/.local\/bin\/mlflow\", line 8, in <module>\r\n    sys.exit(cli())\r\n  File \"\/home\/nwoke\/.local\/lib\/python3.8\/site-packages\/click\/core.py\", line 1128, in __call__\r\n    return self.main(*args, **kwargs)\r\n  File \"\/home\/nwoke\/.local\/lib\/python3.8\/site-packages\/click\/core.py\", line 1053, in main\r\n    rv = self.invoke(ctx)\r\n  File \"\/home\/nwoke\/.local\/lib\/python3.8\/site-packages\/click\/core.py\", line 1659, in invoke\r\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\r\n  File \"\/home\/nwoke\/.local\/lib\/python3.8\/site-packages\/click\/core.py\", line 1659, in invoke\r\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\r\n  File \"\/home\/nwoke\/.local\/lib\/python3.8\/site-packages\/click\/core.py\", line 1395, in invoke\r\n    return ctx.invoke(self.callback, **ctx.params)\r\n  File \"\/home\/nwoke\/.local\/lib\/python3.8\/site-packages\/click\/core.py\", line 754, in invoke\r\n    return __callback(*args, **kwargs)\r\n  File \"\/home\/nwoke\/.local\/lib\/python3.8\/site-packages\/mlflow\/models\/cli.py\", line 57, in serve\r\n    return _get_flavor_backend(\r\n  File \"\/home\/nwoke\/.local\/lib\/python3.8\/site-packages\/mlflow\/pyfunc\/backend.py\", line 78, in serve\r\n    return _execute_in_conda_env(\r\n  File \"\/home\/nwoke\/.local\/lib\/python3.8\/site-packages\/mlflow\/pyfunc\/backend.py\", line 144, in _execute_in_conda_env\r\n    conda_env_name = get_or_create_conda_env(conda_env_path, env_id=env_id)\r\n  File \"\/home\/nwoke\/.local\/lib\/python3.8\/site-packages\/mlflow\/utils\/conda.py\", line 135, in get_or_create_conda_env\r\n    process.exec_cmd(\r\n  File \"\/home\/nwoke\/.local\/lib\/python3.8\/site-packages\/mlflow\/utils\/process.py\", line 40, in exec_cmd\r\n    raise ShellCommandException(\"Non-zero exitcode: %s\" % (exit_code))\r\nmlflow.utils.process.ShellCommandException: Non-zero exitcode: 1\r\n```\r\n\r\n\r\n","pull_request":null,"labels":["bug"],"labels_description":["Something isn't working"],"entities":["conda","pip","dependency","failure"]},{"html_url":"https:\/\/github.com\/mlflow\/mlflow\/issues\/5262","id":1101430020,"number":5262,"title":"[bug] mlflow destructor fails when using tensorflow mirroredstrategy","state":"open","locked":false,"assignee":null,"assignees":[],"comments":3,"created_at":1642066044000,"updated_at":1643394179000,"closed_at":null,"body":"### Willingness to contribute\r\nThe MLflow Community encourages bug fix contributions. Would you or another member of your organization be willing to contribute a fix for this bug to the MLflow code base?\r\n\r\n- [ ] Yes. I can contribute a fix for this bug independently.\r\n- [x] Yes. I would be willing to contribute a fix for this bug with guidance from the MLflow community.\r\n- [ ] No. I cannot contribute a bug fix at this time.\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in MLflow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 20.04.3 LTS\r\n- **MLflow installed from (source or binary)**: pip\r\n- **MLflow version (run ``mlflow --version``)**: mlflow, version 1.22.0\r\n- **Python version**: Python 3.8.12\r\n- **npm version, if running the dev UI**:\r\n- **Exact command to reproduce**: `python example.py`\r\n- **Tensorflow version**: 2.7\r\n\r\n### Describe the problem\r\nWhenever I try to use mlflow with TensorFlow's mirrored strategy, the destructor throws an exception related to Multiprocessing. This error occurs even when using a single GPU.\r\n\r\n### Code to reproduce issue\r\n```python\r\n#! \/usr\/bin\/env python\r\n\r\nimport mlflow\r\nimport tensorflow as tf\r\n\r\nif tf.config.list_physical_devices('GPU'):\r\n    strategy = tf.distribute.MirroredStrategy()\r\nelse:  # Use the Default Strategy\r\n    strategy = tf.distribute.get_strategy()\r\n\r\n\r\ndef main():\r\n    return\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n```\r\n\r\n### Other info \/ logs\r\n\r\n## Output\r\n\r\n```\r\nException ignored in: <function Pool.__del__ at 0x7f791ce11820>\r\nTraceback (most recent call last):\r\n  File \"\/home\/homeGlobal\/lcampos\/anaconda3\/envs\/base-tf-cuda-env\/lib\/python3.8\/multiprocessing\/pool.py\", line 268, in __del__\r\n  File \"\/home\/homeGlobal\/lcampos\/anaconda3\/envs\/base-tf-cuda-env\/lib\/python3.8\/multiprocessing\/queues.py\", line 362, in put\r\nAttributeError: 'NoneType' object has no attribute 'dumps'\r\n```\r\n\r\n\r\n## Conda environment\r\nhttps:\/\/gist.github.com\/LucasCampos\/129db886abca17a76561edc912670e2a\r\n\r\n### What component(s), interfaces, languages, and integrations does this bug affect?\r\nComponents \r\n- [ ] `area\/artifacts`: Artifact stores and artifact logging\r\n- [ ] `area\/build`: Build and test infrastructure for MLflow\r\n- [ ] `area\/docs`: MLflow documentation pages\r\n- [ ] `area\/examples`: Example code\r\n- [ ] `area\/model-registry`: Model Registry service, APIs, and the fluent client calls for Model Registry\r\n- [ ] `area\/models`: MLmodel format, model serialization\/deserialization, flavors\r\n- [ ] `area\/projects`: MLproject format, project running backends\r\n- [ ] `area\/scoring`: MLflow Model server, model deployment tools, Spark UDFs\r\n- [ ] `area\/server-infra`: MLflow Tracking server backend\r\n- [x] `area\/tracking`: Tracking Service, tracking client APIs, autologging","pull_request":null,"labels":["bug","area\/tracking"],"labels_description":["Something isn't working","Tracking service, tracking client APIs, autologging"],"entities":["destructor","tensorflow","mirroredstrategy"]},{"html_url":"https:\/\/github.com\/mlflow\/mlflow\/issues\/5259","id":1100930896,"number":5259,"title":"[bug] keras pyfunc uses keras.model.predict instead of calling as function resulting in large slowdown","state":"open","locked":false,"assignee":null,"assignees":[],"comments":0,"created_at":1642031892000,"updated_at":1642086630000,"closed_at":null,"body":"### Willingness to contribute\r\nThe MLflow Community encourages bug fix contributions. Would you or another member of your organization be willing to contribute a fix for this bug to the MLflow code base?\r\n\r\n- [x] Yes. I can contribute a fix for this bug independently.\r\n- [ ] Yes. I would be willing to contribute a fix for this bug with guidance from the MLflow community.\r\n- [ ] No. I cannot contribute a bug fix at this time.\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in MLflow)**:  No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: All\r\n- **MLflow installed from (source or binary)**: Binary (wheel)\r\n- **MLflow version (run ``mlflow --version``)**: 1.22.0\r\n- **Python version**: All\r\n- **npm version, if running the dev UI**: N\/A\r\n- **Exact command to reproduce**: See below\r\n\r\n### Describe the problem\r\nThe implementation of the Keras pyfunc [calls keras.Model.predict](https:\/\/github.com\/mlflow\/mlflow\/blob\/364aca7daf0fcee3ec407ae0b1b16d9cb3085081\/mlflow\/keras.py#L462-L465) which is designed for large-scale batch prediction (docs: https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/Model#predict). Calling the model as a function directly is orders of magnitude faster and still works on batches of inputs.\r\n\r\n### Code to reproduce issue\r\n```python\r\nimport sklearn.datasets as datasets\r\nimport pandas as pd\r\nimport numpy as np\r\n\r\nimport mlflow\r\n\r\nfrom tensorflow.keras.models import Sequential\r\nfrom tensorflow.keras.layers import Dense\r\nfrom tensorflow.keras.optimizers import SGD\r\n\r\n\r\niris = datasets.load_iris()\r\ndata = pd.DataFrame(\r\n    data=np.c_[iris[\"data\"], iris[\"target\"]], columns=iris[\"feature_names\"] + [\"target\"]\r\n)\r\ny = data[\"target\"]\r\nx = data.drop(\"target\", axis=1)\r\n\r\nmodel = Sequential()\r\nmodel.add(Dense(3, input_dim=4))\r\nmodel.add(Dense(1))\r\nmodel.compile(loss=\"mean_squared_error\", optimizer=SGD())\r\nmodel.fit(x.values, y.values)\r\nmlflow.keras.save_model(model, \"keras_model\")\r\n\r\nmodel_loaded = mlflow.pyfunc.load_model(\"keras_model\")\r\n\r\n# call keras.Model predict method\r\n# %timeit model_loaded._model_impl.keras_model.predict(x.values)\r\n# 30.1 ms \u00b1 4.37 ms per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\r\n\r\n# call keras.Model as function\r\n# %timeit model_loaded._model_impl.keras_model(x.values)\r\n# 556 \u00b5s \u00b1 13.3 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\r\n\r\n# assert parity between calling methods\r\n# np.allclose(model_loaded._model_impl.keras_model.predict(x.values), model_loaded._model_impl.keras_model(x.values))\r\n# True\r\n```\r\n\r\nI don't think this makes a difference in TensorFlow <2.0 since the graph is executed correctly, but in >=2.0, a large amount of setup happens under-the-hood when calling `keras.Model.predict` leading to the slowdown observed here.\r\n\r\n### What component(s), interfaces, languages, and integrations does this bug affect?\r\nComponents \r\n- [ ] `area\/artifacts`: Artifact stores and artifact logging\r\n- [ ] `area\/build`: Build and test infrastructure for MLflow\r\n- [ ] `area\/docs`: MLflow documentation pages\r\n- [ ] `area\/examples`: Example code\r\n- [ ] `area\/model-registry`: Model Registry service, APIs, and the fluent client calls for Model Registry\r\n- [x] `area\/models`: MLmodel format, model serialization\/deserialization, flavors\r\n- [ ] `area\/projects`: MLproject format, project running backends\r\n- [ ] `area\/scoring`: MLflow Model server, model deployment tools, Spark UDFs\r\n- [ ] `area\/server-infra`: MLflow Tracking server backend\r\n- [ ] `area\/tracking`: Tracking Service, tracking client APIs, autologging\r\n\r\nInterface \r\n- [ ] `area\/uiux`: Front-end, user experience, plotting, JavaScript, JavaScript dev server\r\n- [ ] `area\/docker`: Docker use across MLflow's components, such as MLflow Projects and MLflow Models\r\n- [ ] `area\/sqlalchemy`: Use of SQLAlchemy in the Tracking Service or Model Registry\r\n- [ ] `area\/windows`: Windows support\r\n\r\nLanguage \r\n- [ ] `language\/r`: R APIs and clients\r\n- [ ] `language\/java`: Java APIs and clients\r\n- [ ] `language\/new`: Proposals for new client languages\r\n\r\nIntegrations\r\n- [ ] `integrations\/azure`: Azure and Azure ML integrations\r\n- [ ] `integrations\/sagemaker`: SageMaker integrations\r\n- [ ] `integrations\/databricks`: Databricks integrations","pull_request":null,"labels":["bug","area\/models"],"labels_description":["Something isn't working","MLmodel format, model serialization\/deserialization, flavors"],"entities":["function","slowdown"]},{"html_url":"https:\/\/github.com\/mlflow\/mlflow\/issues\/5257","id":1100235885,"number":5257,"title":"[fr] support of nested model output signature","state":"open","locked":false,"assignee":null,"assignees":[],"comments":0,"created_at":1641987362000,"updated_at":1641988015000,"closed_at":null,"body":"Thank you for submitting a feature request. **Before proceeding, please review MLflow's [Issue Policy for feature requests](https:\/\/www.github.com\/mlflow\/mlflow\/blob\/master\/ISSUE_POLICY.md#feature-requests) and the [MLflow Contributing Guide](https:\/\/github.com\/mlflow\/mlflow\/blob\/master\/CONTRIBUTING.rst)**.\r\n\r\n**Please fill in this feature request template to ensure a timely and thorough response.**\r\n\r\n## Willingness to contribute\r\nThe MLflow Community encourages new feature contributions. Would you or another member of your organization be willing to contribute an implementation of this feature (either as an MLflow Plugin or an enhancement to the MLflow code base)?\r\n\r\n- [ ] Yes. I can contribute this feature independently.\r\n- [x] Yes. I would be willing to contribute this feature with guidance from the MLflow community.\r\n- [x] No. I cannot contribute this feature at this time.\r\n\r\n## Proposal Summary\r\n\r\nCurrently, if you would want to make predictions with your model as UDF, only a double, array of doubles, or array of strings is supported. My proposal would be to extend the model output format to more complex output structures, which can be defined using a Spark schema or using the mlflow output signature.\r\n\r\n\r\n## Motivation\r\n- What is the use case for this feature?\r\nMaking model predictions when you have loaded your model as UDF.\r\n- Why is this use case valuable to support for MLflow users in general?\r\nFor example, a model output of `[0.8, 0.9, 2012]`, is much less robust and less flexible than a model output of `{'extracted_x': 2012, 'probabilities': {'label_x': 0.8, 'label_y': 0.9}]`.\r\n- Why is this use case valuable to support for your project(s) or organization?\r\n- Why is it currently difficult to achieve this use case? (please be as specific as possible about why related MLflow features and components are insufficient)\r\n\r\n### What component(s), interfaces, languages, and integrations does this feature affect?\r\nComponents \r\n- [ ] `area\/artifacts`: Artifact stores and artifact logging\r\n- [ ] `area\/build`: Build and test infrastructure for MLflow\r\n- [ ] `area\/docs`: MLflow documentation pages\r\n- [ ] `area\/examples`: Example code\r\n- [ ] `area\/model-registry`: Model Registry service, APIs, and the fluent client calls for Model Registry\r\n- [x] `area\/models`: MLmodel format, model serialization\/deserialization, flavors\r\n- [ ] `area\/projects`: MLproject format, project running backends\r\n- [ ] `area\/scoring`: MLflow Model server, model deployment tools, Spark UDFs\r\n- [ ] `area\/server-infra`: MLflow Tracking server backend\r\n- [ ] `area\/tracking`: Tracking Service, tracking client APIs, autologging\r\n\r\nInterfaces\r\n- [ ] `area\/uiux`: Front-end, user experience, plotting, JavaScript, JavaScript dev server\r\n- [ ] `area\/docker`: Docker use across MLflow's components, such as MLflow Projects and MLflow Models\r\n- [ ] `area\/sqlalchemy`: Use of SQLAlchemy in the Tracking Service or Model Registry\r\n- [ ] `area\/windows`: Windows support\r\n\r\nLanguages \r\n- [ ] `language\/r`: R APIs and clients\r\n- [ ] `language\/java`: Java APIs and clients\r\n- [ ] `language\/new`: Proposals for new client languages\r\n\r\nIntegrations\r\n- [ ] `integrations\/azure`: Azure and Azure ML integrations\r\n- [ ] `integrations\/sagemaker`: SageMaker integrations\r\n- [ ] `integrations\/databricks`: Databricks integrations\r\n\r\n## Details\r\n\r\n(Use this section to include any additional information about the feature. If you have a proposal for how to implement this feature, please include it here. For implementation guidelines, please refer to the [Contributing Guide](https:\/\/github.com\/mlflow\/mlflow\/blob\/master\/CONTRIBUTING.rst#contribution-guidelines).)\r\n\r\n\r\n\r\n","pull_request":null,"labels":["enhancement","area\/models"],"labels_description":["New feature or request","MLmodel format, model serialization\/deserialization, flavors"],"entities":["support","model","output","signature"]},{"html_url":"https:\/\/github.com\/mlflow\/mlflow\/issues\/5251","id":1099187621,"number":5251,"title":"[bug] - error on load_model registered from mlflow cli","state":"open","locked":false,"assignee":null,"assignees":[],"comments":0,"created_at":1641909362000,"updated_at":1641924016000,"closed_at":null,"body":"### Willingness to contribute\r\nThe MLflow Community encourages bug fix contributions. Would you or another member of your organization be willing to contribute a fix for this bug to the MLflow code base?\r\n\r\n- [ ] Yes. I can contribute a fix for this bug independently.\r\n- [X] Yes. I would be willing to contribute a fix for this bug with guidance from the MLflow community.\r\n- [ ] No. I cannot contribute a bug fix at this time.\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in MLflow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: docker image python:3.8-slim-buster\r\n- **MLflow installed from (source or binary)**: binary\r\n- **MLflow version (run ``mlflow --version``)**: 1.22 for the client and server\r\n- **Python version**: 3.8\r\n- **npm version, if running the dev UI**: \r\n- **Exact command to reproduce**: \r\n```\r\nmodel = load_model(\"models:\/{}\/Production\".format(name_registry_model))\r\n```\r\n\r\n### Describe the problem\r\nI am having a weird problem trying to load a model from the uri \"models:\/{}\/Production\" after register the trained model **using the python mlflow cli register_model**.\r\nThe weird part is that when I register the model using the mlflow visual interface, I don't have any problem with it.\r\n\r\n### Code to reproduce issue\r\n```\r\nimport mlflow\r\nfrom kedro_mlflow.config import get_mlflow_config\r\n\r\nmlflow_config = get_mlflow_config()\r\nid_experiment = 0 ##any id you have\r\nname_registry_model = '' ##any registered model you have\r\n\r\n\r\nmlflow_cli = mlflow.tracking.MlflowClient(tracking_uri=mlflow_config.mlflow_tracking_uri)\r\nlast_run = mlflow_cli.search_runs(\r\n                id_experiment , \r\n                run_view_type=mlflow.entities.ViewType.ACTIVE_ONLY, \r\n                order_by=['attribute.start_time DESC'], \r\n                max_results=1)[0]\r\n\r\nmlflow_cli = mlflow.tracking.MlflowClient(tracking_uri=mlflow_config.mlflow_tracking_uri)\r\nmodel_updated_uri = \"runs:\/{}\/{}\".format(last_run.info.run_id, name_experiment)\r\n    new_registry = mlflow.register_model(\r\n    model_updated_uri,\r\n    name_registry_model, \r\n    await_registration_for = 0\r\n)\r\nmlflow_cli.transition_model_version_stage(name_registry_model, new_registry.version, 'production', archive_existing_versions=True)\r\n\r\nmodel = load_model(\"models:\/{}\/Production\".format(name_registry_model))\r\n```\r\n\r\n### Other info \/ logs \/ error message\r\n```\r\nClientError                               Traceback (most recent call last)\r\n~\\AppData\\Local\\Temp\/ipykernel_20040\/3865171810.py in <module>\r\n----> 1 modelo = load_model(\"models:\/{}\/Production\")\r\n      2 modelo\r\n\r\n~\\miniconda3\\envs\\env-dona-herminia\\lib\\site-packages\\mlflow\\pyfunc\\_init_.py in load_model(model_uri, suppress_warnings)\r\n    649                               messages will be emitted.\r\n    650     :param dst_path: The local filesystem path to which to download the model artifact.\r\n--> 651                      This directory must already exist. If unspecified, a local output\r\n    652                      path will be created.\r\n    653     \"\"\"\r\n\r\n~\\miniconda3\\envs\\env-dona-herminia\\lib\\site-packages\\mlflow\\tracking\\artifact_utils.py in _download_artifact_from_uri(artifact_uri, output_path)\r\n     82         prefix = parsed_uri.scheme + \":\"\r\n     83         parsed_uri = parsed_uri._replace(scheme=\"\")\r\n---> 84 \r\n     85     # For models:\/ URIs, it doesn't make sense to initialize a ModelsArtifactRepository with only\r\n     86     # the model name portion of the URI, then call download_artifacts with the version info.\r\n\r\n~\\miniconda3\\envs\\env-dona-herminia\\lib\\site-packages\\mlflow\\store\\artifact\\models_artifact_repo.py in download_artifacts(self, artifact_path, dst_path)\r\n    108         :return: Absolute path of the local filesystem location containing the desired artifacts.\r\n    109         \"\"\"\r\n--> 110         return self.repo.download_artifacts(artifact_path, dst_path)\r\n    111 \r\n    112     def _download_file(self, remote_file_path, local_path):\r\n\r\n~\\miniconda3\\envs\\env-dona-herminia\\lib\\site-packages\\mlflow\\store\\artifact\\artifact_repo.py in download_artifacts(self, artifact_path, dst_path)\r\n    182             )\r\n    183         else:\r\n--> 184             return download_artifact(src_artifact_path=artifact_path, dst_local_dir_path=dst_path)\r\n    185 \r\n    186     @abstractmethod\r\n\r\n~\\miniconda3\\envs\\env-dona-herminia\\lib\\site-packages\\mlflow\\store\\artifact\\artifact_repo.py in download_artifact(src_artifact_path, dst_local_dir_path)\r\n    128             )\r\n    129             self._download_file(\r\n--> 130                 remote_file_path=src_artifact_path, local_path=local_destination_file_path\r\n    131             )\r\n    132             return local_destination_file_path\r\n\r\n~\\miniconda3\\envs\\env-dona-herminia\\lib\\site-packages\\mlflow\\store\\artifact\\s3_artifact_repo.py in _download_file(self, remote_file_path, local_path)\r\n    156         s3_full_path = posixpath.join(s3_root_path, remote_file_path)\r\n    157         s3_client = self._get_s3_client()\r\n--> 158         s3_client.download_file(bucket, s3_full_path, local_path)\r\n    159 \r\n    160     def delete_artifacts(self, artifact_path=None):\r\n\r\n~\\AppData\\Roaming\\Python\\Python37\\site-packages\\boto3\\s3\\inject.py in download_file(self, Bucket, Key, Filename, ExtraArgs, Callback, Config)\r\n    170         return transfer.download_file(\r\n    171             bucket=Bucket, key=Key, filename=Filename,\r\n--> 172             extra_args=ExtraArgs, callback=Callback)\r\n    173 \r\n    174 \r\n\r\n~\\AppData\\Roaming\\Python\\Python37\\site-packages\\boto3\\s3\\transfer.py in download_file(self, bucket, key, filename, extra_args, callback)\r\n    305             bucket, key, filename, extra_args, subscribers)\r\n    306         try:\r\n--> 307             future.result()\r\n    308         # This is for backwards compatibility where when retries are\r\n    309         # exceeded we need to throw the same error from boto3 instead of\r\n\r\n~\\AppData\\Roaming\\Python\\Python37\\site-packages\\s3transfer\\futures.py in result(self)\r\n    104             # however if a KeyboardInterrupt is raised we want want to exit\r\n    105             # out of this and propogate the exception.\r\n--> 106             return self._coordinator.result()\r\n    107         except KeyboardInterrupt as e:\r\n    108             self.cancel()\r\n\r\n~\\AppData\\Roaming\\Python\\Python37\\site-packages\\s3transfer\\futures.py in result(self)\r\n    263         # final result.\r\n    264         if self._exception:\r\n--> 265             raise self._exception\r\n    266         return self._result\r\n    267 \r\n\r\n~\\AppData\\Roaming\\Python\\Python37\\site-packages\\s3transfer\\tasks.py in _main(self, transfer_future, **kwargs)\r\n    253             # Call the submit method to start submitting tasks to execute the\r\n    254             # transfer.\r\n--> 255             self._submit(transfer_future=transfer_future, **kwargs)\r\n    256         except BaseException as e:\r\n    257             # If there was an exception raised during the submission of task\r\n\r\n~\\AppData\\Roaming\\Python\\Python37\\site-packages\\s3transfer\\download.py in _submit(self, client, config, osutil, request_executor, io_executor, transfer_future, bandwidth_limiter)\r\n    341                 Bucket=transfer_future.meta.call_args.bucket,\r\n    342                 Key=transfer_future.meta.call_args.key,\r\n--> 343                 **transfer_future.meta.call_args.extra_args\r\n    344             )\r\n    345             transfer_future.meta.provide_transfer_size(\r\n\r\n~\\AppData\\Roaming\\Python\\Python37\\site-packages\\botocore\\client.py in _api_call(self, *args, **kwargs)\r\n    384                     \"%s() only accepts keyword arguments.\" % py_operation_name)\r\n    385             # The \"self\" in this scope is referring to the BaseClient.\r\n--> 386             return self._make_api_call(operation_name, kwargs)\r\n    387 \r\n    388         _api_call._name_ = str(py_operation_name)\r\n\r\n~\\AppData\\Roaming\\Python\\Python37\\site-packages\\botocore\\client.py in _make_api_call(self, operation_name, api_params)\r\n    703             error_code = parsed_response.get(\"Error\", {}).get(\"Code\")\r\n    704             error_class = self.exceptions.from_code(error_code)\r\n--> 705             raise error_class(parsed_response, operation_name)\r\n    706         else:\r\n    707             return parsed_response\r\n\r\nClientError: An error occurred (404) when calling the HeadObject operation: Not Found\r\n```\r\n\r\n\r\n### What component(s), interfaces, languages, and integrations does this bug affect?\r\nComponents \r\n- [ ] `area\/artifacts`: Artifact stores and artifact logging\r\n- [ ] `area\/build`: Build and test infrastructure for MLflow\r\n- [ ] `area\/docs`: MLflow documentation pages\r\n- [ ] `area\/examples`: Example code\r\n- [X] `area\/model-registry`: Model Registry service, APIs, and the fluent client calls for Model Registry\r\n- [ ] `area\/models`: MLmodel format, model serialization\/deserialization, flavors\r\n- [ ] `area\/projects`: MLproject format, project running backends\r\n- [ ] `area\/scoring`: MLflow Model server, model deployment tools, Spark UDFs\r\n- [ ] `area\/server-infra`: MLflow Tracking server backend\r\n- [ ] `area\/tracking`: Tracking Service, tracking client APIs, autologging\r\n\r\n\r\n\r\n","pull_request":null,"labels":["bug","area\/model-registry"],"labels_description":["Something isn't working","Model registry, model registry APIs, and the fluent client calls for model registry"],"entities":["error","load_model","cli"]},{"html_url":"https:\/\/github.com\/mlflow\/mlflow\/pull\/5250","id":1098867620,"number":5250,"title":"mark \"changing param values is not allowed\" as user initiated failure exception type","state":"open","locked":false,"assignee":null,"assignees":[],"comments":0,"created_at":1641891046000,"updated_at":1642785315000,"closed_at":null,"body":"## What changes are proposed in this pull request?\r\n\r\nMark \"Changing param values is not allowed\" as User initiated failure exception type\r\n\r\n## How is this patch tested?\r\n\r\nN\/A\r\n\r\n## Does this PR change the documentation?\r\n\r\n- [x] No. You can skip the rest of this section.\r\n- [ ] Yes. Make sure the changed pages \/ sections render correctly by following the steps below.\r\n\r\n1. Check the status of the `ci\/circleci: build_doc` check. If it's successful, proceed to the\r\n   next step, otherwise fix it.\r\n2. Click `Details` on the right to open the job page of CircleCI.\r\n3. Click the `Artifacts` tab.\r\n4. Click `docs\/build\/html\/index.html`.\r\n5. Find the changed pages \/ sections and make sure they render correctly.\r\n\r\n## Release Notes\r\n\r\n### Is this a user-facing change?\r\n\r\n- [x] No. You can skip the rest of this section.\r\n- [ ] Yes. Give a description of this change to be included in the release notes for MLflow users.\r\n\r\n(Details in 1-2 sentences. You can just refer to another PR with a description if this PR is part of a larger change.)\r\n\r\n### What component(s), interfaces, languages, and integrations does this PR affect?\r\nComponents \r\n- [ ] `area\/artifacts`: Artifact stores and artifact logging\r\n- [ ] `area\/build`: Build and test infrastructure for MLflow\r\n- [ ] `area\/docs`: MLflow documentation pages\r\n- [ ] `area\/examples`: Example code\r\n- [ ] `area\/model-registry`: Model Registry service, APIs, and the fluent client calls for Model Registry\r\n- [ ] `area\/models`: MLmodel format, model serialization\/deserialization, flavors\r\n- [ ] `area\/projects`: MLproject format, project running backends\r\n- [ ] `area\/scoring`: MLflow Model server, model deployment tools, Spark UDFs\r\n- [ ] `area\/server-infra`: MLflow Tracking server backend\r\n- [x] `area\/tracking`: Tracking Service, tracking client APIs, autologging\r\n\r\nInterface \r\n- [ ] `area\/uiux`: Front-end, user experience, plotting, JavaScript, JavaScript dev server\r\n- [ ] `area\/docker`: Docker use across MLflow's components, such as MLflow Projects and MLflow Models\r\n- [ ] `area\/sqlalchemy`: Use of SQLAlchemy in the Tracking Service or Model Registry\r\n- [ ] `area\/windows`: Windows support\r\n\r\nLanguage \r\n- [ ] `language\/r`: R APIs and clients\r\n- [ ] `language\/java`: Java APIs and clients\r\n- [ ] `language\/new`: Proposals for new client languages\r\n\r\nIntegrations\r\n- [ ] `integrations\/azure`: Azure and Azure ML integrations\r\n- [ ] `integrations\/sagemaker`: SageMaker integrations\r\n- [ ] `integrations\/databricks`: Databricks integrations\r\n\r\n<!--\r\nInsert an empty named anchor here to allow jumping to this section with a fragment URL\r\n(e.g. https:\/\/github.com\/mlflow\/mlflow\/pull\/123#user-content-release-note-category).\r\nNote that GitHub prefixes anchor names in markdown with \"user-content-\".\r\n-->\r\n<a name=\"release-note-category\"><\/a>\r\n### How should the PR be classified in the release notes? Choose one:\r\n\r\n- [ ] `rn\/breaking-change` - The PR will be mentioned in the \"Breaking Changes\" section\r\n- [x] `rn\/none` - No description will be included. The PR will be mentioned only by the PR number in the \"Small Bugfixes and Documentation Updates\" section\r\n- [ ] `rn\/feature` - A new user-facing feature worth mentioning in the release notes\r\n- [ ] `rn\/bug-fix` - A user-facing bug fix worth mentioning in the release notes\r\n- [ ] `rn\/documentation` - A user-facing documentation change worth mentioning in the release notes\r\n","pull_request":{"url":"https:\/\/api.github.com\/repos\/mlflow\/mlflow\/pulls\/5250","html_url":"https:\/\/github.com\/mlflow\/mlflow\/pull\/5250","diff_url":"https:\/\/github.com\/mlflow\/mlflow\/pull\/5250.diff","patch_url":"https:\/\/github.com\/mlflow\/mlflow\/pull\/5250.patch","merged_at":null},"labels":["rn\/none","area\/tracking"],"labels_description":["List under Small Changes in Changelogs.","Tracking service, tracking client APIs, autologging"],"entities":["mark","param","user","failure","exception","type"]},{"html_url":"https:\/\/github.com\/mlflow\/mlflow\/issues\/5247","id":1098301242,"number":5247,"title":"unable to log a tensorflow savedmodel","state":"open","locked":false,"assignee":null,"assignees":[],"comments":1,"created_at":1641843780000,"updated_at":1646721254000,"closed_at":null,"body":"I'm working with MLflow version **1.22.0** and I'm trying to log a TensorFlow model which is in the SavedModel format. When I run the command:\r\n\r\n`mlflow.tensorflow.log_model(tf_saved_model_dir=export_dir, tf_meta_graph_tags=[\"serve\"], artifact_path=\"mlruns\/0\/0dcc80caa5be4574a92a26306e24cdb8\/artifacts\/model\", tf_signature_def_key=None)`\r\n\r\nI get the following error:\r\n\r\n```\r\n2022\/01\/10 14:37:29 INFO mlflow.tensorflow: Validating the specified TensorFlow model by attempting to load it in a new TensorFlow graph...\r\nWARNING:tensorflow:From \/mnt\/3\/shares\/biobert\/miniconda3\/envs\/biobert\/lib\/python3.7\/site-packages\/mlflow\/tensorflow\/__init__.py:405: load (from tensorflow.python.saved_model.loader_impl) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nThis function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.loader.load or tf.compat.v1.saved_model.load. There will be a new function for importing SavedModels in Tensorflow 2.0.\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"\/mnt\/3\/shares\/biobert\/miniconda3\/envs\/biobert\/lib\/python3.7\/site-packages\/mlflow\/utils\/annotations.py\", line 62, in wrapper\r\n    return func(**kwargs)\r\n  File \"\/mnt\/3\/shares\/biobert\/miniconda3\/envs\/biobert\/lib\/python3.7\/site-packages\/mlflow\/tensorflow\/__init__.py\", line 193, in log_model\r\n    extra_pip_requirements=extra_pip_requirements,\r\n  File \"\/mnt\/3\/shares\/biobert\/miniconda3\/envs\/biobert\/lib\/python3.7\/site-packages\/mlflow\/models\/model.py\", line 187, in log\r\n    flavor.save_model(path=local_path, mlflow_model=mlflow_model, **kwargs)\r\n  File \"\/mnt\/3\/shares\/biobert\/miniconda3\/envs\/biobert\/lib\/python3.7\/site-packages\/mlflow\/utils\/annotations.py\", line 62, in wrapper\r\n    return func(**kwargs)\r\n  File \"\/mnt\/3\/shares\/biobert\/miniconda3\/envs\/biobert\/lib\/python3.7\/site-packages\/mlflow\/tensorflow\/__init__.py\", line 263, in save_model\r\n    tf_signature_def_key=tf_signature_def_key,\r\n  File \"\/mnt\/3\/shares\/biobert\/miniconda3\/envs\/biobert\/lib\/python3.7\/site-packages\/mlflow\/tensorflow\/__init__.py\", line 330, in _validate_saved_model\r\n    tf_signature_def_key=tf_signature_def_key,\r\n  File \"\/mnt\/3\/shares\/biobert\/miniconda3\/envs\/biobert\/lib\/python3.7\/site-packages\/mlflow\/tensorflow\/__init__.py\", line 405, in _load_tensorflow_saved_model\r\n    tags=tf_meta_graph_tags, export_dir=tf_saved_model_dir\r\n  File \"\/mnt\/3\/shares\/biobert\/miniconda3\/envs\/biobert\/lib\/python3.7\/site-packages\/tensorflow_core\/python\/util\/deprecation.py\", line 324, in new_func\r\n    return func(*args, **kwargs)\r\nTypeError: load() missing 1 required positional argument: 'sess'\r\n```\r\n\r\nA similar issue is occurring with the command **mlflow.tensorflow.save_model** as well. Was \"**sess**\" required in previous versions? I found some references to this parameter in the MLflow Documentation.\r\n\r\nNote that I'm able to load the model with the TensorFlow library, without passing through MLflow.\r\n\r\nThank you in advance for your help!","pull_request":null,"labels":[],"labels_description":[],"entities":["tensorflow","savedmodel"]},{"html_url":"https:\/\/github.com\/mlflow\/mlflow\/issues\/5241","id":1097283926,"number":5241,"title":"allow users to download all run artifacts from a model uri","state":"open","locked":false,"assignee":null,"assignees":[],"comments":1,"created_at":1641757385000,"updated_at":1644226246000,"closed_at":null,"body":"## Willingness to contribute\r\nThe MLflow Community encourages new feature contributions. Would you or another member of your organization be willing to contribute an implementation of this feature (either as an MLflow Plugin or an enhancement to the MLflow code base)?\r\n\r\n- [x] Yes. I can contribute this feature independently.\r\n- [x] Yes. I would be willing to contribute this feature with guidance from the MLflow community.\r\n- [ ] No. I cannot contribute this feature at this time.\r\n\r\n## Proposal Summary\r\n\r\nAdding an `--include-all` flag to the `mlflow artifacts download` command would facilitate more flexible use MLFlow in CI\/CD flows by linking relative model paths to run artifacts directly. \r\n\r\n## Motivation\r\n- What is the use case for this feature?\r\n\r\n  ML teams orchestrate regular retraining runs in a workflow manager. When adopting MLFlow for tracking these runs, there is a ton of flexibility for workable promotion processes that MLFlow's ModelRegistry and tagging system can facilitate.  \r\n  \r\n  For example, when running CI on a staging branch, we can include a make command that calls `mlflow artifacts download --artifact-uri models:\/my_model\/Staging --include-all` to grab any artifacts required for serving, testing, or reporting. \r\n\r\n- Why is this use case valuable to support for MLflow users in general?\r\n\r\n    It makes integrating the ModelRegistry with automated testing easier. \r\n\r\n- Why is this use case valuable to support for your project(s) or organization?\r\n\r\n    It makes integrating the ModelRegistry with automated testing easier. \r\n\r\n- Why is it currently difficult to achieve this use case? (please be as specific as possible about why related MLflow features and components are insufficient)\r\n\r\n    Rather than being a component of the CLI, we write a script. While this is not difficult, having this feature in the CLI would better enable model testing.\r\n\r\n```Python\r\nfrom mlflow.store.artifact.models_artifact_repo import ModelsArtifactRepository\r\n\r\nmodel_uri = \"models:\/my_model\/Production\"\r\nmodel_path = ModelsArtifactRepository.get_underlying_uri(model_uri)\r\nartifacts_path = model_path.strip('\/model')\r\n```\r\n\r\n### What component(s), interfaces, languages, and integrations does this feature affect?\r\nComponents \r\n- [x] `area\/artifacts`: Artifact stores and artifact logging\r\n- [ ] `area\/build`: Build and test infrastructure for MLflow\r\n- [ ] `area\/docs`: MLflow documentation pages\r\n- [ ] `area\/examples`: Example code\r\n- [ ] `area\/model-registry`: Model Registry service, APIs, and the fluent client calls for Model Registry\r\n- [ ] `area\/models`: MLmodel format, model serialization\/deserialization, flavors\r\n- [ ] `area\/projects`: MLproject format, project running backends\r\n- [ ] `area\/scoring`: MLflow Model server, model deployment tools, Spark UDFs\r\n- [ ] `area\/server-infra`: MLflow Tracking server backend\r\n- [ ] `area\/tracking`: Tracking Service, tracking client APIs, autologging\r\n\r\nInterfaces\r\n- [ ] `area\/uiux`: Front-end, user experience, plotting, JavaScript, JavaScript dev server\r\n- [ ] `area\/docker`: Docker use across MLflow's components, such as MLflow Projects and MLflow Models\r\n- [ ] `area\/sqlalchemy`: Use of SQLAlchemy in the Tracking Service or Model Registry\r\n- [ ] `area\/windows`: Windows support\r\n\r\nLanguages \r\n- [ ] `language\/r`: R APIs and clients\r\n- [ ] `language\/java`: Java APIs and clients\r\n- [ ] `language\/new`: Proposals for new client languages\r\n\r\nIntegrations\r\n- [ ] `integrations\/azure`: Azure and Azure ML integrations\r\n- [ ] `integrations\/sagemaker`: SageMaker integrations\r\n- [ ] `integrations\/databricks`: Databricks integrations\r\n\r\n## Details\r\n\r\nThe `ArtifactRepositoryRegistry` interface is great and I wouldn't want to modify it for this small quality-of-life improvement. It would muddy the distinction between a `RunsArtifactRepository` and a `ModelsArtifactRepository`. \r\n\r\nOne implementation path would look like the following:\r\n\r\n**1.  Add the `include-all` flag to the `mlflow artifacts download` command**\r\n```Python\r\n@commands.command(\"download\")\r\n@click.option(\"--run-id\", \"-r\", help=\"Run ID from which to download\")\r\n@click.option(\r\n    \"--artifact-path\",\r\n    \"-a\",\r\n    help=\"For use with Run ID: if specified, a path relative to the run's root \"\r\n    \"directory to download\",\r\n)\r\n@click.option(\r\n    \"--artifact-uri\",\r\n    \"-u\",\r\n    help=\"URI pointing to the artifact file or artifacts directory; use as an \"\r\n    \"alternative to specifying --run_id and --artifact-path\",\r\n)\r\n@click.option(\"--include-all\/--no-include-all\", default=False)\r\ndef download_artifacts(run_id, artifact_path, artifact_uri, include_all):\r\n    \"\"\"\r\n    Download an artifact file or directory to a local directory.\r\n    The output is the name of the file or directory on the local disk.\r\n\r\n    The ``include-all`` flag allows users to pull all artifacts from a\r\n    run using a relative model uri like models:\/<model>\/Staging\r\n\r\n    Either ``--run-id`` or ``--artifact-uri`` must be provided.\r\n    \"\"\"\r\n    if run_id is None and artifact_uri is None:\r\n        _logger.error(\"Either ``--run-id`` or ``--artifact-uri`` must be provided.\")\r\n        sys.exit(1)\r\n\r\n    if run_id is not None and not include_all:\r\n        _logger.error(\"The ``-no-include-all`` flag only works with ``--artifact-uri``\")\r\n        sys.exit(1)\r\n\r\n    if artifact_uri is not None:\r\n        print(_download_artifact_from_uri(artifact_uri, include_all=include_all))\r\n        return\r\n\r\n    artifact_path = artifact_path if artifact_path is not None else \"\"\r\n    store = _get_store()\r\n    artifact_uri = store.get_run(run_id).info.artifact_uri\r\n    artifact_repo = get_artifact_repository(artifact_uri)\r\n    artifact_location = artifact_repo.download_artifacts(artifact_path)\r\n    print(artifact_location)\r\n```\r\n\r\n**2.  Adjust how this flag is handled in `_download_artifact_from_uri`**\r\n```Python\r\ndef _download_artifact_from_uri(artifact_uri, output_path=None, include_all=False):\r\n    \"\"\"\r\n    :param artifact_uri: The *absolute* URI of the artifact to download.\r\n    :param output_path: The local filesystem path to which to download the artifact. If unspecified,\r\n                        a local output path will be created.\r\n    \"\"\"\r\n    if os.path.exists(artifact_uri):\r\n        if os.name != \"nt\":\r\n            # If we're dealing with local files, just reference the direct pathing.\r\n            # non-nt-based file systems can directly reference path information, while nt-based\r\n            # systems need to url-encode special characters in directory listings to be able to\r\n            # resolve them (i.e., spaces converted to %20 within a file name or path listing)\r\n            root_uri = os.path.dirname(artifact_uri)\r\n            artifact_path = os.path.basename(artifact_uri)\r\n            return get_artifact_repository(artifact_uri=root_uri).download_artifacts(\r\n                artifact_path=artifact_path, dst_path=output_path\r\n            )\r\n        else:  # if we're dealing with nt-based systems, we need to utilize pathname2url to encode.\r\n            artifact_uri = path_to_local_file_uri(artifact_uri)\r\n\r\n    parsed_uri = urllib.parse.urlparse(str(artifact_uri))\r\n    prefix = \"\"\r\n    if parsed_uri.scheme and not parsed_uri.path.startswith(\"\/\"):\r\n        # relative path is a special case, urllib does not reconstruct it properly\r\n        prefix = parsed_uri.scheme + \":\"\r\n        parsed_uri = parsed_uri._replace(scheme=\"\")\r\n\r\n    # For models:\/ URIs, it doesn't make sense to initialize a ModelsArtifactRepository with only\r\n    # the model name portion of the URI, then call download_artifacts with the version info.\r\n    if ModelsArtifactRepository.is_models_uri(artifact_uri):\r\n        # Prototyped change would be here\r\n        if include_all:\r\n            model_only_artifact_uri = ModelsArtifactRepository.get_underlying_uri(artifact_uri)\r\n            artifact_uri = model_only_artifact_uri.strip(\"\/model\")\r\n        root_uri = artifact_uri\r\n        artifact_path = \"\"\r\n    else:\r\n        artifact_path = posixpath.basename(parsed_uri.path)\r\n        parsed_uri = parsed_uri._replace(path=posixpath.dirname(parsed_uri.path))\r\n        root_uri = prefix + urllib.parse.urlunparse(parsed_uri)\r\n\r\n    return get_artifact_repository(artifact_uri=root_uri).download_artifacts(\r\n        artifact_path=artifact_path, dst_path=output_path\r\n    )\r\n\r\n```\r\n","pull_request":null,"labels":["enhancement","area\/artifacts"],"labels_description":["New feature or request","Artifact stores and artifact logging"],"entities":["model","uri"]},{"html_url":"https:\/\/github.com\/mlflow\/mlflow\/pull\/5237","id":1096720464,"number":5237,"title":"bump protobuf-java from 3.6.0 to 3.16.1 in \/mlflow\/java","state":"open","locked":false,"assignee":null,"assignees":[],"comments":0,"created_at":1641594827000,"updated_at":1641594828000,"closed_at":null,"body":"Bumps [protobuf-java](https:\/\/github.com\/protocolbuffers\/protobuf) from 3.6.0 to 3.16.1.\n<details>\n<summary>Release notes<\/summary>\n<p><em>Sourced from <a href=\"https:\/\/github.com\/protocolbuffers\/protobuf\/releases\">protobuf-java's releases<\/a>.<\/em><\/p>\n<blockquote>\n<h2>Protocol Buffers v3.16.1<\/h2>\n<h1>Java<\/h1>\n<ul>\n<li>Improve performance characteristics of UnknownFieldSet parsing (<a href=\"https:\/\/github-redirect.dependabot.com\/protocolbuffers\/protobuf\/issues\/9371\">#9371<\/a>)<\/li>\n<\/ul>\n<h2>Protocol Buffers v3.16.0<\/h2>\n<h1>C++<\/h1>\n<ul>\n<li>Fix compiler warnings issue found in conformance_test_runner <a href=\"https:\/\/github-redirect.dependabot.com\/protocolbuffers\/protobuf\/issues\/8189\">#8189<\/a> (<a href=\"https:\/\/github-redirect.dependabot.com\/protocolbuffers\/protobuf\/issues\/8190\">#8190<\/a>)<\/li>\n<li>Fix MinGW-w64 build issues. (<a href=\"https:\/\/github-redirect.dependabot.com\/protocolbuffers\/protobuf\/issues\/8286\">#8286<\/a>)<\/li>\n<li>[Protoc] C++ Resolved an issue where NO_DESTROY and CONSTINIT are in incorrect order (<a href=\"https:\/\/github-redirect.dependabot.com\/protocolbuffers\/protobuf\/issues\/8296\">#8296<\/a>)<\/li>\n<li>Fix PROTOBUF_CONSTINIT macro redefinition (<a href=\"https:\/\/github-redirect.dependabot.com\/protocolbuffers\/protobuf\/issues\/8323\">#8323<\/a>)<\/li>\n<li>Delete StringPiecePod (<a href=\"https:\/\/github-redirect.dependabot.com\/protocolbuffers\/protobuf\/issues\/8353\">#8353<\/a>)<\/li>\n<li>Fix gcc error: comparison of unsigned expression in '&gt;= 0' is always \u2026 (<a href=\"https:\/\/github-redirect.dependabot.com\/protocolbuffers\/protobuf\/issues\/8309\">#8309<\/a>)<\/li>\n<li>Fix cmake install on iOS (<a href=\"https:\/\/github-redirect.dependabot.com\/protocolbuffers\/protobuf\/issues\/8301\">#8301<\/a>)<\/li>\n<li>Create a CMake option to control whether or not RTTI is enabled (<a href=\"https:\/\/github-redirect.dependabot.com\/protocolbuffers\/protobuf\/issues\/8347\">#8347<\/a>)<\/li>\n<li>Fix endian.h location on FreeBSD (<a href=\"https:\/\/github-redirect.dependabot.com\/protocolbuffers\/protobuf\/issues\/8351\">#8351<\/a>)<\/li>\n<li>Refactor util::Status (<a href=\"https:\/\/github-redirect.dependabot.com\/protocolbuffers\/protobuf\/issues\/8354\">#8354<\/a>)<\/li>\n<li>Make util::Status more similar to absl::Status (<a href=\"https:\/\/github-redirect.dependabot.com\/protocolbuffers\/protobuf\/issues\/8405\">#8405<\/a>)<\/li>\n<li>Fix -Wsuggest-destructor-override for generated C++ proto classes. (<a href=\"https:\/\/github-redirect.dependabot.com\/protocolbuffers\/protobuf\/issues\/8408\">#8408<\/a>)<\/li>\n<li>Refactor StatusOr and StringPiece (<a href=\"https:\/\/github-redirect.dependabot.com\/protocolbuffers\/protobuf\/issues\/8406\">#8406<\/a>)<\/li>\n<li>Refactor uint128 (<a href=\"https:\/\/github-redirect.dependabot.com\/protocolbuffers\/protobuf\/issues\/8416\">#8416<\/a>)<\/li>\n<li>The ::pb namespace is no longer exposed due to conflicts.<\/li>\n<li>Allow MessageDifferencer::TreatAsSet() (and friends) to override previous\ncalls instead of crashing.<\/li>\n<li>Reduce the size of generated proto headers for protos with <code>string<\/code> or\n<code>bytes<\/code> fields.<\/li>\n<li>Move arena() operation on uncommon path to out-of-line routine<\/li>\n<li>For iterator-pair function parameter types, take both iterators by value.<\/li>\n<li>Code-space savings and perhaps some modest performance improvements in\nRepeatedPtrField.<\/li>\n<li>Eliminate nullptr check from every tag parse.<\/li>\n<li>Remove unused _$name$<em>cached_byte_size<\/em> fields.<\/li>\n<li>Serialize extension ranges together when not broken by a proto field in the\nmiddle.<\/li>\n<li>Do out-of-line allocation and deallocation of string object in ArenaString.<\/li>\n<li>Streamline ParseContext::ParseMessage<!-- raw HTML omitted --> to avoid code bloat and improve\nperformance.<\/li>\n<li>New member functions RepeatedField::Assign, RepeatedPtrField::{Add, Assign}.<\/li>\n<li>Fix undefined behavior warning due to innocuous uninitialization of value\non an error path.<\/li>\n<li>Avoid expensive inlined code space for encoding message length for messages\n<blockquote>\n<p>= 128 bytes and instead do a procedure call to a shared out-of-line routine.<\/p>\n<\/blockquote>\n<\/li>\n<li>util::DefaultFieldComparator will be final in a future version of protobuf.\nSubclasses should inherit from SimpleFieldComparator instead.<\/li>\n<\/ul>\n<h1>C#<\/h1>\n<ul>\n<li>Add .NET 5 target and improve WriteString performance with SIMD (<a href=\"https:\/\/github-redirect.dependabot.com\/protocolbuffers\/protobuf\/issues\/8147\">#8147<\/a>)<\/li>\n<\/ul>\n<h1>Java<\/h1>\n<ul>\n<li>deps: update JUnit and Truth (<a href=\"https:\/\/github-redirect.dependabot.com\/protocolbuffers\/protobuf\/issues\/8319\">#8319<\/a>)<\/li>\n<li>Detect invalid overflow of byteLimit and return InvalidProtocolBufferException as documented.<\/li>\n<\/ul>\n<!-- raw HTML omitted -->\n<\/blockquote>\n<p>... (truncated)<\/p>\n<\/details>\n<details>\n<summary>Commits<\/summary>\n<ul>\n<li><a href=\"https:\/\/github.com\/protocolbuffers\/protobuf\/commit\/791a4355c365bd92720160671a7491be168055cb\"><code>791a435<\/code><\/a> Update protobuf version<\/li>\n<li><a href=\"https:\/\/github.com\/protocolbuffers\/protobuf\/commit\/e8918723cfecb60082f067a98df0a16ffc003b62\"><code>e891872<\/code><\/a> Update CHANGES.txt for 3.16.1 release<\/li>\n<li><a href=\"https:\/\/github.com\/protocolbuffers\/protobuf\/commit\/f554ccaa514967232cc494cf22947e1c73ca747f\"><code>f554cca<\/code><\/a> Improve performance of parsing unknown fields in Java (<a href=\"https:\/\/github-redirect.dependabot.com\/protocolbuffers\/protobuf\/issues\/9371\">#9371<\/a>)<\/li>\n<li><a href=\"https:\/\/github.com\/protocolbuffers\/protobuf\/commit\/2dc747c574b68a808ea4699d26942c8132fe2b09\"><code>2dc747c<\/code><\/a> Update PHP release notes and update version to 3.16.0 (<a href=\"https:\/\/github-redirect.dependabot.com\/protocolbuffers\/protobuf\/issues\/8573\">#8573<\/a>)<\/li>\n<li><a href=\"https:\/\/github.com\/protocolbuffers\/protobuf\/commit\/debc03dfc5d71d7d642dd1c8f7d1c04b36e8a065\"><code>debc03d<\/code><\/a> Update protobuf version to 3.16.0-rc2 (<a href=\"https:\/\/github-redirect.dependabot.com\/protocolbuffers\/protobuf\/issues\/8556\">#8556<\/a>)<\/li>\n<li><a href=\"https:\/\/github.com\/protocolbuffers\/protobuf\/commit\/17b0fb9149109e22d56cfa27f1f17b04508ea726\"><code>17b0fb9<\/code><\/a> Make update_version.py compatible with Python 3 (<a href=\"https:\/\/github-redirect.dependabot.com\/protocolbuffers\/protobuf\/issues\/8555\">#8555<\/a>)<\/li>\n<li><a href=\"https:\/\/github.com\/protocolbuffers\/protobuf\/commit\/4aa425c6c5eb7914582ebd67ab8ecac464bdf271\"><code>4aa425c<\/code><\/a> Cherry-pick <a href=\"https:\/\/github-redirect.dependabot.com\/protocolbuffers\/protobuf\/issues\/8356\">#8356<\/a> into 3.16.x (<a href=\"https:\/\/github-redirect.dependabot.com\/protocolbuffers\/protobuf\/issues\/8518\">#8518<\/a>)<\/li>\n<li><a href=\"https:\/\/github.com\/protocolbuffers\/protobuf\/commit\/e8b78f8208971a28e566198d38e43ad5f49a9009\"><code>e8b78f8<\/code><\/a> Fixed memory leak of Ruby arena objects. (<a href=\"https:\/\/github-redirect.dependabot.com\/protocolbuffers\/protobuf\/issues\/8465\">#8465<\/a>)<\/li>\n<li><a href=\"https:\/\/github.com\/protocolbuffers\/protobuf\/commit\/7689f00ba8d1e818f2a8e7a4bf24577d9ccd5d84\"><code>7689f00<\/code><\/a> Update protobuf version (<a href=\"https:\/\/github-redirect.dependabot.com\/protocolbuffers\/protobuf\/issues\/8448\">#8448<\/a>)<\/li>\n<li><a href=\"https:\/\/github.com\/protocolbuffers\/protobuf\/commit\/6099c6505d73681bf98a5c5d8908cb5c3fd1bab9\"><code>6099c65<\/code><\/a> Updated CHANGES.txt for 3.16.0 (<a href=\"https:\/\/github-redirect.dependabot.com\/protocolbuffers\/protobuf\/issues\/8456\">#8456<\/a>)<\/li>\n<li>Additional commits viewable in <a href=\"https:\/\/github.com\/protocolbuffers\/protobuf\/compare\/v3.6.0...v3.16.1\">compare view<\/a><\/li>\n<\/ul>\n<\/details>\n<br \/>\n\n\n[![Dependabot compatibility score](https:\/\/dependabot-badges.githubapp.com\/badges\/compatibility_score?dependency-name=com.google.protobuf:protobuf-java&package-manager=maven&previous-version=3.6.0&new-version=3.16.1)](https:\/\/docs.github.com\/en\/github\/managing-security-vulnerabilities\/about-dependabot-security-updates#about-compatibility-scores)\n\nDependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.\n\n[\/\/]: # (dependabot-automerge-start)\n[\/\/]: # (dependabot-automerge-end)\n\n---\n\n<details>\n<summary>Dependabot commands and options<\/summary>\n<br \/>\n\nYou can trigger Dependabot actions by commenting on this PR:\n- `@dependabot rebase` will rebase this PR\n- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it\n- `@dependabot merge` will merge this PR after your CI passes on it\n- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it\n- `@dependabot cancel merge` will cancel a previously requested merge and block automerging\n- `@dependabot reopen` will reopen this PR if it is closed\n- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually\n- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language\n- `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language\n- `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language\n- `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language\n\nYou can disable automated security fix PRs for this repo from the [Security Alerts page](https:\/\/github.com\/mlflow\/mlflow\/network\/alerts).\n\n<\/details>","pull_request":{"url":"https:\/\/api.github.com\/repos\/mlflow\/mlflow\/pulls\/5237","html_url":"https:\/\/github.com\/mlflow\/mlflow\/pull\/5237","diff_url":"https:\/\/github.com\/mlflow\/mlflow\/pull\/5237.diff","patch_url":"https:\/\/github.com\/mlflow\/mlflow\/pull\/5237.patch","merged_at":null},"labels":["dependencies","java"],"labels_description":["Pull requests that update a dependency file","Pull requests that update Java code"],"entities":["bump","protobuf-java"]},{"html_url":"https:\/\/github.com\/mlflow\/mlflow\/issues\/5233","id":1096128259,"number":5233,"title":"[fr] make autogenerated model prediction code snippet adaptable","state":"open","locked":false,"assignee":null,"assignees":[],"comments":0,"created_at":1641546929000,"updated_at":1641547005000,"closed_at":null,"body":"## Willingness to contribute\r\nThe MLflow Community encourages new feature contributions. Would you or another member of your organization be willing to contribute an implementation of this feature (either as an MLflow Plugin or an enhancement to the MLflow code base)?\r\n\r\n- [ ] Yes. I can contribute this feature independently.\r\n- [ ] Yes. I would be willing to contribute this feature with guidance from the MLflow community.\r\n- [X] No. I cannot contribute this feature at this time.\r\n\r\n## Proposal Summary\r\n\r\nThe browser overview of a single run comes with the displayal of artifcats - one of them being the 'model' artifact. Conveniently, this also comes with code snippets on how to use the model at hand for predictions. As of now, these code snippets [seem hard to adapt](https:\/\/github.com\/mlflow\/mlflow\/blob\/master\/mlflow\/server\/js\/src\/experiment-tracking\/components\/artifact-view-components\/ShowArtifactLoggedModelView.js#L90-L121). For some users, the currently generated code snippets are not applicable and it would be desirable to change the snippets to custom code.\r\n\r\n## Motivation\r\n- What is the use case for this feature?\r\nA priori, a user will believe the code snippets to be correct and applicable - in some scenarios, e.g. when there are further dependencies for prediction such as a chaining of models, it doesn't. In these cases, it would be desirable to overwrite the generated code snippets. Moreover, other aspects, such as data loading, could be included if desired.\r\n- Why is this use case valuable to support for MLflow users in general?\r\nBeing able to customize these snippets could e.g. also allow to allow for some lines on data loading to be included. This, in turn, would make the snippets more complete and valuable.\r\n- Why is this use case valuable to support for your project(s) or organization?\r\nFor new team members especially, it can be quite confusing to display and advertise code snippets which are not applicable for the setup at hand since the mistake is easily assumed to lie on the user end. Independently of that, it seems undesirable to suggest approaches which are not applicable or simply 'don't work' due to particularities of the use case.\r\n- Why is it currently difficult to achieve this use case? (please be as specific as possible about why related MLflow features and components are insufficient)\r\nMy understanding is that these code snippets are more or less hardcoded in javascript on the server side and not able to be defined via the python interface on the client side. See https:\/\/github.com\/mlflow\/mlflow\/blob\/master\/mlflow\/server\/js\/src\/experiment-tracking\/components\/artifact-view-components\/ShowArtifactLoggedModelView.js#L90-L121\r\n\r\n### What component(s), interfaces, languages, and integrations does this feature affect?\r\nComponents \r\n- [X] `area\/artifacts`: Artifact stores and artifact logging\r\n- [ ] `area\/build`: Build and test infrastructure for MLflow\r\n- [ ] `area\/docs`: MLflow documentation pages\r\n- [ ] `area\/examples`: Example code\r\n- [ ] `area\/model-registry`: Model Registry service, APIs, and the fluent client calls for Model Registry\r\n- [ ] `area\/models`: MLmodel format, model serialization\/deserialization, flavors\r\n- [ ] `area\/projects`: MLproject format, project running backends\r\n- [ ] `area\/scoring`: MLflow Model server, model deployment tools, Spark UDFs\r\n- [ ] `area\/server-infra`: MLflow Tracking server backend\r\n- [ ] `area\/tracking`: Tracking Service, tracking client APIs, autologging\r\n\r\nInterfaces\r\n- [X] `area\/uiux`: Front-end, user experience, plotting, JavaScript, JavaScript dev server\r\n- [ ] `area\/docker`: Docker use across MLflow's components, such as MLflow Projects and MLflow Models\r\n- [ ] `area\/sqlalchemy`: Use of SQLAlchemy in the Tracking Service or Model Registry\r\n- [ ] `area\/windows`: Windows support\r\n\r\nLanguages \r\n- [ ] `language\/r`: R APIs and clients\r\n- [ ] `language\/java`: Java APIs and clients\r\n- [ ] `language\/new`: Proposals for new client languages\r\n\r\nIntegrations\r\n- [ ] `integrations\/azure`: Azure and Azure ML integrations\r\n- [ ] `integrations\/sagemaker`: SageMaker integrations\r\n- [ ] `integrations\/databricks`: Databricks integrations\r\n\r\n## Details\r\n\r\nThe section at hand can be seen in the screenshot below:\r\n![Screenshot 2022-01-07 095232](https:\/\/user-images.githubusercontent.com\/43778085\/148520178-8fd8c275-c2e6-460e-baf7-e0187148bc11.png)\r\n\r\n","pull_request":null,"labels":["enhancement","area\/uiux","area\/artifacts"],"labels_description":["New feature or request","Front-end, user experience, plotting, JavaScript, JavaScript dev server","Artifact stores and artifact logging"],"entities":["model","prediction","code","snippet"]},{"html_url":"https:\/\/github.com\/mlflow\/mlflow\/issues\/5231","id":1095943305,"number":5231,"title":"[fr] the use of gcs signed url for mlflow model service","state":"open","locked":false,"assignee":null,"assignees":[],"comments":0,"created_at":1641525564000,"updated_at":1641525636000,"closed_at":null,"body":"Thank you for submitting a feature request. **Before proceeding, please review MLflow's [Issue Policy for feature requests](https:\/\/www.github.com\/mlflow\/mlflow\/blob\/master\/ISSUE_POLICY.md#feature-requests) and the [MLflow Contributing Guide](https:\/\/github.com\/mlflow\/mlflow\/blob\/master\/CONTRIBUTING.rst)**.\r\n\r\n**Please fill in this feature request template to ensure a timely and thorough response.**\r\n\r\n## Willingness to contribute\r\nThe MLflow Community encourages new feature contributions. Would you or another member of your organization be willing to contribute an implementation of this feature (either as an MLflow Plugin or an enhancement to the MLflow code base)?\r\n\r\n- [ ] Yes. I can contribute this feature independently.\r\n- [X] Yes. I would be willing to contribute this feature with guidance from the MLflow community.\r\n- [ ] No. I cannot contribute this feature at this time.\r\n\r\n## Proposal Summary\r\n\r\nto have a way to get the model file from GCS by using signed url\r\n\r\n## Motivation\r\n- What is the use case for this feature? please refer to the detail section below\r\n- Why is this use case valuable to support for MLflow users in general? please refer to the detail section below\r\n- Why is this use case valuable to support for your project(s) or organization? please refer to the detail section below \r\n- Why is it currently difficult to achieve this use case? please refer to the detail section below \r\n\r\n### What component(s), interfaces, languages, and integrations does this feature affect?\r\nComponents \r\n- [X] `area\/artifacts`: Artifact stores and artifact logging\r\n- [ ] `area\/build`: Build and test infrastructure for MLflow\r\n- [ ] `area\/docs`: MLflow documentation pages\r\n- [ ] `area\/examples`: Example code\r\n- [X] `area\/model-registry`: Model Registry service, APIs, and the fluent client calls for Model Registry\r\n- [X] `area\/models`: MLmodel format, model serialization\/deserialization, flavors\r\n- [ ] `area\/projects`: MLproject format, project running backends\r\n- [ ] `area\/scoring`: MLflow Model server, model deployment tools, Spark UDFs\r\n- [ ] `area\/server-infra`: MLflow Tracking server backend\r\n- [ ] `area\/tracking`: Tracking Service, tracking client APIs, autologging\r\n\r\nInterfaces\r\n- [ ] `area\/uiux`: Front-end, user experience, plotting, JavaScript, JavaScript dev server\r\n- [ ] `area\/docker`: Docker use across MLflow's components, such as MLflow Projects and MLflow Models\r\n- [ ] `area\/sqlalchemy`: Use of SQLAlchemy in the Tracking Service or Model Registry\r\n- [ ] `area\/windows`: Windows support\r\n\r\nLanguages \r\n- [ ] `language\/r`: R APIs and clients\r\n- [ ] `language\/java`: Java APIs and clients\r\n- [ ] `language\/new`: Proposals for new client languages\r\n\r\nIntegrations\r\n- [ ] `integrations\/azure`: Azure and Azure ML integrations\r\n- [ ] `integrations\/sagemaker`: SageMaker integrations\r\n- [ ] `integrations\/databricks`: Databricks integrations\r\n\r\n## Details\r\nlet's take a look at the current deployment view of the model development flow with MLFlow.\r\n![mlflow-deployment](https:\/\/user-images.githubusercontent.com\/32842826\/148482528-af6b0a35-1769-4532-adfb-08a8bda369c6.png)\r\nthe picture above shows the high-level of system deployment view while ignoring several detailed components such as relational databases. Here are several important points which are related to this:\r\n- In this context, each model service components are a completely different service. They aren't replicas.\r\n- The VPC in yellow are the user artefacts, while the purple ones are managed by the our team.\r\n- MLFlow service is exposed through a public API that is only accessible by other internal services. We use openresty as the proxy.\r\n- Google Cloud Storage (GCS) doesn't lie inside the VPC. It's accessible through the internet. But, it's protected by a perimeter.\r\n\r\nNext, let's zoom into the main use case of this system, which is to fetch models from the MLFlow service.\r\n![mlflow-deployment (2)](https:\/\/user-images.githubusercontent.com\/32842826\/148485032-145eca4d-c719-4adf-ace9-2a3d210b1d5e.png)\r\nthe picture above shows the flow of the main use case of this system. The flow happens sequentially, but each request can be parallelized. Here is the detail of the flow:\r\n1.  MLFlow returns the GCS URL of the model, not the model itself.\r\n2.  Next, the model service requests the file to the GCS.\r\n3. GCS returns the requested files.\r\n\r\nyou can see the problem lies when the users need to get the model file directly to GCS. they can't do that due to the perimeter.\r\n","pull_request":null,"labels":["enhancement","area\/artifacts","area\/model-registry","area\/models"],"labels_description":["New feature or request","Artifact stores and artifact logging","Model registry, model registry APIs, and the fluent client calls for model registry","MLmodel format, model serialization\/deserialization, flavors"],"entities":["use","url","mlflow","model","service"]},{"html_url":"https:\/\/github.com\/mlflow\/mlflow\/issues\/5227","id":1095155733,"number":5227,"title":"something went wrong [bug]","state":"open","locked":false,"assignee":null,"assignees":[],"comments":3,"created_at":1641463340000,"updated_at":1642529089000,"closed_at":null,"body":"I am trying to start the mlflow ui in the notebook (using !mlfloe ui) but the server page of mlflow  is :\r\n\r\nSomething went wrong\r\nIf this error persists, please report an issue here.\r\n\r\nwhet seems to be the problem? \r\n\r\nthanks for you help:)\r\n![image](https:\/\/user-images.githubusercontent.com\/96474496\/148365262-a9424df7-9a11-4d20-8784-f53522e0c9d9.png)\r\n","pull_request":null,"labels":["bug"],"labels_description":["Something isn't working"],"entities":["something"]},{"html_url":"https:\/\/github.com\/mlflow\/mlflow\/issues\/5224","id":1094903000,"number":5224,"title":"[bug] empty experiment name is allowed","state":"open","locked":false,"assignee":null,"assignees":[],"comments":0,"created_at":1641435557000,"updated_at":1641435576000,"closed_at":null,"body":"### Willingness to contribute\r\nThe MLflow Community encourages bug fix contributions. Would you or another member of your organization be willing to contribute a fix for this bug to the MLflow code base?\r\n\r\n- [ ] Yes. I can contribute a fix for this bug independently.\r\n- [X] Yes. I would be willing to contribute a fix for this bug with guidance from the MLflow community.\r\n- [ ] No. I cannot contribute a bug fix at this time.\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in MLflow)**: No \r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Mac OSX 12.1\r\n- **MLflow installed from (source or binary)**: Source\r\n- **MLflow version (run ``mlflow --version``)**: 1.22.1.dev0\r\n- **Python version**: 3.7.10\r\n- **npm version, if running the dev UI**:\r\n- **Exact command to reproduce**: `mlflow.create_experiment(\" \")`\r\n\r\n### Describe the problem\r\n\r\nIn a conversation resolving a documentation issue [here](https:\/\/github.com\/mlflow\/mlflow\/issues\/4199#issuecomment-1006221093), we realized that we can create an experiment name with an empty string (ie. `\" \"`), and this was indicated to be a [bug](https:\/\/github.com\/mlflow\/mlflow\/issues\/4199#issuecomment-1006221093). \r\n\r\n### Code to reproduce issue\r\n\r\n```\r\nPython 3.7.10 (default, Feb 26 2021, 10:16:00)\r\n[Clang 10.0.0 ] :: Anaconda, Inc. on darwin\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import mlflow\r\n>>> mlflow.__version__\r\n'1.22.1.dev0'\r\n>>> experiment_id = mlflow.create_experiment(\" \")\r\n>>> experiment_id\r\n'1'\r\n>>> experiment = mlflow.get_experiment(experiment_id)\r\n>>> experiment.name\r\n' '\r\n>>> experiment.experiment_id\r\n'1'\r\n>>> experiment.lifecycle_stage\r\n'active'\r\n```\r\n\r\n### What component(s), interfaces, languages, and integrations does this bug affect?\r\nComponents \r\n- [ ] `area\/artifacts`: Artifact stores and artifact logging\r\n- [ ] `area\/build`: Build and test infrastructure for MLflow\r\n- [ ] `area\/docs`: MLflow documentation pages\r\n- [ ] `area\/examples`: Example code\r\n- [X] `area\/model-registry`: Model Registry service, APIs, and the fluent client calls for Model Registry\r\n- [ ] `area\/models`: MLmodel format, model serialization\/deserialization, flavors\r\n- [ ] `area\/projects`: MLproject format, project running backends\r\n- [ ] `area\/scoring`: MLflow Model server, model deployment tools, Spark UDFs\r\n- [ ] `area\/server-infra`: MLflow Tracking server backend\r\n- [ ] `area\/tracking`: Tracking Service, tracking client APIs, autologging\r\n\r\nInterface \r\n- [ ] `area\/uiux`: Front-end, user experience, plotting, JavaScript, JavaScript dev server\r\n- [ ] `area\/docker`: Docker use across MLflow's components, such as MLflow Projects and MLflow Models\r\n- [ ] `area\/sqlalchemy`: Use of SQLAlchemy in the Tracking Service or Model Registry\r\n- [ ] `area\/windows`: Windows support\r\n\r\nLanguage \r\n- [ ] `language\/r`: R APIs and clients\r\n- [ ] `language\/java`: Java APIs and clients\r\n- [ ] `language\/new`: Proposals for new client languages\r\n\r\nIntegrations\r\n- [ ] `integrations\/azure`: Azure and Azure ML integrations\r\n- [ ] `integrations\/sagemaker`: SageMaker integrations\r\n- [ ] `integrations\/databricks`: Databricks integrations\r\n","pull_request":null,"labels":["bug","area\/model-registry"],"labels_description":["Something isn't working","Model registry, model registry APIs, and the fluent client calls for model registry"],"entities":["experiment","name"]},{"html_url":"https:\/\/github.com\/mlflow\/mlflow\/issues\/5223","id":1094769623,"number":5223,"title":"[fr] lightning ecosystem ci","state":"open","locked":false,"assignee":null,"assignees":[],"comments":0,"created_at":1641419160000,"updated_at":1641419160000,"closed_at":null,"body":"Hello and Happy New Year 2022! :tada: \r\nJust wondering if you already heard about quite the new **Pytorch Lightning (PL) ecosystem CI** where we would like to invite you to... You can check out our blog post about it: [Stay Ahead of Breaking Changes with the New Lightning Ecosystem CI](https:\/\/devblog.pytorchlightning.ai\/stay-ahead-of-breaking-changes-with-the-new-lightning-ecosystem-ci-b7e1cf78a6c7) :zap: \r\nWe already have joined integration between PL and your logger, so we would like to level it up. At this moment, we are running tests with your latest version, but it may accidentally happen that your next version will be incompatible with our next release version... :confused:  But here we have a solution - ecosystem CI with testing both - your and our latest development head we can find it very early and prevent releasing eventually bad version... :+1: \r\n\r\n### What is needed to do?\r\n- have some tests, including PL integration\r\n- add config to ecosystem CI - https:\/\/github.com\/PyTorchLightning\/ecosystem-ci\r\n\r\n### What will you get?\r\n- scheduled nightly testing configured development\/stable heads\r\n- slack notification if something went wrong\r\n- testing also on multi-GPU machine","pull_request":null,"labels":["enhancement"],"labels_description":["New feature or request"],"entities":["lightning","ecosystem","ci"]},{"html_url":"https:\/\/github.com\/mlflow\/mlflow\/issues\/5218","id":1094246783,"number":5218,"title":"[fr] adding versioning of mlproject file specification","state":"open","locked":false,"assignee":null,"assignees":[],"comments":0,"created_at":1641381667000,"updated_at":1641381684000,"closed_at":null,"body":"Thank you for submitting a feature request. **Before proceeding, please review MLflow's [Issue Policy for feature requests](https:\/\/www.github.com\/mlflow\/mlflow\/blob\/master\/ISSUE_POLICY.md#feature-requests) and the [MLflow Contributing Guide](https:\/\/github.com\/mlflow\/mlflow\/blob\/master\/CONTRIBUTING.rst)**.\r\n\r\n**Please fill in this feature request template to ensure a timely and thorough response.**\r\n\r\n## Willingness to contribute\r\nThe MLflow Community encourages new feature contributions. Would you or another member of your organization be willing to contribute an implementation of this feature (either as an MLflow Plugin or an enhancement to the MLflow code base)?\r\n\r\n- [ ] Yes. I can contribute this feature independently.\r\n- [x] Yes. I would be willing to contribute this feature with guidance from the MLflow community.\r\n- [ ] No. I cannot contribute this feature at this time.\r\n\r\n## Proposal Summary\r\nAdd versioning of MLroject file specification and add support to mlflow working with different versions of MLproject file. \r\n\r\n## Motivation\r\nToday mlflow uses single file format. In this approach, it is necessary to maintain backward compatibility when adding a new feature. For example, there is feature request #4246 (adding poetry to create environment). To implement it we need to add another if condition to [here](https:\/\/github.com\/mlflow\/mlflow\/blob\/master\/mlflow\/projects\/_project_spec.py#L38)\r\n```python\r\n    docker_env = yaml_obj.get(\"docker_env\")\r\n    if docker_env:\r\n   ...\r\n    # Validate config if conda_env parameter is present\r\n    conda_path = yaml_obj.get(\"conda_env\")\r\n    if conda_path and docker_env:\r\n        raise ExecutionException(\"Project cannot contain both a docker and \" \"conda environment.\")\r\n```\r\nThat approach will look not great and in my opinion it better to add environment field to MLproject file, something like this\r\n```yaml\r\nenv:\r\n  type: docker\r\n  image: python:3.7\r\n```\r\nor \r\n```yaml\r\nenv:\r\n  type: conda\r\n  file: conda.yaml\r\n```\r\nThis type of change will break backaward compability. To get good user experience of using projects explicit information about used version of the file should be given.\r\n\r\nThus, proposal will next: add version field to MLproject file and add mlflow support of using different version of MLproject file specification. For example, like it uses in [docker compose file](https:\/\/docs.docker.com\/compose\/compose-file\/) and [kubernetes apiVersion field](https:\/\/kubernetes.io\/docs\/concepts\/overview\/working-with-objects\/kubernetes-objects\/).  \r\nTo save backaward campability of current version it posible to maintain current functionality without  adding version field in file. To use new functionality user have to add version field to MLproject file.\r\n\r\nFiles in package projects will be affected (I can add more details if needed).  \r\n\r\nNext pros and cons \r\nPros:\r\n- possibility to break backward compatibility and add new features with less effort\r\n- explicit information which functionality is available for installed version of mlflow\r\n\r\nCons:\r\n- user should know version of MLproject file specification (using of major+minor versioning with compatibility for major version can help to reduce this problem).\r\n\r\n\r\n### What component(s), interfaces, languages, and integrations does this feature affect?\r\nComponents \r\n- [ ] `area\/artifacts`: Artifact stores and artifact logging\r\n- [ ] `area\/build`: Build and test infrastructure for MLflow\r\n- [ ] `area\/docs`: MLflow documentation pages\r\n- [ ] `area\/examples`: Example code\r\n- [ ] `area\/model-registry`: Model Registry service, APIs, and the fluent client calls for Model Registry\r\n- [ ] `area\/models`: MLmodel format, model serialization\/deserialization, flavors\r\n- [x] `area\/projects`: MLproject format, project running backends\r\n- [ ] `area\/scoring`: MLflow Model server, model deployment tools, Spark UDFs\r\n- [ ] `area\/server-infra`: MLflow Tracking server backend\r\n- [ ] `area\/tracking`: Tracking Service, tracking client APIs, autologging\r\n\r\nInterfaces\r\n- [ ] `area\/uiux`: Front-end, user experience, plotting, JavaScript, JavaScript dev server\r\n- [ ] `area\/docker`: Docker use across MLflow's components, such as MLflow Projects and MLflow Models\r\n- [ ] `area\/sqlalchemy`: Use of SQLAlchemy in the Tracking Service or Model Registry\r\n- [ ] `area\/windows`: Windows support\r\n\r\nLanguages \r\n- [ ] `language\/r`: R APIs and clients\r\n- [ ] `language\/java`: Java APIs and clients\r\n- [ ] `language\/new`: Proposals for new client languages\r\n\r\nIntegrations\r\n- [ ] `integrations\/azure`: Azure and Azure ML integrations\r\n- [ ] `integrations\/sagemaker`: SageMaker integrations\r\n- [ ] `integrations\/databricks`: Databricks integrations\r\n\r\n## Details\r\n\r\n(Use this section to include any additional information about the feature. If you have a proposal for how to implement this feature, please include it here. For implementation guidelines, please refer to the [Contributing Guide](https:\/\/github.com\/mlflow\/mlflow\/blob\/master\/CONTRIBUTING.rst#contribution-guidelines).)\r\n","pull_request":null,"labels":["enhancement","area\/projects"],"labels_description":["New feature or request","MLproject format, project running backends"],"entities":["versioning","mlproject","file","specification"]},{"html_url":"https:\/\/github.com\/mlflow\/mlflow\/issues\/5214","id":1092581035,"number":5214,"title":"osx rscript not found","state":"open","locked":false,"assignee":null,"assignees":[],"comments":0,"created_at":1641222028000,"updated_at":1641222028000,"closed_at":null,"body":"Hi, \r\n\r\nI'm working on OSX 11.6.1 with RStudio 2021.09.0 and R 4.1.2 installed through Homebew 3.3.9. I used the quickstart instructions to install mlflow and it seemed to work, as far as the single training and the UI go. But when I try to run \r\n\r\n`> mlflow_run(uri = \".\/\", entry_point = \"train.R\")`\r\n\r\nI get the following error:\r\n\r\n> 2022\/01\/03 15:49:15 INFO mlflow.projects.utils: === Created directory \/var\/folders\/lk\/yxcpm09j299_b_kcp0v7pcv00000gn\/T\/tmpfvs7pcgm for downloading remote URIs passed to arguments of type 'path' ===\r\n2022\/01\/03 15:49:15 INFO mlflow.projects.backend.local: === Running command 'source \/Users\/mitch\/Library\/r-miniconda\/bin\/..\/etc\/profile.d\/conda.sh && conda activate mlflow-da39a3ee5e6b4b0d3255bfef95601890afd80709 1>&2 && Rscript -e \"mlflow::mlflow_source('train.R')\" --args' in run with ID '51e04222838f46fcb38343afb7a34d8c' === \r\nbash: Rscript: command not found\r\n2022\/01\/03 15:49:16 ERROR mlflow.cli: === Run (ID '51e04222838f46fcb38343afb7a34d8c') failed ===\r\nError in run(mlflow_bin, args = unlist(args), echo = echo, echo_cmd = verbose,  : \r\n  System command 'mlflow' failed, exit status: 1, stdout & stderr were printed \r\n\r\nNote that it works fine when I try to run the following in the shell:\r\n\r\n`source \/Users\/mitch\/Library\/r-miniconda\/bin\/..\/etc\/profile.d\/conda.sh `\r\n`conda activate mlflow-da39a3ee5e6b4b0d3255bfef95601890afd80709`\r\n`Rscript -e \"mlflow::mlflow_source('train.R')\" --args`\r\n\r\nIt just seems that the CLI does not get the path to Rscript (which is in its standard location \/opt\/homebrew\/bin\/Rscript). \r\n\r\nThanks for your help!\r\nMichel\r\n\r\n\r\n\r\n\r\n","pull_request":null,"labels":[],"labels_description":[],"entities":[]},{"html_url":"https:\/\/github.com\/mlflow\/mlflow\/issues\/5213","id":1092404613,"number":5213,"title":"issue with creating experiments","state":"open","locked":false,"assignee":null,"assignees":[],"comments":0,"created_at":1641206892000,"updated_at":1641206892000,"closed_at":null,"body":"I have created my Own API for Creating Experiments, it always requesting Get method instead of Post. where do i change this to Post Method? it's bit urgent someone help me to solve this\r\n","pull_request":null,"labels":["bug"],"labels_description":["Something isn't working"],"entities":["issue"]},{"html_url":"https:\/\/github.com\/mlflow\/mlflow\/issues\/5209","id":1091085188,"number":5209,"title":"[fr] support an option to use sqlalchemy.pool.nullpool to avoid overwhelming db in parallel runs","state":"open","locked":false,"assignee":null,"assignees":[],"comments":0,"created_at":1640868524000,"updated_at":1640868540000,"closed_at":null,"body":"## Willingness to contribute\r\n\r\n- [ ] Yes. I can contribute this feature independently.\r\n- [x] Yes. I would be willing to contribute this feature with guidance from the MLflow community.\r\n- [ ] No. I cannot contribute this feature at this time.\r\n\r\n## Proposal Summary\r\n\r\nThe requested feature will allow users of mlflow (directly or e.g. through [optuna](https:\/\/github.com\/optuna\/optuna\/tree\/master\/optuna)) to avoid using connection pools when relying on RDBMS. In this case, a connection is made for each transaction then closed afterwards.\r\n\r\n## Motivation\r\n\r\nTo control the connections to the database when the program is run by parallel processes (or nodes). Using `pool_size=1` and `max_overflow=1` can efficiently serve this purpose if the script is run by a single process, but doesn't prevent overwhelming the database server when the program is run by parallel processes.\r\n\r\nThe motivation arose when I deployed my program of model training and evaluation for hyperparameters search within an HPC environment. The program instances were aborted due to SQLAlchemy exception indicating that maximum connections to the database has been reached. I also received a report from the server admin conveying that this behavior has affected other users of the database server and limited their access.\r\n\r\n### What component(s), interfaces, languages, and integrations does this feature affect?\r\nComponents \r\n- [ ] `area\/artifacts`: Artifact stores and artifact logging\r\n- [ ] `area\/build`: Build and test infrastructure for MLflow\r\n- [x] `area\/docs`: MLflow documentation pages\r\n- [ ] `area\/examples`: Example code\r\n- [ ] `area\/model-registry`: Model Registry service, APIs, and the fluent client calls for Model Registry\r\n- [ ] `area\/models`: MLmodel format, model serialization\/deserialization, flavors\r\n- [ ] `area\/projects`: MLproject format, project running backends\r\n- [ ] `area\/scoring`: MLflow Model server, model deployment tools, Spark UDFs\r\n- [ ] `area\/server-infra`: MLflow Tracking server backend\r\n- [ ] `area\/tracking`: Tracking Service, tracking client APIs, autologging\r\n\r\nInterfaces\r\n- [ ] `area\/uiux`: Front-end, user experience, plotting, JavaScript, JavaScript dev server\r\n- [ ] `area\/docker`: Docker use across MLflow's components, such as MLflow Projects and MLflow Models\r\n- [x] `area\/sqlalchemy`: Use of SQLAlchemy in the Tracking Service or Model Registry\r\n- [ ] `area\/windows`: Windows support\r\n\r\nLanguages \r\n- [ ] `language\/r`: R APIs and clients\r\n- [ ] `language\/java`: Java APIs and clients\r\n- [ ] `language\/new`: Proposals for new client languages\r\n\r\nIntegrations\r\n- [ ] `integrations\/azure`: Azure and Azure ML integrations\r\n- [ ] `integrations\/sagemaker`: SageMaker integrations\r\n- [ ] `integrations\/databricks`: Databricks integrations\r\n\r\n## Details\r\n\r\nBy consulting the documentation of [sqlalchemy.create_engine](https:\/\/docs.sqlalchemy.org\/en\/14\/core\/engines.html#sqlalchemy.create_engine.params.poolclass), this turns out to be solved if we set `poolclass` to `pool.NullPool`.\r\n\r\nHere is a proposed solution for this request:\r\n\r\n[A-Alaa\/mlflow\/tree\/null_pool](https:\/\/github.com\/A-Alaa\/mlflow\/tree\/null_pool)\r\n","pull_request":null,"labels":["enhancement","area\/docs","area\/sqlalchemy"],"labels_description":["New feature or request","Documentation issues","Use of SQL alchemy in tracking service or model registry"],"entities":["option","sqlalchemy.pool.nullpool","db"]},{"html_url":"https:\/\/github.com\/mlflow\/mlflow\/issues\/5208","id":1090735322,"number":5208,"title":"[fr] validate request json with json schema or similar","state":"open","locked":false,"assignee":null,"assignees":[],"comments":6,"created_at":1640809771000,"updated_at":1644296105000,"closed_at":null,"body":"Thank you for submitting a feature request. **Before proceeding, please review MLflow's [Issue Policy for feature requests](https:\/\/www.github.com\/mlflow\/mlflow\/blob\/master\/ISSUE_POLICY.md#feature-requests) and the [MLflow Contributing Guide](https:\/\/github.com\/mlflow\/mlflow\/blob\/master\/CONTRIBUTING.rst)**.\r\n\r\n**Please fill in this feature request template to ensure a timely and thorough response.**\r\n\r\n## Willingness to contribute\r\nThe MLflow Community encourages new feature contributions. Would you or another member of your organization be willing to contribute an implementation of this feature (either as an MLflow Plugin or an enhancement to the MLflow code base)?\r\n\r\n- [ ] Yes. I can contribute this feature independently.\r\n- [x] Yes. I would be willing to contribute this feature with guidance from the MLflow community.\r\n- [ ] No. I cannot contribute this feature at this time.\r\n\r\n## Proposal Summary\r\n\r\nAPI calls should return `HTTP 400` when the parameters (e.g.) don't match expected data types instead of failing with a `500`. Creating a JSON schema -- using [jsonschema](https:\/\/github.com\/Julian\/jsonschema), for example -- for the MLFlow REST API to check requests against would fix these issues. This would result in far friendlier UX, easier debugging, more predictable responses, and a generally more RESTful API.\r\n\r\n## Motivation\r\n\r\nI keep getting `500` errors for things like supplying a parameter to an API call that's the wrong data type. See [this issue](https:\/\/github.com\/mlflow\/mlflow\/issues\/5199) for an example. This has also happened with calls to logging parameters (both individually and in batches) and all kinds of other functions.\r\n\r\nRight now, this means that an end user of a running MLFlow service get an error message like this back when something goes wrong:\r\n\r\n```\r\nResponse [https:\/\/<<host>>\/api\/2.0\/mlflow\/runs\/log-batch]\r\n  Date: 2021-12-29 20:15\r\n  Status: 500\r\n  Content-Type: text\/html; charset=utf-8\r\n  Size: 290 B\r\n<!DOCTYPE HTML PUBLIC \"-\/\/W3C\/\/DTD HTML 3.2 Final\/\/EN\">\r\n<title>500 Internal Server Error<\/title>\r\n<h1>Internal Server Error<\/h1>\r\n<p>The server encountered an internal error and was unable to complete your request. Either the server is overloaded or there is an error in the application.<\/p>\r\n```\r\n\r\nThis error was caused by providing a `timestamp` value to `log-batch` that was a character string as opposed to a numeric timestamp.\r\n\r\nObviously, this error is unhelpful. There's no indication of what went wrong or how to fix the issue. More importantly, the `500` is a hint that the client actually _did not do anything wrong_, and that there was a legitimate issue on the server side. For bad parameters (e.g.), this is obviously not the case, and the client should be seeing an error message with information about the incorrect parameter type and what type was expected, not a cryptic `500` with `Unable to complete your request`.\r\n\r\nThe value prop here should be relatively obvious, so I won't write too much beyond just saying that validating requests against a JSON schema would let users of the MLFlow REST API (in other words, every MLFlow user) more easily and reliably use MLFlow, develop wrappers for the MLFlow API, debug their code when things go wrong, etc. etc. etc.\r\n\r\n### What component(s), interfaces, languages, and integrations does this feature affect?\r\nComponents \r\n- [ ] `area\/artifacts`: Artifact stores and artifact logging\r\n- [ ] `area\/build`: Build and test infrastructure for MLflow\r\n- [ ] `area\/docs`: MLflow documentation pages\r\n- [ ] `area\/examples`: Example code\r\n- [ ] `area\/model-registry`: Model Registry service, APIs, and the fluent client calls for Model Registry\r\n- [ ] `area\/models`: MLmodel format, model serialization\/deserialization, flavors\r\n- [ ] `area\/projects`: MLproject format, project running backends\r\n- [ ] `area\/scoring`: MLflow Model server, model deployment tools, Spark UDFs\r\n- [x] `area\/server-infra`: MLflow Tracking server backend\r\n- [x] `area\/tracking`: Tracking Service, tracking client APIs, autologging\r\n\r\nInterfaces\r\n- [ ] `area\/uiux`: Front-end, user experience, plotting, JavaScript, JavaScript dev server\r\n- [ ] `area\/docker`: Docker use across MLflow's components, such as MLflow Projects and MLflow Models\r\n- [ ] `area\/sqlalchemy`: Use of SQLAlchemy in the Tracking Service or Model Registry\r\n- [ ] `area\/windows`: Windows support\r\n\r\nLanguages \r\n- [ ] `language\/r`: R APIs and clients\r\n- [ ] `language\/java`: Java APIs and clients\r\n- [ ] `language\/new`: Proposals for new client languages\r\n\r\nIntegrations\r\n- [ ] `integrations\/azure`: Azure and Azure ML integrations\r\n- [ ] `integrations\/sagemaker`: SageMaker integrations\r\n- [ ] `integrations\/databricks`: Databricks integrations\r\n\r\n## Details\r\n\r\nI haven't written any JSON schema for Python, but in R I know it's easy to just set up a function to validate requests and then use that function to validate the JSON body of any requests that come in before doing any actual work. If a request fails the JSON validation checks, you can easily return an `HTTP 400 -- JSON validation failed with << some error >>`.\r\n\r\nLet me know if I can help with this improvement! I think it'd be a major step forward for everyone using MLFlow and for the project in general.\r\n","pull_request":null,"labels":["enhancement","area\/server-infra"],"labels_description":["New feature or request","MLflow Tracking server backend"],"entities":["request","json","json","schema"]},{"html_url":"https:\/\/github.com\/mlflow\/mlflow\/issues\/5207","id":1090476488,"number":5207,"title":"[fr] support for nullable numeric types for models with pandas-based signature","state":"open","locked":false,"assignee":null,"assignees":[],"comments":0,"created_at":1640779657000,"updated_at":1640779707000,"closed_at":null,"body":"**Please fill in this feature request template to ensure a timely and thorough response.**\r\n\r\n## Willingness to contribute\r\nThe MLflow Community encourages new feature contributions. Would you or another member of your organization be willing to contribute an implementation of this feature (either as an MLflow Plugin or an enhancement to the MLflow code base)?\r\n\r\n- [ X ] No. I cannot contribute this feature at this time.\r\n\r\n## Proposal Summary\r\n\r\nI am unsure if it is a feature request or a bug:\r\nA mlflow model loaded from the registry using `mlflow.pyfunc` that uses a model_signature based on pandas DataFrames does not accept None values for numeric data types (it works for strings). As pandas is supporting nullable numeric types these days, it would be very helpful if mlflow could also support these.\r\n\r\n### What component(s), interfaces, languages, and integrations does this feature affect?\r\nComponents \r\n- [ ] `area\/artifacts`: Artifact stores and artifact logging\r\n- [ ] `area\/build`: Build and test infrastructure for MLflow\r\n- [ ] `area\/docs`: MLflow documentation pages\r\n- [ ] `area\/examples`: Example code\r\n- [x] `area\/model-registry`: Model Registry service, APIs, and the fluent client calls for Model Registry\r\n- [x] `area\/models`: MLmodel format, model serialization\/deserialization, flavors\r\n- [ ] `area\/projects`: MLproject format, project running backends\r\n- [ ] `area\/scoring`: MLflow Model server, model deployment tools, Spark UDFs\r\n- [ ] `area\/server-infra`: MLflow Tracking server backend\r\n- [ ] `area\/tracking`: Tracking Service, tracking client APIs, autologging\r\n\r\n## code to show that this currently does not work:\r\n\r\n```\r\nimport mlflow\r\nmlflow.autolog(\r\n    log_model_signatures=True,\r\n    log_models=True\r\n)\r\n\r\nimport numpy as np\r\nfrom sklearn.ensemble import RandomForestClassifier\r\nfrom sklearn.impute import SimpleImputer\r\nfrom sklearn.pipeline import Pipeline \r\nimport pandas as pd\r\n\r\nX_train = pd.DataFrame([[0, 0, np.nan], [np.nan, 1, 1], [np.nan, 0, 0]], columns=[\"c1\", \"c2\", \"c3\"])\r\nY_train = pd.DataFrame([0, 1, 0], columns=[\"label\"])\r\nX_test = pd.DataFrame([[0, 0, np.nan], [0, np.nan, np.nan], [0, 1,0]], columns=[\"c1\", \"c2\", \"c3\"])\r\n\r\n# Create our imputer to replace missing values with the mean e.g.\r\nimp = SimpleImputer(missing_values=np.nan, strategy='mean')\r\nclf = RandomForestClassifier(n_estimators=10)\r\n\r\npipe = Pipeline(steps=[(\"prepare\",imp), (\"clr\",clf)])\r\npipe.fit(X_train, Y_train)\r\n\r\npipe.predict(X_test)\r\n# outputs: array([0, 0, 0])\r\n```\r\n\r\nThen go to the registry, register the model of the run and adapt the load_function below accordingly\r\n\r\n```\r\nfrom mlflow.pyfunc import load_model\r\nm = load_model(f\"models:\/nan_problem\/1\")\r\nprint(m.metadata.signature)\r\n# inputs: \r\n#   ['c1': double, 'c2': long, 'c3': double]\r\n# outputs: \r\n#   [Tensor('int64', (-1,))]\r\n\r\nm.predict(X_test)\r\n# >> MlflowException: Incompatible input types for column c1. Can not safely convert int64 to float64.\r\n\r\n```\r\n\r\n## Stacktrace:\r\n```\r\n\r\n---------------------------------------------------------------------------\r\nMlflowException                           Traceback (most recent call last)\r\n\/var\/folders\/ym\/_50l9__942705dh4x6gh9d9r0000gn\/T\/ipykernel_85093\/2409685277.py in <module>\r\n      2 m = load_model(f\"models:\/nan_problem\/3\")\r\n      3 print(m.metadata.signature)\r\n----> 4 m.predict(X_test)\r\n\r\n~\/.pyenv\/versions\/3.8.7\/lib\/python3.8\/site-packages\/mlflow\/pyfunc\/__init__.py in predict(self, data)\r\n    605         input_schema = self.metadata.get_input_schema()\r\n    606         if input_schema is not None:\r\n--> 607             data = _enforce_schema(data, input_schema)\r\n    608         return self._model_impl.predict(data)\r\n    609 \r\n\r\n~\/.pyenv\/versions\/3.8.7\/lib\/python3.8\/site-packages\/mlflow\/pyfunc\/__init__.py in _enforce_schema(pfInput, input_schema)\r\n    563         _enforce_tensor_schema(pfInput, input_schema)\r\n    564         if input_schema.is_tensor_spec()\r\n--> 565         else _enforce_col_schema(pfInput, input_schema)\r\n    566     )\r\n    567 \r\n\r\n~\/.pyenv\/versions\/3.8.7\/lib\/python3.8\/site-packages\/mlflow\/pyfunc\/__init__.py in _enforce_col_schema(pfInput, input_schema)\r\n    452     new_pfInput = pandas.DataFrame()\r\n    453     for i, x in enumerate(input_names):\r\n--> 454         new_pfInput[x] = _enforce_mlflow_datatype(x, pfInput[x], input_types[i])\r\n    455     return new_pfInput\r\n    456 \r\n\r\n~\/.pyenv\/versions\/3.8.7\/lib\/python3.8\/site-packages\/mlflow\/pyfunc\/__init__.py in _enforce_mlflow_datatype(name, values, t)\r\n    407             )\r\n    408 \r\n--> 409         raise MlflowException(\r\n    410             \"Incompatible input types for column {0}. \"\r\n    411             \"Can not safely convert {1} to {2}.{3}\".format(name, values.dtype, numpy_type, hint)\r\n\r\nMlflowException: Incompatible input types for column c1. Can not safely convert int64 to float64.\r\n```\r\n\r\n","pull_request":null,"labels":["enhancement","area\/model-registry","area\/models"],"labels_description":["New feature or request","Model registry, model registry APIs, and the fluent client calls for model registry","MLmodel format, model serialization\/deserialization, flavors"],"entities":["support","signature"]},{"html_url":"https:\/\/github.com\/mlflow\/mlflow\/issues\/5204","id":1089527168,"number":5204,"title":"[bug] changing the tracking's uri doesn't change the artifact's uri","state":"open","locked":false,"assignee":null,"assignees":[],"comments":4,"created_at":1640649395000,"updated_at":1647895876000,"closed_at":null,"body":"### Willingness to contribute\r\n- [ ] Yes. I can contribute a fix for this bug independently.\r\n- [x] Yes. I would be willing to contribute a fix for this bug with guidance from the MLflow community.\r\n- [ ] No. I cannot contribute a bug fix at this time.\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in MLflow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: docker `continuumio\/miniconda3` on Azure's Compute Instance\r\n- **MLflow installed from (source or binary)**: R's `install_mlflow()`\r\n- **MLflow version (run ``mlflow --version``)**: 1.22.0\r\n- **Python version**: 3.9\r\n\r\n### Describe the problem\r\nI am trying to use MLFlow on Azure's ML Compute Instances to track my experiments in R. The tracking URI set by the compute instance has the scheme `azureml:\/\/...` which is not supported by MLFlow. Azure has a [package](https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-mlflow\/?view=azure-ml-py) to enable support on Python, but as my code is in R, I found that I need to manually change the URI to `https:\/\/...` to make logging work.\r\n\r\nChanging the Tracking URI works correctly (i.e. I can log params and metrics) when setting the `MLFLOW_ARTIFACT_URI` environment variable or when calling `mlflow_set_tracking_uri()`, but it doesn't seem to have any effect on the artifact URI (related to #3469) as it is still using the old scheme (the one starting with `azureml:\/\/`), so I am not able to log artifacts.\r\n\r\nI see no method on [MLFlow's R API](https:\/\/www.mlflow.org\/docs\/latest\/R-api.html#mlflow-server) to change the artifact's URI scheme (and #2577 seems to be related, but I created a different issue as I don't want to re-define the whole URI, but just the scheme) and workarounds are possible at least while Microsoft implements an R API (I hope it happens someday), but I was wondering if MLFlow has any suggestion on this or if a new feature that could help this use-case is in the plan.\r\n\r\n### What component(s), interfaces, languages, and integrations does this bug affect?\r\nComponents \r\n- [x] `area\/artifacts`: Artifact stores and artifact logging\r\n- [ ] `area\/build`: Build and test infrastructure for MLflow\r\n- [ ] `area\/docs`: MLflow documentation pages\r\n- [ ] `area\/examples`: Example code\r\n- [ ] `area\/model-registry`: Model Registry service, APIs, and the fluent client calls for Model Registry\r\n- [ ] `area\/models`: MLmodel format, model serialization\/deserialization, flavors\r\n- [ ] `area\/projects`: MLproject format, project running backends\r\n- [ ] `area\/scoring`: MLflow Model server, model deployment tools, Spark UDFs\r\n- [ ] `area\/server-infra`: MLflow Tracking server backend\r\n- [ ] `area\/tracking`: Tracking Service, tracking client APIs, autologging\r\n\r\nLanguage \r\n- [x] `language\/r`: R APIs and clients\r\n- [ ] `language\/java`: Java APIs and clients\r\n- [ ] `language\/new`: Proposals for new client languages\r\n\r\nIntegrations\r\n- [x] `integrations\/azure`: Azure and Azure ML integrations\r\n- [ ] `integrations\/sagemaker`: SageMaker integrations\r\n- [ ] `integrations\/databricks`: Databricks integrations\r\n","pull_request":null,"labels":["bug","language\/r","area\/artifacts","integrations\/azure"],"labels_description":["Something isn't working","R APIs and clients","Artifact stores and artifact logging","Azure and Azure ML integrations"],"entities":["uri","uri"]},{"html_url":"https:\/\/github.com\/mlflow\/mlflow\/issues\/5203","id":1089282048,"number":5203,"title":"[fr] display of parameters in comparison plot.","state":"open","locked":false,"assignee":null,"assignees":[],"comments":0,"created_at":1640615678000,"updated_at":1640615678000,"closed_at":null,"body":"Thank you for submitting a feature request. **Before proceeding, please review MLflow's [Issue Policy for feature requests](https:\/\/www.github.com\/mlflow\/mlflow\/blob\/master\/ISSUE_POLICY.md#feature-requests) and the [MLflow Contributing Guide](https:\/\/github.com\/mlflow\/mlflow\/blob\/master\/CONTRIBUTING.rst)**.\r\n\r\n**Please fill in this feature request template to ensure a timely and thorough response.**\r\n\r\n## Willingness to contribute\r\nThe MLflow Community encourages new feature contributions. Would you or another member of your organization be willing to contribute an implementation of this feature (either as an MLflow Plugin or an enhancement to the MLflow code base)?\r\n\r\n- [ ] Yes. I can contribute this feature independently.\r\n- [ ] Yes. I would be willing to contribute this feature with guidance from the MLflow community.\r\n- [x ] No. I cannot contribute this feature at this time.\r\n\r\n## Proposal Summary\r\nIn the experiment comparison, \"compare data on hover\" shows the metric. I feel it might be more useful if the parameters can be shown. \r\n![newplot](https:\/\/user-images.githubusercontent.com\/46158005\/147480978-34621901-7d81-48e9-b909-71ab16c6dd62.png)\r\n\r\n## Motivation\r\n- What is the use case for this feature? Comparison of different experimental runs; when the experiments are grouped by parameters. It is currently to convenient (at least for me to track the parameters from graphs).\r\n- Why is this use case valuable to support for MLflow users in general? Pls. see above and the graphs.\r\n- Why is this use case valuable to support for your project(s) or organization? Pls see above and graphs.\r\n- Why is it currently difficult to achieve this use case? Currently, the comparison is not convenient when the experiment management is based on experiments. Eg, if the project name is \"CatsDogsClassification\", the experiment might involve trying out a couple of models. Those would be the experiment name: eg \"DenseNet121\", \"SENet154\".  There would be multiple runs of each of these networks. Now, if we want to compare key parameters, this is not possible unless we start encoding the parameter name in the experiment Run Name. This makes it unnaturally long. Therefore, along with the key metric, if there is some way of displaying the parameters; i think it will be useful.\r\n\r\n### What component(s), interfaces, languages, and integrations does this feature affect?\r\nComponents \r\n- [ ] `area\/artifacts`: Artifact stores and artifact logging\r\n- [ ] `area\/build`: Build and test infrastructure for MLflow\r\n- [ ] `area\/docs`: MLflow documentation pages\r\n- [ ] `area\/examples`: Example code\r\n- [ ] `area\/model-registry`: Model Registry service, APIs, and the fluent client calls for Model Registry\r\n- [ ] `area\/models`: MLmodel format, model serialization\/deserialization, flavors\r\n- [ ] `area\/projects`: MLproject format, project running backends\r\n- [ ] `area\/scoring`: MLflow Model server, model deployment tools, Spark UDFs\r\n- [ ] `area\/server-infra`: MLflow Tracking server backend\r\n- [ x] `area\/tracking`: Tracking Service, tracking client APIs, autologging\r\n\r\nInterfaces\r\n- [ x] `area\/uiux`: Front-end, user experience, plotting, JavaScript, JavaScript dev server\r\n- [ ] `area\/docker`: Docker use across MLflow's components, such as MLflow Projects and MLflow Models\r\n- [ ] `area\/sqlalchemy`: Use of SQLAlchemy in the Tracking Service or Model Registry\r\n- [ ] `area\/windows`: Windows support\r\n\r\nLanguages \r\n- [ ] `language\/r`: R APIs and clients\r\n- [ ] `language\/java`: Java APIs and clients\r\n- [ ] `language\/new`: Proposals for new client languages\r\n\r\nIntegrations\r\n- [ ] `integrations\/azure`: Azure and Azure ML integrations\r\n- [ ] `integrations\/sagemaker`: SageMaker integrations\r\n- [ ] `integrations\/databricks`: Databricks integrations\r\n\r\n## Details\r\n\r\n(Use this section to include any additional information about the feature. If you have a proposal for how to implement this feature, please include it here. For implementation guidelines, please refer to the [Contributing Guide](https:\/\/github.com\/mlflow\/mlflow\/blob\/master\/CONTRIBUTING.rst#contribution-guidelines).)\r\n","pull_request":null,"labels":["enhancement"],"labels_description":["New feature or request"],"entities":["display","comparison","plot."]},{"html_url":"https:\/\/github.com\/mlflow\/mlflow\/pull\/5201","id":1088770850,"number":5201,"title":"adding new example sklearn heart attack predictor","state":"open","locked":false,"assignee":null,"assignees":[],"comments":3,"created_at":1640524985000,"updated_at":1640582359000,"closed_at":null,"body":"## What changes are proposed in this pull request?\r\n\r\nAdding Scikit-learn Example to get the better understanding of the following :\r\n\r\n- How to log a pre-trained model (all the existing examples have added the training code with it)\r\n- How to add signature in your model\r\n- How to register your model through code\r\n- How to shift your model into production through code\r\n\r\n## How is this patch tested?\r\n\r\nThis code is tested locally using mlflow CLI\r\n\r\n## Does this PR change the documentation?\r\n\r\n- [x] No. You can skip the rest of this section.\r\n- [ ] Yes. Make sure the changed pages \/ sections render correctly by following the steps below.\r\n\r\n1. Check the status of the `ci\/circleci: build_doc` check. If it's successful, proceed to the\r\n   next step, otherwise fix it.\r\n2. Click `Details` on the right to open the job page of CircleCI.\r\n3. Click the `Artifacts` tab.\r\n4. Click `docs\/build\/html\/index.html`.\r\n5. Find the changed pages \/ sections and make sure they render correctly.\r\n\r\n## Release Notes\r\n\r\n### Is this a user-facing change?\r\n\r\n- [x] No. You can skip the rest of this section.\r\n- [ ] Yes. Give a description of this change to be included in the release notes for MLflow users.\r\n\r\n(Details in 1-2 sentences. You can just refer to another PR with a description if this PR is part of a larger change.)\r\n\r\n### What component(s), interfaces, languages, and integrations does this PR affect?\r\nComponents \r\n- [ ] `area\/artifacts`: Artifact stores and artifact logging\r\n- [ ] `area\/build`: Build and test infrastructure for MLflow\r\n- [ ] `area\/docs`: MLflow documentation pages\r\n- [x] `area\/examples`: Example code\r\n- [ ] `area\/model-registry`: Model Registry service, APIs, and the fluent client calls for Model Registry\r\n- [ ] `area\/models`: MLmodel format, model serialization\/deserialization, flavors\r\n- [ ] `area\/projects`: MLproject format, project running backends\r\n- [ ] `area\/scoring`: MLflow Model server, model deployment tools, Spark UDFs\r\n- [ ] `area\/server-infra`: MLflow Tracking server backend\r\n- [ ] `area\/tracking`: Tracking Service, tracking client APIs, autologging\r\n\r\nInterface \r\n- [ ] `area\/uiux`: Front-end, user experience, plotting, JavaScript, JavaScript dev server\r\n- [ ] `area\/docker`: Docker use across MLflow's components, such as MLflow Projects and MLflow Models\r\n- [ ] `area\/sqlalchemy`: Use of SQLAlchemy in the Tracking Service or Model Registry\r\n- [ ] `area\/windows`: Windows support\r\n\r\nLanguage \r\n- [ ] `language\/r`: R APIs and clients\r\n- [ ] `language\/java`: Java APIs and clients\r\n- [ ] `language\/new`: Proposals for new client languages\r\n\r\nIntegrations\r\n- [ ] `integrations\/azure`: Azure and Azure ML integrations\r\n- [ ] `integrations\/sagemaker`: SageMaker integrations\r\n- [ ] `integrations\/databricks`: Databricks integrations\r\n\r\n<!--\r\nInsert an empty named anchor here to allow jumping to this section with a fragment URL\r\n(e.g. https:\/\/github.com\/mlflow\/mlflow\/pull\/123#user-content-release-note-category).\r\nNote that GitHub prefixes anchor names in markdown with \"user-content-\".\r\n-->\r\n<a name=\"release-note-category\"><\/a>\r\n### How should the PR be classified in the release notes? Choose one:\r\n\r\n- [ ] `rn\/breaking-change` - The PR will be mentioned in the \"Breaking Changes\" section\r\n- [x] `rn\/none` - No description will be included. The PR will be mentioned only by the PR number in the \"Small Bugfixes and Documentation Updates\" section\r\n- [ ] `rn\/feature` - A new user-facing feature worth mentioning in the release notes\r\n- [ ] `rn\/bug-fix` - A user-facing bug fix worth mentioning in the release notes\r\n- [ ] `rn\/documentation` - A user-facing documentation change worth mentioning in the release notes\r\n","pull_request":{"url":"https:\/\/api.github.com\/repos\/mlflow\/mlflow\/pulls\/5201","html_url":"https:\/\/github.com\/mlflow\/mlflow\/pull\/5201","diff_url":"https:\/\/github.com\/mlflow\/mlflow\/pull\/5201.diff","patch_url":"https:\/\/github.com\/mlflow\/mlflow\/pull\/5201.patch","merged_at":null},"labels":["rn\/none","area\/examples"],"labels_description":["List under Small Changes in Changelogs.","Example code"],"entities":["example","sklearn","heart","attack","predictor"]},{"html_url":"https:\/\/github.com\/mlflow\/mlflow\/issues\/5199","id":1088547474,"number":5199,"title":"`mlflow_transition_model_version_stage` (r) fails with http `500` [bug]","state":"open","locked":false,"assignee":null,"assignees":[],"comments":2,"created_at":1640408655000,"updated_at":1640452051000,"closed_at":null,"body":"### Willingness to contribute\r\nThe MLflow Community encourages bug fix contributions. Would you or another member of your organization be willing to contribute a fix for this bug to the MLflow code base?\r\n\r\n- [ ] Yes. I can contribute a fix for this bug independently.\r\n- [x] Yes. I would be willing to contribute a fix for this bug with guidance from the MLflow community.\r\n- [ ] No. I cannot contribute a bug fix at this time.\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in MLflow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: MacOS Monterrey (MLFlow hosted on Linux -- Docker + Heroku)\r\n- **MLflow installed from (source or binary)**:\r\n- **MLflow version (run ``mlflow --version``)**:\r\n- **Python version**:\r\n- **npm version, if running the dev UI**:\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\n\r\nTrying to run `mlflow::mlflow_transition_model_version_stage` as follows:\r\n\r\n```r\r\nmlflow_transition_model_version_stage(\r\n    name = \"foo\",\r\n    version = 1,\r\n    stage = \"Staging\"\r\n)\r\n```\r\n\r\ngives me the following errors. In R, I see this:\r\n\r\n```r\r\nError: API request to endpoint 'model-versions\/transition-stage' failed with error code 500. Reponse body: '<!DOCTYPE HTML PUBLIC \"-\/\/W3C\/\/DTD HTML 3.2 Final\/\/EN\">\r\n<title>500 Internal Server Error<\/title>\r\n<h1>Internal Server Error<\/h1>\r\n<p>The server encountered an internal error and was unable to complete your request. Either the server is overloaded or there is an error in the application.<\/p>\r\n'\r\n```\r\n\r\nand in my Heroku logs, I see this:\r\n\r\n```bash\r\n2021-12-25T04:55:31.636699+00:00 heroku[router]: at=info method=POST path=\"\/api\/2.0\/mlflow\/model-versions\/transition-stage\" host=<<my host>> request_id=<<rid>> fwd=... dyno=web.1 connect=0ms service=2ms status=500 bytes=463 protocol=https\r\n2021-12-25T04:55:31.635410+00:00 app[web.1]: 2021\/12\/25 04:55:31 ERROR mlflow.server: Exception on \/api\/2.0\/mlflow\/model-versions\/transition-stage [POST]\r\n2021-12-25T04:55:31.635418+00:00 app[web.1]: Traceback (most recent call last):\r\n2021-12-25T04:55:31.635419+00:00 app[web.1]:   File \"\/opt\/conda\/lib\/python3.9\/site-packages\/google\/protobuf\/json_format.py\", line 585, in _ConvertFieldValuePair\r\n2021-12-25T04:55:31.635420+00:00 app[web.1]:     setattr(message, field.name, _ConvertScalarFieldValue(value, field))\r\n2021-12-25T04:55:31.635420+00:00 app[web.1]:   File \"\/opt\/conda\/lib\/python3.9\/site-packages\/google\/protobuf\/json_format.py\", line 720, in _ConvertScalarFieldValue\r\n2021-12-25T04:55:31.635421+00:00 app[web.1]:     return _ConvertBool(value, require_str)\r\n2021-12-25T04:55:31.635421+00:00 app[web.1]:   File \"\/opt\/conda\/lib\/python3.9\/site-packages\/google\/protobuf\/json_format.py\", line 838, in _ConvertBool\r\n2021-12-25T04:55:31.635422+00:00 app[web.1]:     raise ParseError('Expected true or false without quotes.')\r\n2021-12-25T04:55:31.635423+00:00 app[web.1]: google.protobuf.json_format.ParseError: Expected true or false without quotes.\r\n2021-12-25T04:55:31.635423+00:00 app[web.1]: \r\n2021-12-25T04:55:31.635424+00:00 app[web.1]: During handling of the above exception, another exception occurred:\r\n2021-12-25T04:55:31.635424+00:00 app[web.1]: \r\n2021-12-25T04:55:31.635424+00:00 app[web.1]: Traceback (most recent call last):\r\n2021-12-25T04:55:31.635425+00:00 app[web.1]:   File \"\/opt\/conda\/lib\/python3.9\/site-packages\/flask\/app.py\", line 2073, in wsgi_app\r\n2021-12-25T04:55:31.635425+00:00 app[web.1]:     response = self.full_dispatch_request()\r\n2021-12-25T04:55:31.635426+00:00 app[web.1]:   File \"\/opt\/conda\/lib\/python3.9\/site-packages\/flask\/app.py\", line 1518, in full_dispatch_request\r\n2021-12-25T04:55:31.635426+00:00 app[web.1]:     rv = self.handle_user_exception(e)\r\n2021-12-25T04:55:31.635426+00:00 app[web.1]:   File \"\/opt\/conda\/lib\/python3.9\/site-packages\/flask\/app.py\", line 1516, in full_dispatch_request\r\n2021-12-25T04:55:31.635427+00:00 app[web.1]:     rv = self.dispatch_request()\r\n2021-12-25T04:55:31.635427+00:00 app[web.1]:   File \"\/opt\/conda\/lib\/python3.9\/site-packages\/flask\/app.py\", line 1502, in dispatch_request\r\n2021-12-25T04:55:31.635427+00:00 app[web.1]:     return self.ensure_sync(self.view_functions[rule.endpoint])(**req.view_args)\r\n2021-12-25T04:55:31.635428+00:00 app[web.1]:   File \"\/opt\/conda\/lib\/python3.9\/site-packages\/mlflow\/server\/handlers.py\", line 238, in wrapper\r\n2021-12-25T04:55:31.635428+00:00 app[web.1]:     return func(*args, **kwargs)\r\n2021-12-25T04:55:31.635428+00:00 app[web.1]:   File \"\/opt\/conda\/lib\/python3.9\/site-packages\/mlflow\/server\/handlers.py\", line 767, in _transition_stage\r\n2021-12-25T04:55:31.635429+00:00 app[web.1]:     request_message = _get_request_message(TransitionModelVersionStage())\r\n2021-12-25T04:55:31.635429+00:00 app[web.1]:   File \"\/opt\/conda\/lib\/python3.9\/site-packages\/mlflow\/server\/handlers.py\", line 219, in _get_request_message\r\n2021-12-25T04:55:31.635429+00:00 app[web.1]:     parse_dict(request_json, request_message)\r\n2021-12-25T04:55:31.635430+00:00 app[web.1]:   File \"\/opt\/conda\/lib\/python3.9\/site-packages\/mlflow\/utils\/proto_json_utils.py\", line 153, in parse_dict\r\n2021-12-25T04:55:31.635430+00:00 app[web.1]:     ParseDict(js_dict=js_dict, message=message, ignore_unknown_fields=True)\r\n2021-12-25T04:55:31.635430+00:00 app[web.1]:   File \"\/opt\/conda\/lib\/python3.9\/site-packages\/google\/protobuf\/json_format.py\", line 445, in ParseDict\r\n2021-12-25T04:55:31.635431+00:00 app[web.1]:     parser.ConvertMessage(js_dict, message)\r\n2021-12-25T04:55:31.635431+00:00 app[web.1]:   File \"\/opt\/conda\/lib\/python3.9\/site-packages\/google\/protobuf\/json_format.py\", line 476, in ConvertMessage\r\n2021-12-25T04:55:31.635431+00:00 app[web.1]:     self._ConvertFieldValuePair(value, message)\r\n2021-12-25T04:55:31.635432+00:00 app[web.1]:   File \"\/opt\/conda\/lib\/python3.9\/site-packages\/google\/protobuf\/json_format.py\", line 588, in _ConvertFieldValuePair\r\n2021-12-25T04:55:31.635432+00:00 app[web.1]:     raise ParseError('Failed to parse {0} field: {1}.'.format(name, e))\r\n2021-12-25T04:55:31.635433+00:00 app[web.1]: google.protobuf.json_format.ParseError: Failed to parse archive_existing_versions field: Expected true or false without quotes..\r\n```\r\n\r\nI'm not sure how to repro this more cleanly, but it seems like a bug. I tried explicitly setting the `archive_existing_versions` argument to a bunch of different things (i.e. `TRUE`, `FALSE`, `\"true\"`, `\"false\"`, etc.) to no avail.\r\n\r\nAnd while we're at I also noticed a small typo in the R package documentation under `?mlflow_transition_model_version_stage`: `Transition 'model_version' to this tage.` --> `Transition 'model_version' to this stage.`. It'd also be helpful to list out what the allowed stages are in the docs and to use something like `match.arg()` [at this point in the code](https:\/\/github.com\/mlflow\/mlflow\/blob\/master\/mlflow\/R\/mlflow\/R\/model-registry.R#L302-L306) to match against the allowed stages (I'd be happy to PR this change -- it'd be easy). \r\n\r\n### What component(s), interfaces, languages, and integrations does this bug affect?\r\nComponents \r\n- [ ] `area\/artifacts`: Artifact stores and artifact logging\r\n- [ ] `area\/build`: Build and test infrastructure for MLflow\r\n- [ ] `area\/docs`: MLflow documentation pages\r\n- [ ] `area\/examples`: Example code\r\n- [x] `area\/model-registry`: Model Registry service, APIs, and the fluent client calls for Model Registry\r\n- [ ] `area\/models`: MLmodel format, model serialization\/deserialization, flavors\r\n- [ ] `area\/projects`: MLproject format, project running backends\r\n- [ ] `area\/scoring`: MLflow Model server, model deployment tools, Spark UDFs\r\n- [ ] `area\/server-infra`: MLflow Tracking server backend\r\n- [ ] `area\/tracking`: Tracking Service, tracking client APIs, autologging\r\n\r\nInterface \r\n- [ ] `area\/uiux`: Front-end, user experience, plotting, JavaScript, JavaScript dev server\r\n- [ ] `area\/docker`: Docker use across MLflow's components, such as MLflow Projects and MLflow Models\r\n- [ ] `area\/sqlalchemy`: Use of SQLAlchemy in the Tracking Service or Model Registry\r\n- [ ] `area\/windows`: Windows support\r\n\r\nLanguage \r\n- [x] `language\/r`: R APIs and clients\r\n- [ ] `language\/java`: Java APIs and clients\r\n- [ ] `language\/new`: Proposals for new client languages\r\n\r\nIntegrations\r\n- [ ] `integrations\/azure`: Azure and Azure ML integrations\r\n- [ ] `integrations\/sagemaker`: SageMaker integrations\r\n- [ ] `integrations\/databricks`: Databricks integrations\r\n","pull_request":null,"labels":["bug","language\/r","area\/model-registry"],"labels_description":["Something isn't working","R APIs and clients","Model registry, model registry APIs, and the fluent client calls for model registry"],"entities":["http"]},{"html_url":"https:\/\/github.com\/mlflow\/mlflow\/issues\/5195","id":1087405915,"number":5195,"title":"[bug] mlflow ui not launching in iframe","state":"open","locked":false,"assignee":null,"assignees":[],"comments":1,"created_at":1640239624000,"updated_at":1647295993000,"closed_at":null,"body":"I am trying to launch mlflow ui in iframe but it gives DOM exception ...Blocked a frame with origin from accessing a cross origin frame error. I am working in python DASH app. Inside DASH app I want to launch mlflow ui but I am not able to do so with iframe .\r\n\r\nCan anyone suggest how to solve this issue.","pull_request":null,"labels":["bug"],"labels_description":["Something isn't working"],"entities":[]},{"html_url":"https:\/\/github.com\/mlflow\/mlflow\/issues\/5193","id":1086731308,"number":5193,"title":"[bug]","state":"open","locked":false,"assignee":null,"assignees":[],"comments":0,"created_at":1640172636000,"updated_at":1640173380000,"closed_at":null,"body":"Thank you for submitting an issue. Please refer to our [issue policy](https:\/\/www.github.com\/mlflow\/mlflow\/blob\/master\/ISSUE_POLICY.md) for additional information about bug reports. For help with debugging your code, please refer to [Stack Overflow](https:\/\/stackoverflow.com\/questions\/tagged\/mlflow).\r\n\r\n**Please fill in this bug report template to ensure a timely and thorough response.**\r\n\r\n### Willingness to contribute\r\nThe MLflow Community encourages bug fix contributions. Would you or another member of your organization be willing to contribute a fix for this bug to the MLflow code base?\r\n\r\n- [ ] Yes. I can contribute a fix for this bug independently.\r\n- [ ] Yes. I would be willing to contribute a fix for this bug with guidance from the MLflow community.\r\n- [x] No. I cannot contribute a bug fix at this time.\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in MLflow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **MLflow installed from (source or binary)**:\r\n- **MLflow version (run ``mlflow --version``)**:\r\n- **Python version**:\r\n- **npm version, if running the dev UI**:\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Include descriptions of the expected behavior and the actual behavior.\r\nI was serving a model using MLFlow model serve, the model won't serve until I put the source code in the same directory of the MLModel file so that I serve it. I want to serve the MLModel file without the need of the source code.\r\n\r\n### Code to reproduce issue\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\nCan't reproduce.\r\n\r\n### Other info \/ logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\nI thought the method load_context() in a wrapper class would help me to avoid this but I couldn't make it didn't have the result I was hoping for either. I implemented it like this:\r\n\r\n```\r\nclass Wrapper(PythonModel):\r\n    def __init__(self):\r\n        self.logger = get_logger(name='model')\r\n        self.model = None\r\n\r\n    def predict(self, context, model_input):\r\n        self.logger.warning(model_input)\r\n        return self.model.predict(model_input.values)\r\n\r\n    def load_context(self, context):\r\n        self.model = mlflow.pyfunc.load_model(path_to_model)\r\n```\r\n\r\n### What component(s), interfaces, languages, and integrations does this bug affect?\r\nComponents \r\n- [ ] `area\/artifacts`: Artifact stores and artifact logging\r\n- [ ] `area\/build`: Build and test infrastructure for MLflow\r\n- [x] `area\/docs`: MLflow documentation pages\r\n- [ ] `area\/examples`: Example code\r\n- [ ] `area\/model-registry`: Model Registry service, APIs, and the fluent client calls for Model Registry\r\n- [ ] `area\/models`: MLmodel format, model serialization\/deserialization, flavors\r\n- [ ] `area\/projects`: MLproject format, project running backends\r\n- [ ] `area\/scoring`: MLflow Model server, model deployment tools, Spark UDFs\r\n- [ ] `area\/server-infra`: MLflow Tracking server backend\r\n- [ ] `area\/tracking`: Tracking Service, tracking client APIs, autologging\r\n\r\nInterface \r\n- [ ] `area\/uiux`: Front-end, user experience, plotting, JavaScript, JavaScript dev server\r\n- [ ] `area\/docker`: Docker use across MLflow's components, such as MLflow Projects and MLflow Models\r\n- [ ] `area\/sqlalchemy`: Use of SQLAlchemy in the Tracking Service or Model Registry\r\n- [ ] `area\/windows`: Windows support\r\n\r\nLanguage \r\n- [ ] `language\/r`: R APIs and clients\r\n- [ ] `language\/java`: Java APIs and clients\r\n- [ ] `language\/new`: Proposals for new client languages\r\n\r\nIntegrations\r\n- [ ] `integrations\/azure`: Azure and Azure ML integrations\r\n- [ ] `integrations\/sagemaker`: SageMaker integrations\r\n- [ ] `integrations\/databricks`: Databricks integrations\r\n","pull_request":null,"labels":["bug","area\/docs"],"labels_description":["Something isn't working","Documentation issues"],"entities":[]},{"html_url":"https:\/\/github.com\/mlflow\/mlflow\/issues\/5192","id":1086415003,"number":5192,"title":"mlflow.exceptions.mlflowexception ","state":"open","locked":false,"assignee":null,"assignees":[],"comments":0,"created_at":1640143887000,"updated_at":1640143887000,"closed_at":null,"body":"how to fix this error ?\r\nmlflow.exceptions.MlflowException : la demande d'API \u00e0 http:\/\/0.0.0.0:5000\/api\/2.0\/mlflow\/runs\/create a \u00e9chou\u00e9 avec l'exception HTTPConnectionPool(host='0.0.0.0', port=5000) : nombre maximal de tentatives d\u00e9pass\u00e9 avec URL : \/api\/2.0\/mlflow\/runs\/create (caus\u00e9 par ResponseError('trop de 500 r\u00e9ponses d'erreur'))","pull_request":null,"labels":["bug"],"labels_description":["Something isn't working"],"entities":["mlflow.exceptions.mlflowexception"]},{"html_url":"https:\/\/github.com\/mlflow\/mlflow\/pull\/5187","id":1085515508,"number":5187,"title":"fix issue #2804 - set run name in mlproject execution","state":"open","locked":false,"assignee":null,"assignees":[],"comments":0,"created_at":1640071161000,"updated_at":1644322130000,"closed_at":null,"body":"Signed-off-by: bramrodenburg <14278376+bramrodenburg@users.noreply.github.com>\r\n\r\n## What changes are proposed in this pull request?\r\n\r\nFixed issue #2804 in which users cannot specify the `run_name` when running an MLproject. \r\n\r\n## How is this patch tested?\r\n\r\n\r\n\r\n## Does this PR change the documentation?\r\n\r\n- [ ] No. You can skip the rest of this section.\r\n- [x] Yes. Make sure the changed pages \/ sections render correctly by following the steps below.\r\n\r\n1. Check the status of the `ci\/circleci: build_doc` check. If it's successful, proceed to the\r\n   next step, otherwise fix it.\r\n2. Click `Details` on the right to open the job page of CircleCI.\r\n3. Click the `Artifacts` tab.\r\n4. Click `docs\/build\/html\/index.html`.\r\n5. Find the changed pages \/ sections and make sure they render correctly.\r\n\r\n## Release Notes\r\n\r\n### Is this a user-facing change?\r\n\r\n- [ ] No. You can skip the rest of this section.\r\n- [x] Yes. Give a description of this change to be included in the release notes for MLflow users.\r\n\r\nUser can now specify the run_name when running an MLflow Project.\r\n\r\n### What component(s), interfaces, languages, and integrations does this PR affect?\r\nComponents \r\n- [ ] `area\/artifacts`: Artifact stores and artifact logging\r\n- [ ] `area\/build`: Build and test infrastructure for MLflow\r\n- [ ] `area\/docs`: MLflow documentation pages\r\n- [ ] `area\/examples`: Example code\r\n- [ ] `area\/model-registry`: Model Registry service, APIs, and the fluent client calls for Model Registry\r\n- [ ] `area\/models`: MLmodel format, model serialization\/deserialization, flavors\r\n- [x] `area\/projects`: MLproject format, project running backends\r\n- [ ] `area\/scoring`: MLflow Model server, model deployment tools, Spark UDFs\r\n- [ ] `area\/server-infra`: MLflow Tracking server backend\r\n- [ ] `area\/tracking`: Tracking Service, tracking client APIs, autologging\r\n\r\nInterface \r\n- [ ] `area\/uiux`: Front-end, user experience, plotting, JavaScript, JavaScript dev server\r\n- [ ] `area\/docker`: Docker use across MLflow's components, such as MLflow Projects and MLflow Models\r\n- [ ] `area\/sqlalchemy`: Use of SQLAlchemy in the Tracking Service or Model Registry\r\n- [ ] `area\/windows`: Windows support\r\n\r\nLanguage \r\n- [ ] `language\/r`: R APIs and clients\r\n- [ ] `language\/java`: Java APIs and clients\r\n- [ ] `language\/new`: Proposals for new client languages\r\n\r\nIntegrations\r\n- [ ] `integrations\/azure`: Azure and Azure ML integrations\r\n- [ ] `integrations\/sagemaker`: SageMaker integrations\r\n- [ ] `integrations\/databricks`: Databricks integrations\r\n\r\n<!--\r\nInsert an empty named anchor here to allow jumping to this section with a fragment URL\r\n(e.g. https:\/\/github.com\/mlflow\/mlflow\/pull\/123#user-content-release-note-category).\r\nNote that GitHub prefixes anchor names in markdown with \"user-content-\".\r\n-->\r\n<a name=\"release-note-category\"><\/a>\r\n### How should the PR be classified in the release notes? Choose one:\r\n\r\n- [ ] `rn\/breaking-change` - The PR will be mentioned in the \"Breaking Changes\" section\r\n- [ ] `rn\/none` - No description will be included. The PR will be mentioned only by the PR number in the \"Small Bugfixes and Documentation Updates\" section\r\n- [ ] `rn\/feature` - A new user-facing feature worth mentioning in the release notes\r\n- [x] `rn\/bug-fix` - A user-facing bug fix worth mentioning in the release notes\r\n- [ ] `rn\/documentation` - A user-facing documentation change worth mentioning in the release notes\r\n","pull_request":{"url":"https:\/\/api.github.com\/repos\/mlflow\/mlflow\/pulls\/5187","html_url":"https:\/\/github.com\/mlflow\/mlflow\/pull\/5187","diff_url":"https:\/\/github.com\/mlflow\/mlflow\/pull\/5187.diff","patch_url":"https:\/\/github.com\/mlflow\/mlflow\/pull\/5187.patch","merged_at":null},"labels":["rn\/bug-fix","area\/projects"],"labels_description":["Mention under Bug Fixes in Changelogs.","MLproject format, project running backends"],"entities":["fix","issue","set","name","mlproject","execution"]},{"html_url":"https:\/\/github.com\/mlflow\/mlflow\/issues\/5184","id":1084632761,"number":5184,"title":"[bug] metric visualization in ui becomes broken. rectangular\/constant curve instead of real curve.","state":"open","locked":false,"assignee":null,"assignees":[],"comments":0,"created_at":1639996991000,"updated_at":1639997522000,"closed_at":null,"body":"### Willingness to contribute\r\nThe MLflow Community encourages bug fix contributions. Would you or another member of your organization be willing to contribute a fix for this bug to the MLflow code base?\r\n\r\n- [ ] Yes. I can contribute a fix for this bug independently.\r\n- [ x] Yes. I would be willing to contribute a fix for this bug with guidance from the MLflow community.\r\n- [ ] No. I cannot contribute a bug fix at this time.\r\n\r\n### System information\r\n- **Have I written custom code: Yes\r\n- **OS Platform and Distribution:  Linux Ubuntu 18.04\r\n- **MLflow installed from: binary\r\n- **MLflow version: 1.22.0\r\n- **Python version: 3.8\r\n\r\n### Describe the problem\r\nThe metrics visualization of all runs in an experiment becomes broken\/are just plotted as a single constant value. Instead of showing the actual metric curve just a rectangular big blue box is plotted. Before everything is logged and visualized fine and at some point the metric visualization gets broken. This is then also the case for all metric values and also for previous runs under the same experiment name that were vsualized correctly before. It looks like in the database something is messed up and only the most recent metric value is stored without the past values which results in a overall constant metric. The broken curve looks like this (same for all other metric curves in the same experiment):\r\n![grafik](https:\/\/user-images.githubusercontent.com\/16177860\/146753090-9a971e69-5fbe-47cf-b834-eae1be66d685.png)\r\n\r\nI still could not find out when this happens. The only thing I know is that this happens after some time after multiple experiment runs then suddenly all metrics are broken. I should also add that I mostly stopp runs with Ctrl+C and not let them properly finish. \r\n\r\nMinimal code for logging that reproduces this behaviour is: \r\n   ```\r\n mlflow.set_experiment(\"my-example-experiment\")\r\n with mlflow.start_run():\r\n        mlflow.log_param(\"a\", 1)\r\n        mlflow.log_metric(\"b\", 2, step=0)\r\n        mlflow.log_metric(\"b\", 3, step=1)\r\n```\r\n\r\nRunning this many times after some time the metric visualization of \"b\" looks rectangualr\/broken instead of a line going from 2 to 3. Did anyone also experience this issue?\r\n\r\nIntegrations\r\n- [ x] `integrations\/azure`: MLFlow tracking server deployed in Azure App Service where the backend store is a managed PostgreSQL database.\r\n","pull_request":null,"labels":["bug","integrations\/azure"],"labels_description":["Something isn't working","Azure and Azure ML integrations"],"entities":["metric","visualization","curve","curve."]},{"html_url":"https:\/\/github.com\/mlflow\/mlflow\/issues\/5181","id":1084226138,"number":5181,"title":"[fr] [r] add support for {sparklyr} models","state":"open","locked":false,"assignee":null,"assignees":[],"comments":1,"created_at":1639958419000,"updated_at":1640142718000,"closed_at":null,"body":"## Willingness to contribute\r\nThe MLflow Community encourages new feature contributions. Would you or another member of your organization be willing to contribute an implementation of this feature (either as an MLflow Plugin or an enhancement to the MLflow code base)?\r\n\r\n- [ ] Yes. I can contribute this feature independently.\r\n- [x] Yes. I would be willing to contribute this feature with guidance from the MLflow community.\r\n- [ ] No. I cannot contribute this feature at this time.\r\n\r\n## Proposal Summary\r\n\r\nThe mlflow R package has dropped support for `{mleap}` (https:\/\/github.com\/mlflow\/mlflow\/pull\/5166) which will become problematic as people who've depended on this have no alternative flavor to use. `{carrier}` package will not work for these cases.\r\n\r\nPropose that we either add `{mleap}` support back and help maintain `{mleap}` as it will continue to be the method used via python.\r\n\r\nAlternatively we can add a flavour specific to `{sparklyr}` but it will break more supporting functions (I've got a half-working prototype).\r\n\r\n\r\n\r\n## Motivation\r\nCurrently there is no supported flavor for models trained via `{sparklyr}` and it will become a barrier for existing users as they upgrade.\r\n\r\n### What component(s), interfaces, languages, and integrations does this feature affect?\r\nComponents \r\n- [x] `area\/artifacts`: Artifact stores and artifact logging\r\n- [ ] `area\/build`: Build and test infrastructure for MLflow\r\n- [x] `area\/docs`: MLflow documentation pages\r\n- [x] `area\/examples`: Example code\r\n- [ ] `area\/model-registry`: Model Registry service, APIs, and the fluent client calls for Model Registry\r\n- [x] `area\/models`: MLmodel format, model serialization\/deserialization, flavors\r\n- [ ] `area\/projects`: MLproject format, project running backends\r\n- [ ] `area\/scoring`: MLflow Model server, model deployment tools, Spark UDFs\r\n- [ ] `area\/server-infra`: MLflow Tracking server backend\r\n- [ ] `area\/tracking`: Tracking Service, tracking client APIs, autologging\r\n\r\nInterfaces\r\n- [ ] `area\/uiux`: Front-end, user experience, plotting, JavaScript, JavaScript dev server\r\n- [ ] `area\/docker`: Docker use across MLflow's components, such as MLflow Projects and MLflow Models\r\n- [ ] `area\/sqlalchemy`: Use of SQLAlchemy in the Tracking Service or Model Registry\r\n- [ ] `area\/windows`: Windows support\r\n\r\nLanguages \r\n- [x] `language\/r`: R APIs and clients\r\n- [ ] `language\/java`: Java APIs and clients\r\n- [ ] `language\/new`: Proposals for new client languages\r\n\r\nIntegrations\r\n- [ ] `integrations\/azure`: Azure and Azure ML integrations\r\n- [ ] `integrations\/sagemaker`: SageMaker integrations\r\n- [ ] `integrations\/databricks`: Databricks integrations\r\n\r\n## Details\r\n\r\nCode that could be basis of an alternative to `{mleap}`, I don't think we should pursue this but just incase I tested it.\r\n\r\n```\r\nmlflow_save_model.ml_pipeline_model <- function(model,\r\n                                       path,\r\n                                       model_spec = list(),\r\n                                       conda_env = NULL,\r\n                                       ...) {\r\n  \r\n  mlflow:::assert_pkg_installed(\"sparklyr\")\r\n  if (dir.exists(path)) unlink(path, recursive = TRUE)\r\n  dir.create(path)\r\n  \r\n  model_data_subpath <- \"sparklyr\"\r\n  dir.create(file.path(path, model_data_subpath))\r\n  \r\n  # sparklyr::ml_save uses `invoke(\"save\", path)` so we add `file:\/` to path (avoid writing to DBFS on Databricks)\r\n  destination <- sparklyr::ml_save(model, path = file.path(\"file:\/\/\/\", path, model_data_subpath), overwrite = TRUE)\r\n  \r\n  conda_env <- mlflow:::create_default_conda_env_if_absent(\r\n    path, conda_env, default_pip_deps = list(\"mlflow\", paste0(\"sparklyr==\", as.character(utils::packageVersion(\"sparklyr\"))))\r\n  )\r\n  \r\n  sparklyr_conf <- list(\r\n    ml_pipeline_model = list(sparklyr_version = version, model_data = model_data_subpath)\r\n  )\r\n  \r\n  pyfunc_conf <- mlflow:::create_pyfunc_conf(\r\n    loader_module = \"mlflow.ml_pipeline_model\",\r\n    data = model_data_subpath,\r\n    env = conda_env\r\n  )\r\n  \r\n  model_spec$flavors <- c(model_spec$flavors, sparklyr_conf, pyfunc_conf)\r\n  mlflow:::mlflow_write_model_spec(path, model_spec)\r\n}\r\n\r\n\r\nmlflow_load_flavor.mlflow_flavor_ml_pipeline_model <- function(flavor, model_path) {\r\n  mlflow:::assert_pkg_installed(\"sparklyr\")\r\n  # `mlflow::mlflow_load_model()` does not have `...` we return a function that can be used with sparkContext \r\n  fn <- rlang::new_function(\r\n    args = rlang::exprs(sc =),\r\n    rlang::expr(sparklyr::ml_load(sc, file.path(\"file:\/\/\/\", !!model_path, \"sparklyr\")))\r\n   )\r\n  return(fn)\r\n}\r\n\r\nmlflow_predict.ml_pipeline_model <- function(sc, model, data, ...) {\r\n  mlflow:::assert_pkg_installed(\"sparklyr\")\r\n  sparklyr::ml_transform(model, data)\r\n}\r\n\r\n# ----------------------------------------\r\n# override mlflow function\r\nR.utils::reassignInPackage(\r\n  \"mlflow_save_model.ml_pipeline_model\",\r\n  pkgName = \"mlflow\",\r\n  value = mlflow_save_model.ml_pipeline_model\r\n)\r\n\r\n```\r\n\r\n","pull_request":null,"labels":["enhancement","area\/docs","language\/r","area\/artifacts","area\/examples","area\/models"],"labels_description":["New feature or request","Documentation issues","R APIs and clients","Artifact stores and artifact logging","Example code","MLmodel format, model serialization\/deserialization, flavors"],"entities":["support"]},{"html_url":"https:\/\/github.com\/mlflow\/mlflow\/issues\/5179","id":1082624048,"number":5179,"title":"[question] query regarding multi-threading, threadlock, concurrent futures inside mlflow?","state":"open","locked":false,"assignee":null,"assignees":[],"comments":0,"created_at":1639687195000,"updated_at":1639726001000,"closed_at":null,"body":"I've seen usage of `threading.lock()` api -\r\n1. https:\/\/github.com\/mlflow\/mlflow\/blob\/0fa849ad75e5733bf76cc14a4455657c5c32f107\/mlflow\/store\/tracking\/sqlalchemy_store.py#L89\r\n2. https:\/\/github.com\/mlflow\/mlflow\/blob\/4195541c7f6a79fba4ccad2295534f16eaa8db39\/mlflow\/_spark_autologging.py#L28\r\n\r\nOn searching for threading use cases - https:\/\/github.com\/mlflow\/mlflow\/search?p=1&q=threading , it seems to be used in a few tests, some Kubernetes usage - https:\/\/github.com\/mlflow\/mlflow\/blob\/7a2e079b546644defba5bed35b5a8baa97693050\/mlflow\/projects\/kubernetes.py#L109 and a threadpool usage in code related to TensorFlow - https:\/\/github.com\/mlflow\/mlflow\/blob\/0fa849ad75e5733bf76cc14a4455657c5c32f107\/mlflow\/tensorflow\/__init__.py#L73\r\n\r\nNow coming back to my queries, would it be safe to assume -\r\n1. MLFlow uses multi-threading?\r\n2. If we're contributing to or using MLFlow OSS code, which parts should we be cautious of concurrency-related issues? Maybe Database? Logging? Flask-server?\r\n3. Multi-processing wasn't used for certain specific reasons?","pull_request":null,"labels":[],"labels_description":[],"entities":["query"]},{"html_url":"https:\/\/github.com\/mlflow\/mlflow\/issues\/5178","id":1082413226,"number":5178,"title":"[bug] modulenotfounderror: no module named 'custom_transformer'","state":"open","locked":false,"assignee":null,"assignees":[],"comments":2,"created_at":1639672456000,"updated_at":1640138090000,"closed_at":null,"body":"### Willingness to contribute\r\n- [ ] Yes. I can contribute a fix for this bug independently.\r\n- [ ] Yes. I would be willing to contribute a fix for this bug with guidance from the MLflow community.\r\n- [x] No. I cannot contribute a bug fix at this time.\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in MLflow)**: -\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10 Home 21H2\r\n- **MLflow installed from (source or binary)**: Source\r\n- **MLflow version (run ``mlflow --version``)**: 1.22.0\r\n- **Python version**: 3.9\r\n- **npm version, if running the dev UI**: -\r\n\r\n### Describe the problem\r\nI use mlflow autologging for my sklearn pipeline and after exporting model to a different project, whenever I try to load the model, I get an error that modules are missing.\r\n\r\n### Code to reproduce issue\r\nI created an easily reproducible example.\r\n\r\nStep 1: Create a module to import in train.py\r\n\r\n    # custom_transformer.py\r\n\r\n    from sklearn.base import BaseEstimator, TransformerMixin\r\n\r\n    class ColumnSelector(BaseEstimator, TransformerMixin):\r\n\r\n        def fit(self, X, y=None):\r\n            return self\r\n\r\n        def transform(self, X):\r\n            return X\r\n\r\nStep 2: Create and log sklearn model\r\n\r\n    # train.py\r\n\r\n    import mlflow.sklearn\r\n    from sklearn.datasets import load_breast_cancer\r\n    from sklearn.naive_bayes import GaussianNB\r\n    from sklearn.pipeline import Pipeline, make_pipeline\r\n    from custom_transformer import ColumnSelector\r\n\r\n    mlflow.autolog()\r\n    \r\n    pipe = Pipeline([(\"selector\", ColumnSelector())])\r\n\r\n    data = load_breast_cancer()\r\n    labels = data[\"target\"]\r\n    features = data[\"data\"]\r\n\r\n    model = make_pipeline(pipe, GaussianNB())\r\n    model.fit(features, labels)\r\n\r\n\r\nStep 3: Copy model folder to a different project and try to load\r\n\r\n    # load.py\r\n\r\n    import mlflow.sklearn\r\n\r\n    mlflow.sklearn.load_model(\"model\")\r\n\r\n\r\n### Other info \/ logs\r\n    Traceback (most recent call last):\r\n      File \"C:\\Users\\issue_test\\load.py\", line 6, in load\r\n        mlflow.sklearn.load_model(\"model\")\r\n      File \"C:\\Users\\issue_example\\venv\\lib\\site-packages\\mlflow\\sklearn\\__init__.py\", line 549, in load_model\r\n        return _load_model_from_local_file(\r\n      File \"C:\\Users\\issue_example\\venv\\lib\\site-packages\\mlflow\\sklearn\\__init__.py\", line 417, in _load_model_from_local_file\r\n        return cloudpickle.load(f)\r\n    ModuleNotFoundError: No module named 'custom_transformer'\r\n\r\n\r\n### What component(s), interfaces, languages, and integrations does this bug affect?\r\nComponents \r\n- [x] `area\/artifacts`: Artifact stores and artifact logging\r\n- [ ] `area\/build`: Build and test infrastructure for MLflow\r\n- [ ] `area\/docs`: MLflow documentation pages\r\n- [ ] `area\/examples`: Example code\r\n- [ ] `area\/model-registry`: Model Registry service, APIs, and the fluent client calls for Model Registry\r\n- [x] `area\/models`: MLmodel format, model serialization\/deserialization, flavors\r\n- [ ] `area\/projects`: MLproject format, project running backends\r\n- [ ] `area\/scoring`: MLflow Model server, model deployment tools, Spark UDFs\r\n- [ ] `area\/server-infra`: MLflow Tracking server backend\r\n- [x] `area\/tracking`: Tracking Service, tracking client APIs, autologging\r\n\r\nInterface \r\n- [ ] `area\/uiux`: Front-end, user experience, plotting, JavaScript, JavaScript dev server\r\n- [x] `area\/docker`: Docker use across MLflow's components, such as MLflow Projects and MLflow Models\r\n- [ ] `area\/sqlalchemy`: Use of SQLAlchemy in the Tracking Service or Model Registry\r\n- [x] `area\/windows`: Windows support\r\n","pull_request":null,"labels":["bug","area\/windows","help wanted","area\/artifacts","area\/models","area\/tracking","area\/docker"],"labels_description":["Something isn't working","Issue is unique to windows.","We would like help from the community to add this support","Artifact stores and artifact logging","MLmodel format, model serialization\/deserialization, flavors","Tracking service, tracking client APIs, autologging","Docker use anywhere, such as MLprojects and MLmodels"],"entities":["module"]},{"html_url":"https:\/\/github.com\/mlflow\/mlflow\/issues\/5173","id":1081465747,"number":5173,"title":"[fr] add `mlflow.set_run_name` function","state":"open","locked":false,"assignee":null,"assignees":[],"comments":1,"created_at":1639598622000,"updated_at":1640029617000,"closed_at":null,"body":"## Willingness to contribute\r\n\r\n- [x] Yes. I can contribute this feature independently.\r\n- [ ] Yes. I would be willing to contribute this feature with guidance from the MLflow community.\r\n- [ ] No. I cannot contribute this feature at this time.\r\n\r\n## Proposal Summary\r\n\r\nCreate a new mlflow function in order provide a way to rename the run name. This function could be the `mlflow.set_run_name`\r\n\r\n## Motivation\r\n\r\nThis feature will allow the user to rename the run name using the Python API. A use-case that makes this feature valuable is the following: Many tools implements their own mlflow integration without allowing the user to specify the flow run name, in that case it's useful to provide a high-level API for this purpose. This renaming functionality is currently supported by setting the `mlflow.runName` tag but this approach is not friendly for a new user. \r\n\r\n### What component(s), interfaces, languages, and integrations does this feature affect?\r\nComponents \r\n- [ ] `area\/artifacts`: Artifact stores and artifact logging\r\n- [ ] `area\/build`: Build and test infrastructure for MLflow\r\n- [x] `area\/docs`: MLflow documentation pages\r\n- [x] `area\/examples`: Example code\r\n- [ ] `area\/model-registry`: Model Registry service, APIs, and the fluent client calls for Model Registry\r\n- [ ] `area\/models`: MLmodel format, model serialization\/deserialization, flavors\r\n- [ ] `area\/projects`: MLproject format, project running backends\r\n- [ ] `area\/scoring`: MLflow Model server, model deployment tools, Spark UDFs\r\n- [ ] `area\/server-infra`: MLflow Tracking server backend\r\n- [x] `area\/tracking`: Tracking Service, tracking client APIs, autologging\r\n\r\nInterfaces\r\n- [ ] `area\/uiux`: Front-end, user experience, plotting, JavaScript, JavaScript dev server\r\n- [ ] `area\/docker`: Docker use across MLflow's components, such as MLflow Projects and MLflow Models\r\n- [ ] `area\/sqlalchemy`: Use of SQLAlchemy in the Tracking Service or Model Registry\r\n- [ ] `area\/windows`: Windows support\r\n\r\nLanguages \r\n- [ ] `language\/r`: R APIs and clients\r\n- [ ] `language\/java`: Java APIs and clients\r\n- [ ] `language\/new`: Proposals for new client languages\r\n\r\nIntegrations\r\n- [ ] `integrations\/azure`: Azure and Azure ML integrations\r\n- [ ] `integrations\/sagemaker`: SageMaker integrations\r\n- [ ] `integrations\/databricks`: Databricks integrations\r\n\r\n## Details\r\n\r\nThis new function could look like:\r\n\r\n```python\r\ndef set_run_name(run_id: str, name: str):\r\n    pass  # use the given run_id in order to `mlflow.set_tag('mlflow.runName', name)\r\n```\r\n","pull_request":null,"labels":["enhancement","area\/docs","area\/examples","area\/tracking"],"labels_description":["New feature or request","Documentation issues","Example code","Tracking service, tracking client APIs, autologging"],"entities":["`mlflow.set_run_name`","function"]},{"html_url":"https:\/\/github.com\/mlflow\/mlflow\/issues\/5171","id":1081258821,"number":5171,"title":"[bug] using mlflowclient.get_latest_version with an older server instance causes 404","state":"open","locked":false,"assignee":{"login":"daanknoope","id":8389610,"node_id":"MDQ6VXNlcjgzODk2MTA=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8389610?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/daanknoope","html_url":"https:\/\/github.com\/daanknoope","followers_url":"https:\/\/api.github.com\/users\/daanknoope\/followers","following_url":"https:\/\/api.github.com\/users\/daanknoope\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/daanknoope\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/daanknoope\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/daanknoope\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/daanknoope\/orgs","repos_url":"https:\/\/api.github.com\/users\/daanknoope\/repos","events_url":"https:\/\/api.github.com\/users\/daanknoope\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/daanknoope\/received_events","type":"User","site_admin":false},"assignees":[{"login":"daanknoope","id":8389610,"node_id":"MDQ6VXNlcjgzODk2MTA=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8389610?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/daanknoope","html_url":"https:\/\/github.com\/daanknoope","followers_url":"https:\/\/api.github.com\/users\/daanknoope\/followers","following_url":"https:\/\/api.github.com\/users\/daanknoope\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/daanknoope\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/daanknoope\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/daanknoope\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/daanknoope\/orgs","repos_url":"https:\/\/api.github.com\/users\/daanknoope\/repos","events_url":"https:\/\/api.github.com\/users\/daanknoope\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/daanknoope\/received_events","type":"User","site_admin":false}],"comments":7,"created_at":1639586739000,"updated_at":1643840338000,"closed_at":null,"body":"### Willingness to contribute\r\nThe MLflow Community encourages bug fix contributions. Would you or another member of your organization be willing to contribute a fix for this bug to the MLflow code base?\r\n\r\n- [ ] Yes. I can contribute a fix for this bug independently.\r\n- [x] Yes. I would be willing to contribute a fix for this bug with guidance from the MLflow community.\r\n- [ ] No. I cannot contribute a bug fix at this time.\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in MLflow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: SUSE Linux Enterprise Server 12 SP2\r\n- **MLflow installed from (source or binary)**: Binary\r\n- **MLflow version (run ``mlflow --version``)**: 1.22 for the **client**, 1.11.0 for the **server**\r\n- **Python version**: 3.6.12\r\n- **npm version, if running the dev UI**:\r\n- **Exact command to reproduce**: `MlflowClient().get_latest_versions(\"SOME_REGISTERED_MODEL\")`\r\n\r\n### Describe the problem\r\nWe have MlFlow 1.11.0 installed on a server, and version 1.22 on the client. When the client is connected to the server, and executing the `get_latest_version` command, the following message is shown:\r\n\r\n```python\r\nMlflowException: API request to endpoint \/api\/2.0\/mlflow\/registered-models\/get-latest-versions failed with error code 404 != 200. Response body: '<!DOCTYPE HTML PUBLIC \"-\/\/W3C\/\/DTD HTML 3.2 Final\/\/EN\">\r\n\r\n<title>404 Not Found<\/title>\r\n\r\n<h1>Not Found<\/h1>\r\n\r\n<p>The requested URL was not found on the server. If you entered the URL manually please check your spelling and try again.<\/p>\r\n```\r\nThe expected result would have been the latest version information of the model.\r\n\r\n\r\n### Code to reproduce issue\r\n\r\nPrerequisites:\r\n\r\n1. Run mlflow version 1.11.0 on a server in a conda environment\r\n2. Connect a client to the mlflow server using `mlflow.set_tracking_uri()`\r\n3. Create an experiment on the server using `MlflowClient.create_experiment`\r\n\r\nNow run:\r\n\r\n```python\r\nimport mlflow\r\nfrom mlflow.tracking import MlflowClient\r\n\r\nmlflow.set_tracking_uri(\"TRACKING_URI\")\r\n\r\nclient = MlflowClient()\r\nclient.get_latest_versions(\"EXPERIMENT_NAME\")\r\n```\r\n\r\n\r\n### Other info \/ logs\r\n\r\nThis problem seems to have been introduced with #4999. In that PR, support for POST-calls on get-latest-version was added. \r\nThis by itself is not a problem, since the author of the PR checks that if the POST-call is not available on the server, it catches the `ENDPOINT_NOT_FOUND` exception and tries to use a GET-call. \r\n\r\nThe problem is created however, because it calls `\/mlflow\/registered-models\/get-latest-version` instead of the usual `\/preview\/mlflow\/registered-models\/get-latest-versions`. This means  that not an `ENDPONT_NOT_FOUND` exception is thrown, but a 404 Not Found. This exception is not caught, meaning that the program will not continue to try the GET-call and crashes instead.\r\n\r\nIt seems to me that the omission of `preview` in the URL is the root cause of the bug (see: https:\/\/github.com\/stevenchen-db\/mlflow\/blob\/9bbbb0c28d285476e0f3e2a81ecfbf577d1b03ca\/mlflow\/protos\/model_registry.proto#L133), but I'm not sure if that is on purpose or not. If the omission of `preview` is on purpose, some other way of exception handling could be implemented to avoid getting 404-errors when working with older servers that do not support POST-calls.\r\n\r\nSince the reason behind the missing `preview` part is not entirely clear to me, I did not want to provide a bug fix immediately. If the maintainers could provide some guidance on which approach to fix this bug would be best for this project, I'd be happy to help implement the change.\r\n\r\n### What component(s), interfaces, languages, and integrations does this bug affect?\r\nComponents \r\n- [ ] `area\/artifacts`: Artifact stores and artifact logging\r\n- [ ] `area\/build`: Build and test infrastructure for MLflow\r\n- [ ] `area\/docs`: MLflow documentation pages\r\n- [ ] `area\/examples`: Example code\r\n- [x] `area\/model-registry`: Model Registry service, APIs, and the fluent client calls for Model Registry\r\n- [ ] `area\/models`: MLmodel format, model serialization\/deserialization, flavors\r\n- [ ] `area\/projects`: MLproject format, project running backends\r\n- [ ] `area\/scoring`: MLflow Model server, model deployment tools, Spark UDFs\r\n- [ ] `area\/server-infra`: MLflow Tracking server backend\r\n- [ ] `area\/tracking`: Tracking Service, tracking client APIs, autologging\r\n\r\nInterface \r\n- [ ] `area\/uiux`: Front-end, user experience, plotting, JavaScript, JavaScript dev server\r\n- [ ] `area\/docker`: Docker use across MLflow's components, such as MLflow Projects and MLflow Models\r\n- [ ] `area\/sqlalchemy`: Use of SQLAlchemy in the Tracking Service or Model Registry\r\n- [ ] `area\/windows`: Windows support\r\n\r\nLanguage \r\n- [ ] `language\/r`: R APIs and clients\r\n- [ ] `language\/java`: Java APIs and clients\r\n- [ ] `language\/new`: Proposals for new client languages\r\n\r\nIntegrations\r\n- [ ] `integrations\/azure`: Azure and Azure ML integrations\r\n- [ ] `integrations\/sagemaker`: SageMaker integrations\r\n- [ ] `integrations\/databricks`: Databricks integrations\r\n","pull_request":null,"labels":["bug","Acknowledged","area\/model-registry"],"labels_description":["Something isn't working","This issue has been read and acknowledged by the MLflow admins.","Model registry, model registry APIs, and the fluent client calls for model registry"],"entities":["mlflowclient.get_latest_version","server","instance"]},{"html_url":"https:\/\/github.com\/mlflow\/mlflow\/issues\/5170","id":1081206120,"number":5170,"title":"[fr] better and configurable logging in docker container.","state":"open","locked":false,"assignee":null,"assignees":[],"comments":0,"created_at":1639583780000,"updated_at":1639583795000,"closed_at":null,"body":"Thank you for submitting a feature request. **Before proceeding, please review MLflow's [Issue Policy for feature requests](https:\/\/www.github.com\/mlflow\/mlflow\/blob\/master\/ISSUE_POLICY.md#feature-requests) and the [MLflow Contributing Guide](https:\/\/github.com\/mlflow\/mlflow\/blob\/master\/CONTRIBUTING.rst)**.\r\n\r\n**Please fill in this feature request template to ensure a timely and thorough response.**\r\n\r\n## Willingness to contribute\r\nThe MLflow Community encourages new feature contributions. Would you or another member of your organization be willing to contribute an implementation of this feature (either as an MLflow Plugin or an enhancement to the MLflow code base)?\r\n\r\n- [ ] Yes. I can contribute this feature independently.\r\n- [ ] Yes. I would be willing to contribute this feature with guidance from the MLflow community.\r\n- [x] No. I cannot contribute this feature at this time.\r\n\r\n## Proposal Summary\r\nWhen serving a model in a production environment, which is what we would like to use MLflow for, descriptive logs are crucial for being able to do that reliably. \r\nWe serve the model in a docker image built with `mlflow models build-docker` and currently to our best knowledge there are no logs whatsoever (with the exception of the information about starting gunicorn processes). Currently we would appreciate literally any additional pieces of information during runtime - after startup there is no information about whether the service is doing something.\r\nUseful logs could include: Endpoint access, some information about received data (size, structure, ...), output - model prediction, response time, ...\r\nAs building the image is automated and `mlflow models build-docker` currently does not seem to have any possibility to configure logging, achieving this currently seems impossible. \r\n\r\n\r\n## Motivation\r\n- What is the use case for this feature?\r\n- Why is this use case valuable to support for MLflow users in general?\r\n- Why is this use case valuable to support for your project(s) or organization?\r\n- Why is it currently difficult to achieve this use case? (please be as specific as possible about why related MLflow features and components are insufficient)\r\n\r\n### What component(s), interfaces, languages, and integrations does this feature affect?\r\nComponents \r\n- [ ] `area\/artifacts`: Artifact stores and artifact logging\r\n- [ ] `area\/build`: Build and test infrastructure for MLflow\r\n- [ ] `area\/docs`: MLflow documentation pages\r\n- [ ] `area\/examples`: Example code\r\n- [ ] `area\/model-registry`: Model Registry service, APIs, and the fluent client calls for Model Registry\r\n- [x] `area\/models`: MLmodel format, model serialization\/deserialization, flavors\r\n- [ ] `area\/projects`: MLproject format, project running backends\r\n- [ ] `area\/scoring`: MLflow Model server, model deployment tools, Spark UDFs\r\n- [ ] `area\/server-infra`: MLflow Tracking server backend\r\n- [ ] `area\/tracking`: Tracking Service, tracking client APIs, autologging\r\n\r\nInterfaces\r\n- [ ] `area\/uiux`: Front-end, user experience, plotting, JavaScript, JavaScript dev server\r\n- [x] `area\/docker`: Docker use across MLflow's components, such as MLflow Projects and MLflow Models\r\n- [ ] `area\/sqlalchemy`: Use of SQLAlchemy in the Tracking Service or Model Registry\r\n- [ ] `area\/windows`: Windows support\r\n\r\nLanguages \r\n- [ ] `language\/r`: R APIs and clients\r\n- [ ] `language\/java`: Java APIs and clients\r\n- [ ] `language\/new`: Proposals for new client languages\r\n\r\nIntegrations\r\n- [ ] `integrations\/azure`: Azure and Azure ML integrations\r\n- [ ] `integrations\/sagemaker`: SageMaker integrations\r\n- [ ] `integrations\/databricks`: Databricks integrations\r\n\r\n\r\n","pull_request":null,"labels":["enhancement","area\/models","area\/docker"],"labels_description":["New feature or request","MLmodel format, model serialization\/deserialization, flavors","Docker use anywhere, such as MLprojects and MLmodels"],"entities":["logging","docker","container."]},{"html_url":"https:\/\/github.com\/mlflow\/mlflow\/issues\/5169","id":1081069373,"number":5169,"title":"[bug] tensorflow.autolog or keras.autolog submitting empty runs","state":"open","locked":false,"assignee":null,"assignees":[],"comments":3,"created_at":1639576482000,"updated_at":1642151405000,"closed_at":null,"body":"Thank you for submitting an issue. Please refer to our [issue policy](https:\/\/www.github.com\/mlflow\/mlflow\/blob\/master\/ISSUE_POLICY.md) for additional information about bug reports. For help with debugging your code, please refer to [Stack Overflow](https:\/\/stackoverflow.com\/questions\/tagged\/mlflow).\r\n\r\n**Please fill in this bug report template to ensure a timely and thorough response.**\r\n\r\n### Willingness to contribute\r\nThe MLflow Community encourages bug fix contributions. Would you or another member of your organization be willing to contribute a fix for this bug to the MLflow code base?\r\n\r\n- [ ] Yes. I can contribute a fix for this bug independently.\r\n- [ ] Yes. I would be willing to contribute a fix for this bug with guidance from the MLflow community.\r\n- [x] No. I cannot contribute a bug fix at this time.\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in MLflow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 20.04\r\n- **MLflow installed from (source or binary)**: from binary (pip install)\r\n- **MLflow version (run ``mlflow --version``)**: 1.18.0 (also tried with 1.22.0)\r\n- **Python version**: Python 3.8.10\r\n- **npm version, if running the dev UI**:\r\n- **Exact command to reproduce**: \r\n\r\n### Describe the problem\r\nUsing .autolog (mlflow.keras.autolog() or mlflow.tensorflow.autolog()) on a Keras model is submitting empty runs (no params and no metrics)\r\n![mlflow_issue](https:\/\/user-images.githubusercontent.com\/10974651\/146200426-30940698-7cdf-495f-9720-362dbd3307b2.jpg)\r\n\r\n ### Code to reproduce issue\r\n```\r\nepoch = self.load_checkpoint(network=self.training_model, directory_helper=self.directory_helper)\r\nself._save_model_description()\r\ncheckpoint_path = os.path.join(self.directory_helper.checkpoint_dir(), CHECKPOINT_PATH_PATTERN)\r\nsave_callback = ModelCheckpoint(\r\n            filepath=checkpoint_path, save_weights_only=True, save_freq=\"epoch\", monitor=\"loss\"\r\n        )\r\nMLFLOW_TRACKING_SERVER_INSTANCE = \"http:\/\/localhost:5000\"\r\nmlflow.set_tracking_uri(MLFLOW_TRACKING_SERVER_INSTANCE)\r\nmlflow.set_experiment(\"my-test\")\r\nmlflow.tensorflow.autolog()\r\n# mlflow.keras.autolog() # tried also this\r\nwith mlflow.start_run() as run:\r\n    print(\"Active run_id: {}\".format(run.info.run_id)) # giving me a valid id\r\n    self.training_model.fit(\r\n        sample_iterator.framework_iterable(),\r\n        epochs=self.epochs,\r\n        steps_per_epoch=self.steps_per_epoch,\r\n        callbacks=self.callbacks + [save_callback],\r\n        initial_epoch=epoch,\r\n    )\r\n    self.training_model.evaluate(sample_iterator.framework_iterable(), steps=1)\r\n    self.training_model.save(\"\/tmp\/model\")\r\n```\r\nThe code runs successfully (train, evaluate and a model is saved)\r\n\r\n### Other info \/ logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\nTesting started at 14:48 ...\r\n\/<somedir>\/bin\/python \/home\/<somedir>\/pycharm-2020.2.2\/plugins\/python\/helpers\/pycharm\/_jb_pytest_runner.py --target network_integration_test.py::test_training_integration\r\nLaunching pytest with arguments network_integration_test.py::test_training_integration in <script>\r\n\r\n============================= test session starts ==============================\r\nplatform linux -- Python 3.8.10, pytest-6.2.5, py-1.11.0, pluggy-1.0.0 -- \/<somedir>\/bin\/python\r\ncachedir: .pytest_cache\r\nrootdir: <script_dir>\r\ncollecting ... collected 1 item\r\n\r\nnetwork_integration_test.py::test_training_integration PASSED            [100%]Using data set train and preprocessor mode train_steps.\r\nActive run_id: 27da2a3697764209b64851b9b3f95c1f\r\n1\/1 [==============================] - 0s 19ms\/step - batch: 0.0000e+00 - size: 1.0000 - loss: 4113.8296\r\nTraining is done.\r\nwaiting 2s---------------------------\r\nUsing data set validation and preprocessor mode validation_steps.\r\ninferred output:\r\n {'image_extractor': {'output': array([[[0.0000000e+00, 6.8138964e-03, 0.0000000e+00],\r\n        [0.0000000e+00, 1.1089750e-01, 0.0000000e+00],\r\n        [0.0000000e+00, 1.4115120e-01, 0.0000000e+00],\r\n        ...,\r\n        [0.0000000e+00, 4.0602821e+01, 0.0000000e+00],\r\n        [0.0000000e+00, 7.1283195e+01, 0.0000000e+00],\r\n        [0.0000000e+00, 7.4124146e+01, 0.0000000e+00]],\r\n\r\n       [[0.0000000e+00, 2.6389563e-01, 0.0000000e+00],\r\n        [0.0000000e+00, 6.3583678e-01, 0.0000000e+00],\r\n        [0.0000000e+00, 9.5497471e-01, 0.0000000e+00],\r\n        ...,\r\n        [0.0000000e+00, 8.9535469e+01, 0.0000000e+00],\r\n        [0.0000000e+00, 8.9088066e+01, 0.0000000e+00],\r\n        [0.0000000e+00, 8.9849754e+01, 0.0000000e+00]],\r\n\r\n       [[0.0000000e+00, 2.4204983e-01, 0.0000000e+00],\r\n        [0.0000000e+00, 4.7542697e-01, 0.0000000e+00],\r\n        [0.0000000e+00, 7.0950073e-01, 0.0000000e+00],\r\n        ...,\r\n        [0.0000000e+00, 5.1282654e+01, 0.0000000e+00],\r\n        [7.3204637e+00, 4.3331047e+01, 0.0000000e+00],\r\n        [0.0000000e+00, 5.6928738e+01, 0.0000000e+00]],\r\n\r\n       ...,\r\n\r\n       [[0.0000000e+00, 1.8032816e-01, 0.0000000e+00],\r\n        [0.0000000e+00, 3.6331969e-01, 0.0000000e+00],\r\n        [0.0000000e+00, 5.4345101e-01, 0.0000000e+00],\r\n        ...,\r\n        [0.0000000e+00, 1.9824995e+01, 0.0000000e+00],\r\n        [0.0000000e+00, 1.1003504e+01, 0.0000000e+00],\r\n        [0.0000000e+00, 3.6498924e+01, 0.0000000e+00]],\r\n\r\n       [[0.0000000e+00, 7.8257382e-02, 0.0000000e+00],\r\n        [0.0000000e+00, 1.9469540e-01, 0.0000000e+00],\r\n        [0.0000000e+00, 2.3878448e-01, 0.0000000e+00],\r\n        ...,\r\n        [0.0000000e+00, 2.6312938e+00, 0.0000000e+00],\r\n        [9.3650808e+00, 0.0000000e+00, 0.0000000e+00],\r\n        [1.0873189e+01, 8.4439039e+00, 0.0000000e+00]],\r\n\r\n       [[0.0000000e+00, 2.2829400e-02, 0.0000000e+00],\r\n        [0.0000000e+00, 8.9347780e-02, 0.0000000e+00],\r\n        [0.0000000e+00, 1.3833980e-01, 0.0000000e+00],\r\n        ...,\r\n        [5.5682340e+00, 6.9527259e+00, 0.0000000e+00],\r\n        [1.1677513e+01, 0.0000000e+00, 0.0000000e+00],\r\n        [5.9880052e+00, 0.0000000e+00, 0.0000000e+00]]], dtype=float32)}}\r\n2021-12-15 14:48:59.091233: I tensorflow\/stream_executor\/platform\/default\/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1\r\n2021-12-15 14:48:59.112495: E tensorflow\/stream_executor\/cuda\/cuda_driver.cc:314] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\r\n2021-12-15 14:48:59.112559: I tensorflow\/stream_executor\/cuda\/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: cmucl855212\r\n2021-12-15 14:48:59.112570: I tensorflow\/stream_executor\/cuda\/cuda_diagnostics.cc:176] hostname: cmucl855212\r\n2021-12-15 14:48:59.112671: I tensorflow\/stream_executor\/cuda\/cuda_diagnostics.cc:200] libcuda reported version is: 470.57.2\r\n2021-12-15 14:48:59.112713: I tensorflow\/stream_executor\/cuda\/cuda_diagnostics.cc:204] kernel reported version is: 470.57.2\r\n2021-12-15 14:48:59.112720: I tensorflow\/stream_executor\/cuda\/cuda_diagnostics.cc:310] kernel version seems to match DSO: 470.57.2\r\n2021-12-15 14:48:59.112968: I tensorflow\/core\/platform\/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 FMA\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2021-12-15 14:48:59.136761: I tensorflow\/core\/platform\/profile_utils\/cpu_utils.cc:104] CPU Frequency: 2599990000 Hz\r\n2021-12-15 14:48:59.138434: I tensorflow\/compiler\/xla\/service\/service.cc:168] XLA service 0x2302b70 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2021-12-15 14:48:59.138455: I tensorflow\/compiler\/xla\/service\/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2021-12-15 14:48:59.961617: W tensorflow\/python\/util\/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\r\n\r\n\r\n=============================== warnings summary ===============================\r\n..\/..\/..\/..\/..\/..\/..\/..\/..\/venv\/lib\/python3.8\/site-packages\/tensorflow\/python\/autograph\/utils\/testing.py:21\r\n  \/<somedir>\/lib\/python3.8\/site-packages\/tensorflow\/python\/autograph\/utils\/testing.py:21: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\r\n    import imp\r\n\r\nnetwork_integration_test.py::test_training_integration\r\n  \/<somedir>\/lib\/python3.8\/site-packages\/pkg_resources\/__init__.py:1144: DeprecationWarning: Use of .. or absolute path in a resource path is not allowed and will raise exceptions in a future release.\r\n    return get_provider(package_or_requirement).get_resource_filename(\r\n\r\n-- Docs: https:\/\/docs.pytest.org\/en\/stable\/warnings.html\r\n======================== 1 passed, 2 warnings in 5.24s =========================\r\n\r\nProcess finished with exit code 0\r\n\r\n\r\n\r\n### What component(s), interfaces, languages, and integrations does this bug affect?\r\nComponents \r\n- [x] `area\/artifacts`: Artifact stores and artifact logging\r\n- [ ] `area\/build`: Build and test infrastructure for MLflow\r\n- [ ] `area\/docs`: MLflow documentation pages\r\n- [x] `area\/examples`: Example code\r\n- [ ] `area\/model-registry`: Model Registry service, APIs, and the fluent client calls for Model Registry\r\n- [ ] `area\/models`: MLmodel format, model serialization\/deserialization, flavors\r\n- [ ] `area\/projects`: MLproject format, project running backends\r\n- [ ] `area\/scoring`: MLflow Model server, model deployment tools, Spark UDFs\r\n- [ ] `area\/server-infra`: MLflow Tracking server backend\r\n- [x] `area\/tracking`: Tracking Service, tracking client APIs, autologging\r\n\r\nInterface \r\n- [ ] `area\/uiux`: Front-end, user experience, plotting, JavaScript, JavaScript dev server\r\n- [ ] `area\/docker`: Docker use across MLflow's components, such as MLflow Projects and MLflow Models\r\n- [ ] `area\/sqlalchemy`: Use of SQLAlchemy in the Tracking Service or Model Registry\r\n- [ ] `area\/windows`: Windows support\r\n\r\nLanguage \r\n- [ ] `language\/r`: R APIs and clients\r\n- [ ] `language\/java`: Java APIs and clients\r\n- [ ] `language\/new`: Proposals for new client languages\r\n\r\nIntegrations\r\n- [ ] `integrations\/azure`: Azure and Azure ML integrations\r\n- [ ] `integrations\/sagemaker`: SageMaker integrations\r\n- [ ] `integrations\/databricks`: Databricks integrations\r\n\r\n\r\n","pull_request":null,"labels":["bug","area\/artifacts","area\/examples","area\/tracking"],"labels_description":["Something isn't working","Artifact stores and artifact logging","Example code","Tracking service, tracking client APIs, autologging"],"entities":["tensorflow.autolog","keras.autolog"]},{"html_url":"https:\/\/github.com\/mlflow\/mlflow\/issues\/5168","id":1080902898,"number":5168,"title":"[question] repository structure of model flavours","state":"open","locked":false,"assignee":null,"assignees":[],"comments":0,"created_at":1639566436000,"updated_at":1639568870000,"closed_at":null,"body":"Hi,\r\n\r\nThis is more of a question than a bug or a feature.\r\n\r\nOn browsing through the model \"flavour\" modules (e.g. pytorch, paddlepaddle etc.), I note that a majority of the code base is contained within the `__init__.py` file of each flavour module and I feel this is slightly unconventional.\r\n\r\n**Is there a reason for this being implemented this way?**\r\n\r\nIf shortening imports is the reason, then bringing all functions to the module level can be achieved by setting `__all__` attribute in the `__init__.py` file. For example, the pytorch model flavour `mlflow.pytorch` could be done this way, with all functions moved to a `pytorch.py` file in the `mflow\/pytorch` folder, the `__init__.py` would look as follows:\r\n\r\n```python\r\nfrom .python import log_model, save_model, get_default_conda_env, get_default_pip_requirements\r\n\r\n__all__ = [log_model, save_model, get_default_conda_env, get_default_pip_requirements]\r\n```\r\n\r\nThis would result in the above functions being accessible through the import:\r\n\r\n`from mlflow.pytorch import log_model`\r\n\r\nJust as though the functions themselves were contained within the `__init__.py` as they currently are.\r\n\r\nFurthermore, breaking the functions out into even more files is possible, as the imports can be collected into the `__init__.py` namespace. If the functions were saved into two separate python file, `model.py` and `env.py`, then they can be called from the pytorch module with the following `__init__.py` structure:\r\n\r\n```python\r\nfrom .model import log_model, save_model\r\nfrom .env import get_default_conda_env, get_default_pip_requirements\r\n\r\n__all__ = [log_model, save_model, get_default_conda_env, get_default_pip_requirements]\r\n```","pull_request":null,"labels":[],"labels_description":[],"entities":["structure","model"]},{"html_url":"https:\/\/github.com\/mlflow\/mlflow\/pull\/5157","id":1077664014,"number":5157,"title":"create custom_class.py","state":"open","locked":false,"assignee":null,"assignees":[],"comments":1,"created_at":1639261269000,"updated_at":1639261286000,"closed_at":null,"body":"In order to demonstrate the use of a custom transformer we will create a new feature based upon the ratio of the resting blood pressure to the maximum blood pressure. This feature will be created as a new class and saved into a separate file so it can be output to the MLflow tracking server to be used during deployment in combination with the saved model.\r\n\r\n## What changes are proposed in this pull request?\r\n\r\n(Please fill in changes proposed in this fix)\r\n\r\n## How is this patch tested?\r\n\r\n(Details)\r\n\r\n## Does this PR change the documentation?\r\n\r\n- [ ] No. You can skip the rest of this section.\r\n- [ ] Yes. Make sure the changed pages \/ sections render correctly by following the steps below.\r\n\r\n1. Check the status of the `ci\/circleci: build_doc` check. If it's successful, proceed to the\r\n   next step, otherwise fix it.\r\n2. Click `Details` on the right to open the job page of CircleCI.\r\n3. Click the `Artifacts` tab.\r\n4. Click `docs\/build\/html\/index.html`.\r\n5. Find the changed pages \/ sections and make sure they render correctly.\r\n\r\n## Release Notes\r\n\r\n### Is this a user-facing change?\r\n\r\n- [ ] No. You can skip the rest of this section.\r\n- [ ] Yes. Give a description of this change to be included in the release notes for MLflow users.\r\n\r\n(Details in 1-2 sentences. You can just refer to another PR with a description if this PR is part of a larger change.)\r\n\r\n### What component(s), interfaces, languages, and integrations does this PR affect?\r\nComponents \r\n- [ ] `area\/artifacts`: Artifact stores and artifact logging\r\n- [ ] `area\/build`: Build and test infrastructure for MLflow\r\n- [ ] `area\/docs`: MLflow documentation pages\r\n- [ ] `area\/examples`: Example code\r\n- [ ] `area\/model-registry`: Model Registry service, APIs, and the fluent client calls for Model Registry\r\n- [ ] `area\/models`: MLmodel format, model serialization\/deserialization, flavors\r\n- [ ] `area\/projects`: MLproject format, project running backends\r\n- [ ] `area\/scoring`: MLflow Model server, model deployment tools, Spark UDFs\r\n- [ ] `area\/server-infra`: MLflow Tracking server backend\r\n- [ ] `area\/tracking`: Tracking Service, tracking client APIs, autologging\r\n\r\nInterface \r\n- [ ] `area\/uiux`: Front-end, user experience, plotting, JavaScript, JavaScript dev server\r\n- [ ] `area\/docker`: Docker use across MLflow's components, such as MLflow Projects and MLflow Models\r\n- [ ] `area\/sqlalchemy`: Use of SQLAlchemy in the Tracking Service or Model Registry\r\n- [ ] `area\/windows`: Windows support\r\n\r\nLanguage \r\n- [ ] `language\/r`: R APIs and clients\r\n- [ ] `language\/java`: Java APIs and clients\r\n- [ ] `language\/new`: Proposals for new client languages\r\n\r\nIntegrations\r\n- [ ] `integrations\/azure`: Azure and Azure ML integrations\r\n- [ ] `integrations\/sagemaker`: SageMaker integrations\r\n- [ ] `integrations\/databricks`: Databricks integrations\r\n\r\n<!--\r\nInsert an empty named anchor here to allow jumping to this section with a fragment URL\r\n(e.g. https:\/\/github.com\/mlflow\/mlflow\/pull\/123#user-content-release-note-category).\r\nNote that GitHub prefixes anchor names in markdown with \"user-content-\".\r\n-->\r\n<a name=\"release-note-category\"><\/a>\r\n### How should the PR be classified in the release notes? Choose one:\r\n\r\n- [ ] `rn\/breaking-change` - The PR will be mentioned in the \"Breaking Changes\" section\r\n- [ ] `rn\/none` - No description will be included. The PR will be mentioned only by the PR number in the \"Small Bugfixes and Documentation Updates\" section\r\n- [ ] `rn\/feature` - A new user-facing feature worth mentioning in the release notes\r\n- [ ] `rn\/bug-fix` - A user-facing bug fix worth mentioning in the release notes\r\n- [ ] `rn\/documentation` - A user-facing documentation change worth mentioning in the release notes\r\n","pull_request":{"url":"https:\/\/api.github.com\/repos\/mlflow\/mlflow\/pulls\/5157","html_url":"https:\/\/github.com\/mlflow\/mlflow\/pull\/5157","diff_url":"https:\/\/github.com\/mlflow\/mlflow\/pull\/5157.diff","patch_url":"https:\/\/github.com\/mlflow\/mlflow\/pull\/5157.patch","merged_at":null},"labels":[],"labels_description":[],"entities":["custom_class.py"]}]